{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"kcp Documentation","text":""},{"location":"#overview","title":"Overview","text":"<p>kcp is a Kubernetes-like control plane focusing on:</p> <ul> <li>A control plane for many independent, isolated \u201cclusters\u201d known as workspaces</li> <li>Enabling API service providers to offer APIs centrally using multi-tenant operators</li> <li>Easy API consumption for users in their workspaces</li> <li>Advanced deployment strategies for scenarios such as affinity/anti-affinity, geographic replication, cross-cloud   replication, etc.</li> </ul> <p>kcp can be a building block for SaaS service providers who need a massively multi-tenant platform to offer services to a large number of fully isolated tenants using Kubernetes-native APIs. The goal is to be useful to cloud providers as well as enterprise IT departments offering APIs within their company.</p>"},{"location":"#quickstart","title":"Quickstart","text":"<p>To get started with trying out kcp on your local system, check out our Quickstart instructions.</p>"},{"location":"#contributing","title":"Contributing","text":"<p>We \u2764\ufe0f our contributors! If you're interested in helping us out, please head over to our Contributing guide.</p>"},{"location":"#getting-in-touch","title":"Getting in touch","text":"<p>There are several ways to communicate with us:</p> <ul> <li>The <code>#kcp-dev</code> channel in the Kubernetes Slack workspace.</li> <li>Our mailing lists:<ul> <li>kcp-dev for development discussions.</li> <li>kcp-users for discussions among users and potential users.</li> </ul> </li> <li>By joining the kcp-dev mailing list, you should receive an invite to our bi-weekly community meetings.</li> <li>See recordings of past community meetings on YouTube.</li> <li>The next community meeting dates are available via our CNCF community group.</li> <li>Check the community meeting notes document for future and past meeting agendas.</li> <li>Browse the shared Google Drive to share design docs, notes, etc.<ul> <li>Members of the kcp-dev mailing list can view this drive.</li> </ul> </li> </ul>"},{"location":"#additional-references","title":"Additional references","text":"<ul> <li>KubeCon EU 2021: Kubernetes as the Hybrid Cloud Control Plane Keynote - Clayton Coleman (video)</li> <li>OpenShift Commons: Kubernetes as the Control Plane for the Hybrid Cloud - Clayton Coleman (video)</li> <li>TGI Kubernetes 157: Exploring kcp: apiserver without Kubernetes</li> <li>K8s SIG Architecture meeting discussing kcp - June 29, 2021</li> <li>Let's Learn kcp - A minimal Kubernetes API server with Saiyam Pathak - July 7, 2021</li> </ul>"},{"location":"GOALS/","title":"Project Goals","text":"<p><code>kcp</code>'s goal is to be a cloud native control plane for APIs that resemble IaaS/PaaS/SaaS patterns. The central motivation of building <code>kcp</code> is to be able to provide arbitrary services through a unified platform and resource model. <code>kcp</code> is not providing significant as-a-Service implementations but understands itself as a foundational piece of technology that can be built upon, whether it be a SaaS platform, an IdP (internal developer platform) or a multi-cluster scheduling solution. We aim to build an ecosystem that centers around a Kubernetes-like control plane (<code>kcp</code>) but services many scenarios that benefit from having a declarative data store.</p> <p>Warning</p> <p>Previously, <code>kcp</code> had a bigger scope of providing a transparent multi-cluster (TMC) solution. These efforts have been put on hold for the time being. If you are interested in reviving them, please reach out to the maintainers.</p>"},{"location":"GOALS/#the-manifesto","title":"The Manifesto","text":"<p>Our mission is to improve building and running cloud-native services. We see a convergence in tooling and technology between clusters, clouds, and services as being both possible and desirable and <code>kcp</code> explores how the existing Kubernetes ecosystem might evolve to serve that need.</p> <p>Not every idea below may bear fruit, but it's never the wrong time to look for new ways to change.</p>"},{"location":"GOALS/#key-concepts","title":"Key Concepts","text":"<ul> <li>Use Kubernetes APIs to decouple desired intent and actual state for replicating applications to multiple clusters</li> </ul> <p>Kubernetes' strength is separating user intent from actual state so that machines can ensure recovery as infrastructure changes. Since clusters are intended to be a single failure domain, by separating the desired state from any one \"real\" cluster we can potentially unlock better resiliency, simpler workload isolation, and allow workloads to move through the dev/stage/prod pipeline more cleanly. If we can keep familiar tools and APIs working, but separate the app just a bit from the cluster, that can help us move and react to failure more effectively.</p> <ul> <li>Use logical tenant clusters as the basis for application and security isolation</li> </ul> <p>Allow a single kube-apiserver to support multiple (up to 1000) logical clusters that can map/sync/schedule to zero or many physical clusters. Each logical cluster could be much more focused - only the resources needed to support a single application or team, but with the ability to scale to lots of applications. Because the logical clusters are served by the same server, we could amortize the cost of each individual cluster (things like RBAC, CRDs, and authentication can be shared / hierarchal).</p> <p>We took inspiration from the virtual cluster project within sig-multicluster as well as vcluster and other similar approaches that leverage cluster tenancy which led us to ask if we could make those clusters an order of magnitude cheaper by building within the kube-apiserver rather than running full copies. Most applications are small, which means amortizing costs can become a huge win. Single process sharing would let us embed significantly more powerful tenancy concepts like hierarchy across clusters, virtualizing key interfaces, and a much more resilient admission chain than what can be done in webhooks.</p> <p>See the investigations doc for logical clusters for more.</p> <p>Most importantly, if clusters are cheap, we can:</p> <ul> <li>Support stronger tenancy and isolation of CRDs and applications</li> </ul> <p>Lots of little clusters gives us the opportunity to improve how CRDs can be isolated (for development or individual teams), shared (one source for many consumers), and evolved (identify and flag incompatibilities between APIs provided by different clusters). A control plane above Kubernetes lets us separate the \"data plane\" of controllers/integrations from the infrastructure that runs them and allows for centralization of integrations. If you have higher level workloads, talking to higher level abstractions like cloud services, and the individual clusters are just a component, suddenly integrating new patterns and controls becomes more valuable. Conversely, if we have a control plane and a data plane, the types of integrations at each level can begin to differ. More powerful integrations to physical clusters might be run only by infrastructure operations teams, while application integrations could be safely namespaced within the control plane.</p> <p>Likewise, as we split up applications into smaller chunks, we can more carefully define their dependencies.  The account service from the identity team doesn't need to know the details of the frontend website or even where or how it runs. Instead, teams could have the freedom of their own personal clusters, with the extensions they need, without being able to access the details of their peer's except by explicit contract.</p> <p>If we can make extending Kubernetes more interesting by providing this higher level control plane, we likewise need to deal with the scalability of that extensibility:</p> <ul> <li>Make Kubernetes controllers more scalable and flexible on both the client and the server</li> </ul> <p>Subdividing one cluster into hundreds makes integrations harder - a controller would need to be able to access resources across all of those clusters (whether logical, virtual, or physical). For this model to work, we need to explore improvements to the Kubernetes API that would make multi-cluster controllers secure and easy. That involves ideas like watching multiple resources at the same time, listing or watching in bulk across lots of logical clusters, filtering server side, and better client tooling. Many of these improvements could also benefit single-cluster use cases and scalability.</p> <p>To go further, standardizing some of the multi-cluster concepts (whether scheduling, location, or resiliency) into widely used APIs could benefit everyone in the Kubernetes ecosystem, as we often end up building and rebuilding custom platform tooling. The best outcome would be small incremental improvements across the entire Kube ecosystem leading to increased reuse and a reduced need to invest in specific solutions, regardless of the level of the project.</p> <p>Finally, the bar is still high to writing controllers. Lowering the friction of automation and integration is in everyone's benefit - whether that's a bash script, a Terraform configuration, or custom SRE services.  If we can reduce the cost of both infrastructure as code and new infrastructure APIs we can potentially make operational investments more composable.</p> <p>See the investigations doc for minimal API server for more on    improving the composability of the Kube API server.</p>"},{"location":"GOALS/#process","title":"Process","text":"<p>Right now we are interested in assessing how these goals fit within the larger ecosystem.</p>"},{"location":"GOALS/#principles","title":"Principles","text":"<p>Principles are the high level guiding rules we'd like to frame designs around. This is generally useful for resolving design debates by finding thematic connections that reinforce other choices. A few early principles have been discussed:</p> <ol> <li> <p>Convention over configuration / optimize for the user's benefit</p> <p>Do as much as possible for the user the \"right way by default\" (conventions over configuration). For example, <code>kcp</code> embeds the data store for local iteration, but still allows (should allow) remote etcd.</p> </li> <li> <p>Support both a push model and a pull model that fit the control plane mindset</p> <p>Both push (control plane tells others what to do) and pull (agents derive truth from control plane) models have their place. Pull works well when pulling small amounts of desired state and when local resiliency is desired as well as to create a security boundary. Push works well in simple getting started scenarios and when the process is \"acting on behalf\" of a user. For example, <code>kcp</code> and the <code>cluster-controller</code> example in the demo can work in both the push model (talk to each cluster to grab CRDs and sync resources) and the pull model (run as a separate controller so that customized security rules could be in place). Users should have the ability to pick the right tradeoff for their scale and how their control planes are structured.</p> </li> <li> <p>Balance between speed of local development AND running as a high scale service</p> <p><code>kcp</code> should not overly bias towards \"just the demo\" (in the long run) or take explicit steps that would prevent it from being a highly scalable control-plane-as-a-service. The best outcome would be a simple tool that works at multiple scales and layers well.</p> </li> <li> <p>Be simple, composable, and orthogonal</p> <p>The core Kubernetes model is made of simple composable resources (pods vs services, deployments vs replica sets, persistent volumes vs inline volumes) with a focus on solving a core use case well. <code>kcp</code> should look for the key composable, orthogonal, and \"minimum-viable-simple\" concepts that help people build control planes, support API driven infra across a wide footprint, and provides a center of gravity for \"integrate all the things\". It however should not be afraid to make specific sets of users happy in their daily workflow.</p> </li> <li> <p>Be open to change</p> <p>There is a massive ecosystem of users, vendors, service providers, hackers, operators, developers, and machine AIs (maybe not the last one) building and developing on Kubernetes. This is a starting point, a stake in the ground, a rallying cry. It should challenge, excite, and inspire others, but never limit. As we evolve, we should stay open to new ideas and also opening the door for dramatic rethinks of the possibilities by ourselves or others. Whether this becomes a project, inspires many projects, or fails gloriously, it's about making our lives a bit easier and our tools a bit more reliable, and a meaningful dialogue with the real world is fundamental to success.</p> </li> <li> <p>Consolidate efforts in the ecosystem into a more focused effort</p> <p>Kubernetes is mature and changes to the core happen slowly. By concentrating use cases among a number of participants we can better articulate common needs, focus the design time spent in the core project into a smaller set of efforts, and bring new investment into common shared problems strategically. We should make fast progress and be able to suggest high-impact changes without derailing other important Kubernetes initiatives.</p> </li> </ol>"},{"location":"concepts/","title":"Concepts","text":""},{"location":"concepts/#terminology","title":"Terminology","text":"<p>The Terminology document contains definitions used throughout the kcp project. We strongly recommend familiarizing yourself with this terminology before proceeding. Note that some terms might get changed in the future, and that will be covered by respective design documents.</p>"},{"location":"concepts/#quickstart-tenancy-and-apis","title":"Quickstart: Tenancy and APIs","text":"<p>How to create a new API and use it with tenancy.</p>"},{"location":"concepts/quickstart-tenancy-and-apis/","title":"Quickstart: Tenancy and APIs","text":""},{"location":"concepts/quickstart-tenancy-and-apis/#prerequisites","title":"Prerequisites","text":"<p>A running kcp server. The quickstart is a good starting point.</p>"},{"location":"concepts/quickstart-tenancy-and-apis/#set-your-kubeconfig","title":"Set your KUBECONFIG","text":"<p>To access a workspace, you need credentials and an Kubernetes API server URL for the workspace, both of which are stored in a <code>kubeconfig</code> file.</p> <p>The default context in the kcp-provided <code>admin.kubeconfig</code> gives access to the <code>root</code> workspace as the <code>kcp-admin</code> user.</p> <pre><code>$ export KUBECONFIG=.kcp/admin.kubeconfig\n$ kubectl config get-contexts\nCURRENT   NAME           CLUSTER   AUTHINFO      NAMESPACE\n          base           base      kcp-admin\n*         root           root      kcp-admin\n          system:admin   base      shard-admin\n</code></pre> <p>You can use API discovery to see what resources are available in this <code>root</code> workspace. We're here to explore the <code>tenancy.kcp.io</code> and <code>apis.kcp.io</code> resources.</p> <pre><code>$ kubectl api-resources\nNAME                              SHORTNAMES   APIVERSION                             NAMESPACED   KIND\nconfigmaps                        cm           v1                                     true         ConfigMap\nevents                            ev           v1                                     true         Event\n...\napibindings                                    apis.kcp.io/v1alpha1                  false        APIBinding\napiexports                                     apis.kcp.io/v1alpha1                  false        APIExport\n...\nworkspaces                        ws           tenancy.kcp.io/v1alpha1               false        Workspace\n</code></pre>"},{"location":"concepts/quickstart-tenancy-and-apis/#create-and-navigate-some-workspaces","title":"Create and Navigate Some Workspaces","text":"<p>The <code>ws</code> plugin for <code>kubectl</code> makes it easy to switch your <code>kubeconfig</code> between workspaces, and to create new ones:</p> <pre><code>$ kubectl ws .\nCurrent workspace is \"root\".\n$ kubectl ws create a --enter\nWorkspace \"a\" (type root:organization) created. Waiting for it to be ready...\nWorkspace \"a\" (type root:organization) is ready to use.\nCurrent workspace is \"root:a\".\n$ kubectl ws create b\nWorkspace \"b\" (type root:universal) created. Waiting for it to be ready...\nWorkspace \"b\" (type root:universal) is ready to use.\n$ kubectl get workspaces\nNAME   TYPE        PHASE   URL\nb      universal   Ready   https://myhost:6443/clusters/root:a:b\n$ kubectl ws b\nCurrent workspace is \"root:a:b\".\n$ kubectl ws ..\nCurrent workspace is \"root:a\".\n$ kubectl ws -\nCurrent workspace is \"root:a:b\".\n$ kubectl ws root\nCurrent workspace is \"root\".\n$ kubectl get workspaces\nNAME    TYPE           PHASE   URL\na       organization   Ready   https://myhost:6443/clusters/root:a\n</code></pre> <p>Our <code>kubeconfig</code> now contains two additional contexts, one which represents the current workspace, and the other to keep track of our most recently used workspace. This highlights that the <code>kubectl ws</code> plugin is primarily a convenience wrapper for managing a <code>kubeconfig</code> that can be used for working within a workspace.</p> <pre><code>$ kubectl config get-contexts\nCURRENT   NAME                         CLUSTER                      AUTHINFO      NAMESPACE\n          base                         base                         kcp-admin\n          root                         root                         kcp-admin\n          system:admin                 base                         shard-admin\n*         workspace.kcp.io/current    workspace.kcp.io/current    kcp-admin\n          workspace.kcp.io/previous   workspace.kcp.io/previous   kcp-admin\n</code></pre>"},{"location":"concepts/quickstart-tenancy-and-apis/#understand-workspace-types","title":"Understand Workspace Types","text":"<p>As we can see above, workspaces can contain sub-workspaces, and workspaces have different types. A workspace type defines which sub-workspace types can be created under such workspaces. So, for example:</p> <ul> <li>A universal workspace is the base workspace type that most other workspace types inherit from - they may contain other   universal workspaces, and they have a \"default\" namespace</li> <li>The root workspace primarily contains organization workspaces</li> <li>An organization workspace can contain universal workspaces, or can be further subdivided using team workspaces</li> </ul> <p>The final type of workspace is \"home workspaces\". These are workspaces that are intended to be used privately by individual users. They appear under the <code>root:users</code> workspace (type <code>homeroot</code>) and they are further organized into a hierarchy of <code>homebucket</code> workspaces based on a hash of their name.</p> <pre><code>$ kubectl ws\nCurrent workspace is \"root:users:zu:yc:kcp-admin\".\n$ kubectl ws root\nCurrent workspace is \"root\".\n$ kubectl get workspaces\nNAME    TYPE           PHASE   URL\na       organization   Ready   https://myhost:6443/clusters/root:a\nusers   homeroot       Ready   https://myhost:6443/clusters/root:users\n</code></pre> <p>Workspace types and their behaviors are defined using the <code>WorkspaceType</code> resource:</p> <pre><code>$ kubectl get workspacetypes\nNAME           AGE\nhome           74m\nhomebucket     74m\nhomeroot       74m\norganization   74m\nroot           74m\nteam           74m\nuniversal      74m\n$ kubectl describe workspacetype/team\nName:         team\n...\nAPI Version:  tenancy.kcp.io/v1alpha1\nKind:         WorkspaceType\n...\nSpec:\n  Default Child Workspace Type:\n    Name:  universal\n    Path:  root\n  Extend:\n    With:\n      Name:  universal\n      Path:  root\n  Limit Allowed Parents:\n    Types:\n      Name:  organization\n      Path:  root\n      Name:  team\n      Path:  root\nStatus:\n  Conditions:\n    Status:                True\n    Type:                  Ready\n...\n</code></pre>"},{"location":"concepts/quickstart-tenancy-and-apis/#publish-some-apis-as-a-service-provider","title":"Publish Some APIs as a Service Provider","text":"<p>kcp offers <code>APIExport</code> and <code>APIBinding</code> resources which allow a service provider operating in one workspace to offer its capabilities to service consumers in other workspaces.</p> <p>First we'll create an organization workspace, and then within that create a service provider workspace.</p> <pre><code>$ kubectl ws create wildwest --enter\nWorkspace \"wildwest\" (type root:organization) created. Waiting for it to be ready...\nWorkspace \"wildwest\" (type root:organization) is ready to use.\nCurrent workspace is \"root:wildwest\".\n$ kubectl ws create cowboys-service --enter\nWorkspace \"cowboys-service\" (type root:universal) created. Waiting for it to be ready...\nWorkspace \"cowboys-service\" (type root:universal) is ready to use.\nCurrent workspace is \"root:wildwest:cowboys-service\".\n</code></pre> <p>Then we'll use a CRD to generate an <code>APIResourceSchema</code> and <code>APIExport</code> and apply these within the service provider workspace.</p> <p>The <code>apigen</code> tool used below can be found here. Builds for the tool are not currently published as part of the kcp release process.</p> <pre><code>$ mkdir wildwest-schemas/\n$ ./bin/apigen --input-dir test/e2e/customresourcedefinition/ --output-dir wildwest-schemas/\n$ ls -1 wildwest-schemas/\napiexport-wildwest.dev.yaml\napiresourceschema-cowboys.wildwest.dev.yaml\n\n$ kubectl apply -f wildwest-schemas/apiresourceschema-cowboys.wildwest.dev.yaml\napiresourceschema.apis.kcp.io/v220920-6039d110.cowboys.wildwest.dev created\n$ kubectl apply -f wildwest-schemas/apiexport-wildwest.dev.yaml\napiexport.apis.kcp.io/wildwest.dev created\n</code></pre> <p>You can think of an <code>APIResourceSchema</code> as being equivalent to a CRD, and an <code>APIExport</code> makes a set of schemas available to consumers.</p>"},{"location":"concepts/quickstart-tenancy-and-apis/#use-those-apis-as-a-service-consumer","title":"Use Those APIs as a Service Consumer","text":"<p>Now we can adopt the service consumer persona and create a workspace from which we will use this new <code>APIExport</code>:</p> <pre><code>$ kubectl ws\nCurrent workspace is \"root:users:zu:yc:kcp-admin\".\n$ kubectl ws create --enter test-consumer\nWorkspace \"test-consumer\" (type root:universal) created. Waiting for it to be ready...\nWorkspace \"test-consumer\" (type root:universal) is ready to use.\nCurrent workspace is \"root:users:zu:yc:kcp-admin:test-consumer\".\n\n$ kubectl apply -f - &lt;&lt;EOF\napiVersion: apis.kcp.io/v1alpha1\nkind: APIBinding\nmetadata:\n  name: cowboys\nspec:\n  reference:\n    workspace:\n      path: root:wildwest:cowboys-service\n      exportName: wildwest.dev\nEOF\napibinding.apis.kcp.io/cowboys created\n</code></pre> <p>Now this resource type is available for use within our workspace, so let's create an instance!</p> <pre><code>$ kubectl api-resources | grep wildwest\ncowboys      wildwest.dev/v1alpha1    true    Cowboy\n\n$ kubectl apply -f - &lt;&lt;EOF\napiVersion: wildwest.dev/v1alpha1\nkind: Cowboy\nmetadata:\n  name: one\nspec:\n  intent: one\nEOF\ncowboy.wildwest.dev/one created\n</code></pre>"},{"location":"concepts/quickstart-tenancy-and-apis/#managing-permissions","title":"Managing Permissions","text":"<p>Besides publishing APIs and reconciliating the related resources service providers' controllers may need access to core resources or resources exported by other services in the user workspaces as part of their duties. This access needs for security reason to get authorized. <code>permissionClaims</code> address this need.</p> <p>A service provider wanting to access <code>ConfigMaps</code> needs to specify such a claim in the <code>APIExport</code>:</p> <p><pre><code>spec:\n...\n  permissionClaims:\n    - group: \"\"\n      resource: \"configmaps\"\n</code></pre> Users can then authorize access to this resource type in their workspace by accepting the claim in the <code>APIBinding</code>:</p> <pre><code>spec:\n...\n  permissionClaims:\n    - group: \"\"\n      resource: \"configmaps\"\n      state: Accepted\n</code></pre> <p>There is the possibility to further limit the access claim to single resources.</p>"},{"location":"concepts/quickstart-tenancy-and-apis/#dig-deeper-into-apiexports","title":"Dig Deeper into APIExports","text":"<p>Switching back to the service provider persona:</p> <pre><code>$ kubectl ws root:wildwest:cowboys-service\nCurrent workspace is \"root:wildwest:cowboys-service\".\n$ kubectl get apiexport/wildwest.dev -o yaml\napiVersion: apis.kcp.io/v1alpha1\nkind: APIExport\nmetadata:\n  name: wildwest.dev\n  ...\nstatus:\n  ...\n  identityHash: a6a0cc778bec8c4b844e6326965fbb740b6a9590963578afe07276e6a0d41e20\n</code></pre> <p>We can see that our <code>APIExport</code> has a key attribute in its status - its identity (more on this below). This identity can be used in permissionClaims for referring to non-core resources.</p>"},{"location":"concepts/quickstart-tenancy-and-apis/#apiexportendpointslice","title":"APIExportEndpointSlice","text":"<p><code>APIExportEndpointSlices</code> allow service provider to retrieve the URL of service endpoints, acting as a sink for them. You can think of this endpoint as behaving just like a workspace or cluster, except it searches across all workspaces for instances of the resource types provided by the <code>APIExport</code>. An <code>APIExportEndpointSlice</code> is created by a service provider, references a single <code>APIExport</code> and optionally a <code>Partition</code>. <code>Partitions</code> are a mechanism for filtering service endpoints. Within a multi-sharded kcp, each shard will offer its own service endpoint URL for an <code>APIExport</code>. Service provider may decide to have multiple instances of their controller reconciliating, for instance, resources of shards in the same region. For that they may create an <code>APIExportEndpointSlice</code> in the same workspace where a controller instance is deployed. This <code>APIExportEndpointSlice</code> will then reference a specific <code>Partition</code> by its name in the same workspace filtering the service endpoints for a subset of shards. If an <code>APIExportEndpointSlice</code> does not reference a <code>Partition</code> all the available endpoints are populated in its <code>status</code>. More on <code>Partitions</code> here.</p> <pre><code>$ kubectl apply -f - &lt;&lt;EOF\nkind: APIExportEndpointSlice\napiVersion: apis.kcp.io/v1alpha1\nmetadata:\n    name: cowboys\nspec:\n    export:\n        path: root:wildwest:cowboys-service\n        name: cowboy\n    # optional\n    partition: cloud-region-gcp-europe-xdfgs\nEOF\napiexportendpointslice.apis.kcp.io/cowboys created\n</code></pre> <p>Looking at the status populated by the controller</p> <pre><code>$ kubectl get APIExportEndpointSlice/cowboys -o yaml\nkind: APIExportEndpointSlice\napiVersion: apis.kcp.io/v1alpha1\nmetadata:\n    name: cowboys\n...\nstatus:\n    endpoints\n        - url: https://host1:6443/services/apiexport/root:wildwest:cowboys-service/wildwest.dev\n        - url: https://host2:6443/services/apiexport/root:wildwest:cowboys-service/wildwest.dev\n...\n</code></pre> <p>We can use API discovery to see what resource types are available via the endpoint URL:</p> <pre><code>$ kubectl --server='https://host1:6443/services/apiexport/root:wildwest:cowboys-service/wildwest.dev/clusters/*/' api-resources\nNAME      SHORTNAMES   APIVERSION              NAMESPACED   KIND\ncowboys                wildwest.dev/v1alpha1   true         Cowboy\n</code></pre> <p>The question is ... can we see the instance created by the consumer?</p> <pre><code>$ kubectl --server='https://host1:6443/services/apiexport/root:wildwest:cowboys-service/wildwest.dev/clusters/*/' get -A cowboys \\\n    -o custom-columns='WORKSPACE:.metadata.annotations.kcp\\.dev/cluster,NAME:.metadata.name'\nWORKSPACE                                  NAME\nroot:users:zu:yc:kcp-admin:test-consumer   one\n</code></pre> <p>Yay!</p>"},{"location":"concepts/quickstart-tenancy-and-apis/#apis-faq","title":"APIs FAQ","text":"<p>Q: Why is there a new <code>APIResourceSchema</code> resource type that appears to be very similar to <code>CustomResourceDefinition</code>?</p> <p>A: An APIResourceSchema defines a single custom API type. It is almost identical to a CRD, but creating an APIResourceSchema instance does not add a usable API to the server. By intentionally decoupling the schema definition from serving, API owners can be more explicit about API evolution.</p> <p>Q: Why do I have to append <code>/clusters/*/</code> to the <code>APIExport</code> service endpoint URL?</p> <p>A: The URL represents the base path of a virtual kcp API server. With a standard kcp API server, workspaces live under the <code>/clusters/</code> path, so <code>/clusters/*/</code> represents a wildcard search across all workspaces via this virtual API server.</p> <p>Q: How should we understand an <code>APIExport</code> <code>identityHash</code>?</p> <p>A: Unlike with CRDs, a kcp instance might have many <code>APIResourceSchemas</code> of the same Group/Version/Kind, and users need some way of securely distinguishing them.</p> <p>Each <code>APIExport</code> is allocated a randomized private secret - this is currently just a large random number - and a public identity - just a SHA256 hash of the private secret - which securely identifies this <code>APIExport</code> from others.</p> <p>This is important because an <code>APIExport</code> makes service endpoints available to interact with all instances of a particular <code>APIResourceShema</code>, and we want to make sure that users are clear on which service provider <code>APIExports</code> they are trusting and only the owners of those <code>APIExport</code> have access to their resources via the service endpoints.</p> <p>Q: Why do you have to use <code>--all-namespaces</code> with the <code>APIExport</code> service endpoint?</p> <p>A: Think of this endpoint as representing a wildcard listing across all workspaces. It doesn't make sense to look at a specific namespace across all workspaces, so you have to list across all namespaces too.</p> <p>Q: If I attempt to use an <code>APIExport</code> endpoint before there are any <code>APIBindings</code> I get the \"Error from server (NotFound): Unable to list ...: the server could not find the requested resource\". Is this a bug?</p> <p>A: It is a bug. See https://github.com/kcp-dev/kcp/issues/1183</p> <p>When fixed, we expect the <code>APIExport</code> behavior will change such that an empty list is returned instead of the error.</p>"},{"location":"concepts/terminology/","title":"Terminology","text":"<p>This document contains definitions used throughout the kcp project. We strongly recommend familiarizing yourself with this terminology before proceeding.</p> <p>Note</p> <p>Some terms might get changed as the project develops. This document will be kept up to date with all the relevant changes, while additional details and the history of changes will be covered by the respective design documents.</p>"},{"location":"concepts/terminology/#kcp-server","title":"<code>kcp</code> server","text":"<p>kcp server is a Kubernetes-like API server. It extends the Kubernetes API server to provide multi-tenancy and capabilities for building platforms on top of it. The kcp server can be subdivided into multiple \"logical clusters\" where different logical clusters can be used by different users in the organization.</p> <p>The kcp server only provides resources to accomplish this, such as <code>LogicalCluster</code>, <code>Workspace</code>, <code>APIBinding</code> and <code>APIExport</code>, on top of some core Kubernetes resources, like <code>ConfigMap</code> and <code>Secret</code>. It doesn't provide Kubernetes resources for managing and orchestrating workloads, such as Pods and Deployments. The list of resources provided by default can be found in the Built-in APIs document.</p> <p>kcp follows the Kubernetes API semantics in each logical cluster, i.e. kcp should be conformant to the subset of the Kubernetes conformance suite that applies to the APIs available in kcp. In other words, if you're building a platform on top of kcp, you'll be required to build Kubernetes-like resources and APIs, think of CustomResourceDefinitions (CRDs) and controllers. The difference is that in some cases you might have to use a kcp-specific resources instead of regular Kubernetes resources to accomplish something. For example, to define your own resource that can be used across different logical clusters, you'll have to use the <code>APIResourceSchema</code> resource instead of <code>CustomResourceDefinition</code>.</p> <p>As the kcp server is based on the Kubernetes API server, it provides very similar configuration options and uses etcd as its datastore, so if you have experience with operating Kubernetes, you can very easily apply it to kcp.</p> <p>At the end, the kcp server and all logical clusters created on top of it can be accessed using the regular Kubernetes tooling such as kubectl and client-go.</p> <p>Note</p> <p>The only exception to this are Kubernetes controllers. The regular Kubernetes controllers can work with a single (logical) cluster. However, if you're building a developer platform on top of kcp, you likely want your controller to reconcile objects across multiple logical clusters. At the moment, this is not supported by upstream Kubernetes controller-runtime. Instead you have to use our fork of it. We're working with the upstream community on adding support for the multi-cluster setups to the Kubernetes controller-runtime.</p>"},{"location":"concepts/terminology/#logical-cluster","title":"Logical Cluster","text":"<p>A logical cluster is a way to subdivide a kcp server and its <code>etcd</code> datastore into multiple clusters without requiring multiple API server and etcd instances. A logical cluster is able to amortize the cost of a new cluster to be near-zero memory and storage (similar cost as a namespace), so that we can create a huge number of empty clusters cheaply.</p> <p>Each logical cluster has it's own set of APIs installed, it's own objects, and different access semantics and policies, i.e. RBAC rules. In other words, each logical cluster is isolated from other logical clusters.</p> <p>This is accomplished by adding an additional attribute to an object's identifier in etcd and kcp server to associate an object with its logical cluster. On the storage level, the regular Kubernetes API server identifies objects using (group, version, resource, optional namespace, name). The kcp server enriches this identifier with logical cluster's name, so we have (group, version, resource, logical cluster name, optional namespace, name).</p> <p>A logical cluster provides Kubernetes-cluster-like HTTPS/CRUD endpoints, i.e. endpoints that the typical Kubernetes client tooling (e.g. client-go, controller-runtime, and others) can interact with as they would with regular Kubernetes clusters. Using the appropriate kubeconfig file, a user can run <code>kubectl get foo -A</code> to return all objects of type <code>foo</code> across all namespaces in that concrete logical cluster.</p> <p>To a user, a logical cluster appears to be a Kubernetes cluster without any of the container orchestration specific resources (e.g. Pods, ReplicaSets, Deployments...). It has its own discovery, its own OpenAPI spec, and follows the Kubernetes-like constraints about uniqueness of Group-Version-Resource and its behavior. For example, it's not possible to have two identical GVRs with different schemas in one logical cluster, but it's possible to have two identical GVRs with different schemas in different logical clusters.</p> <p>However, in the most of cases, you'll never create a logical cluster on its own. Instead, you would use an abstraction, such as Workspace, to create logical cluster.</p> <p>Note</p> <p>There could be multiple different models that result in logical clusters being created, with different policies or lifecycle. The Workspace is intended to be the most canonical way to create logical clusters for anyone willing to build control planes.</p> <p>Note</p> <p>Throughout the documentation, terms \"workspace\" and \"logical cluster\" might be interchanged. In the most of cases, unless stated otherwise, what applies to a workspace applies to a logical cluster as well, because Workspaces are abstractions built on top of logical clusters.</p>"},{"location":"concepts/terminology/#workspace","title":"Workspace","text":"<p>A Workspace is an abstraction used to create and provision logical clusters. Aside from creating the LogicalCluster object, the responsibility of a Workspace is to initialize the given logical cluster by installing the required resources for the given workspace type, creating the initial objects, and more.</p> <p>Workspaces are managed via the <code>Workspace</code> resource. It's a simple resource that mainly contains the workspace name, type, and URL.</p> <p>Workspaces are organized in a multi-root tree structure. The root workspace is created by the kcp server by default and it doesn't have its own <code>Workspace</code> object (it only has the corresponding <code>LogicalCluster</code> object). Each type of workspace may restrict the types of its children and may restrict the types it may be a child of; a parent-child relationship is allowed if and only if the parent allows the child and the child allows the parent.</p> <p>A Workspace's path is based on the hierarchy and the user provided name. For example, if you create a workspace called <code>bar</code> in a workspace called <code>foo</code> (which is a child of the root workspace), the relevant workspace paths will be:</p> <ul> <li><code>root:foo</code> for the <code>foo</code> workspace</li> <li><code>root:foo:bar</code> for the <code>bar</code> workspace</li> </ul> <p>The workspace path is used for building the workspace URL and for accessing the workspace via the <code>ws</code> kubectl plugin.</p> <p>More information, including examples, can be found in the the Workspaces document.</p>"},{"location":"concepts/terminology/#workspace-types","title":"Workspace Types","text":"<p>Workspaces have types, which are mostly oriented around a set of default or optional APIs exposed. For example:</p> <ul> <li>a workspace intended for deploying applications might expose the same API objects a user would encounter on a   physical cluster,</li> <li>a workspace intended for building Knative functions might expose only the Knative serving APIs, ConfigMaps, Secrets,   and optionally enable Knative Eventing APIs.</li> </ul> <p>By default, each workspace has the built-in APIs installed and available to its users.</p> <p>More information, including a list of Workspace Types and examples, can be found in the Workspace Types document.</p>"},{"location":"concepts/terminology/#virtual-workspaces","title":"Virtual Workspaces","text":"<p>A Virtual Workspace API server is a Kubernetes-like API server under a custom URL that's serving Virtual Workspaces or more precisely Virtual Logical Clusters. The API surface looks very similar to the kcp server itself, sharing the same HTTP path structure, with each (virtual) logical cluster having its own HTTP endpoint under <code>/clusters/&lt;logical cluster&gt;</code>. As with kcp itself, each of these endpoints looks like a Kubernetes cluster on its own, i.e. with <code>/api/v1</code> and API groups under <code>/apis/&lt;group&gt;/&lt;version&gt;</code>.</p> <p>The Virtual Logical Clusters are called virtual because they don\u2019t actually have their own storage, i.e. they are not  a source of truth, instead they are just proxied representations of the real logical clusters from kcp. In the process of proxying, the Virtual Workspace API server might filter the visible objects, hide certain API groups completely, or even transform objects depending on the use case (e.g. strip the sensitive data). Also, the permission semantics might be completely different from the real logical clusters.</p> <p>Virtual Workspace API servers are often coupled with certain APIs in the kcp platform. kcp ships a couple of Virtual Workspace API servers by default, served by the kcp binary itself under <code>/services/&lt;name&gt;</code>. The actual URL is announced by the API objects, e.g. in <code>APIExport.status</code> or <code>WorkspaceType.status</code>.</p> <p>Such a Virtual Workspaces concept enables some important use cases:</p> <ul> <li>a service provider might want to get a list of all instances (from all users) of the API resource they provide,   so that they can do reconciliation, have insights into how their API is used, and more,</li> <li>a <code>WorkspaceType</code> owner wants to initialize a workspace of that type before they become ready for the user. They can   write a controller that watches the Virtual Workspaces to show up, giving the necessary visibility and the necessary   access (which they otherwise would not have) to do initialization from a controller.</li> </ul> <p>Usually RBAC is used to authorize access to Virtual Workspaces, but the semantics might differ from directly accessing the underlying logical cluster. For example, a service provider usually cannot see all the objects a user might have, or even see the user's workspace at all. But when a user uses the service provider API, the service provider will see the objects for the resources they provide and potentially other related objects through the Virtual Workspaces.</p> <p>It's important to note that the Virtual Workspaces are not necessarily read-only, but that they can also mutate resources in the corresponding real logical cluster.</p> <p>Finally, for brevity, the Virtual Workspace API server is often simply called the Virtual Workspace.  The Virtual Workspace doesn't exist as a resource in kcp, it's purely a very flexible API (server) that can connect to the kcp server for the sake of gathering and mutating the needed objects.</p> <p>While kcp provides a set of default Virtual Workspaces, you can also build and run your own. The Virtual Workspaces are implemented in the Go programming language using the appropriate kcp library. Naturally, such Virtual Workspaces are not integrated in the kcp binary, but are supposed to be run as a dedicated binary alongside the kcp server.</p> <p>Note</p> <p>In a sharded setup, you need to run the Virtual Workspace (API server) for each kcp shard/server that you have.</p> <p>More information, including concrete examples and a list of frequently asked questions, can be found in the Virtual Workspaces document.</p>"},{"location":"concepts/terminology/#exportingbinding-apis","title":"Exporting/Binding APIs","text":"<p>One of the core values of the kcp project is to enable providing APIs that can be consumed inside multiple logical clusters. This is done via an exporting/binding mechanism, i.e. via API Exports and API Bindings.</p> <p>The general workflow is:</p> <ul> <li>Define your API resources through <code>APIResourceSchemes</code> similar to how you would define CustomResourceDefinitions   (CRDs)</li> <li>Create <code>APIExport</code> in the same workspace/logical cluster with a list of <code>APIResourceSchemes</code> that you want to export</li> <li>Users can create <code>APIBinding</code> in their workspaces referring to your <code>APIExport</code> if you grant the permission. This   will result in API resources that are defined in the referenced <code>APIExport</code> to be installed in the user's workspace</li> <li>Users can now create API resources of those types in their workspace. You can build and run controllers that are   going to reconcile those resources across different workspaces.</li> </ul>"},{"location":"concepts/terminology/#shard","title":"Shard","text":"<p>A failure domain within the larger control plane service that cuts across the primary functionality. Most distributed systems must separate functionality across shards to mitigate failures, and typically users interact with shards through some transparent serving infrastructure. Since the primary problem of building distributed systems is reasoning about failure domains and dependencies across them, it is critical to allow operators to effectively match shards, understand dependencies, and bring them together.</p> <p>A control plane should be shardable in a way that maximizes application SLO - gives users a tool that allows them to better define their applications not to fail.</p>"},{"location":"concepts/terminology/#sharding-in-kcp","title":"Sharding in kcp","text":"<p>In the sense of kcp, sharding involves:</p> <ul> <li>running multiple kcp server instances, each with its own etcd datastore,</li> <li>registering all kcp instances with the root shard by creating a <code>Shard</code> object in the root workspace.</li> </ul> <p>A shard hosts its own set of Workspaces. The sharding mechanism in kcp allows you to make the workspace hierarchy span many shards transparently, while ensuring you can bind APIs from logical clusters running in different shards. The Sharding documentation has more details about possibilities of sharding, and we strongly recommend reading this document if you want to shard your kcp setup.</p> <p>In a sharded setup, you'll be sending requests to a component called Front Proxy (<code>kcp-front-proxy</code>) instead to a specific shard (kcp server). Front Proxy is a shard-aware stateless proxy that's running in front of kcp API servers. Front proxy is able to determine the shard where your workspace is running and redirect/forward your request to that shard. Upon creating a workspace, you'll be sending request to front proxy, which will schedule a workspace on one of available shards.</p> <p>Note</p> <p>Depending on how you setup kcp, you might have front proxy running even though you don't have a sharded setup.</p>"},{"location":"concepts/apis/","title":"APIs in kcp","text":""},{"location":"concepts/apis/#overview","title":"Overview","text":"<p>kcp supports several built-in Kubernetes APIs, provides extensibility using CustomResourceDefinitions, and adds a new way to export custom APIs for sharing with other workspaces.</p>"},{"location":"concepts/apis/#customresourcedefinitions","title":"CustomResourceDefinitions","text":"<p>kcp, like Kubernetes, allows developers to add new APIs using CustomResourceDefinitions (CRDs). Unlike Kubernetes, kcp allows multiple copies of a CRD (e.g. <code>widgets.example.com</code>) to be installed in multiple workspaces at the same time. Each copy is entirely independent and isolated from all other copies. This means the API versions and schemas can be entirely different. kcp makes this possible because each workspace is its own isolated \"cluster.\"</p> <p>There are currently some limitations to be aware of with CRDs in kcp:</p> <ul> <li>Conversion webhooks are not supported</li> <li><code>service</code>-based validating/mutating webhooks are not supported; you must use <code>url</code>-based  <code>clientConfigs</code> instead.</li> </ul> <p>CRDs are a fantastic way to add new APIs to a workspace, but if you want to share a CRD with other workspaces, you have to install it in each workspace separately. You also need a controller that can reconcile CRs in all the workspaces where your CRD is installed, which typically means 1 distinct controller per workspace. CRDs are not \"cheap\" in the API server (each one consumes memory), and kcp offers an improved workflow that significantly reduces overhead.</p>"},{"location":"concepts/apis/#pages","title":"Pages","text":""},{"location":"concepts/apis/#built-in-apis","title":"Built-in APIs","text":""},{"location":"concepts/apis/#exporting-apis","title":"Exporting APIs","text":""},{"location":"concepts/apis/#rest-access-patterns","title":"REST Access Patterns","text":"<p>Information on the different types of URLs that kcp serves.</p>"},{"location":"concepts/apis/built-in/","title":"Built-in APIs","text":"<p>kcp includes some, but not all, of the APIs you are likely familiar with from Kubernetes:</p>"},{"location":"concepts/apis/built-in/#core-v1","title":"(core) v1","text":"<ul> <li>Namespaces</li> <li>ConfigMaps</li> <li>Secrets</li> <li>Events</li> <li>LimitRanges</li> <li>ResourceQuotas</li> <li>ServiceAccounts</li> </ul>"},{"location":"concepts/apis/built-in/#admissionregistrationk8siov1","title":"admissionregistration.k8s.io/v1","text":"<ul> <li>MutatingWebhookConfigurations</li> <li>ValidatingWebhookConfigurations</li> <li>ValidatingAdmissionPolicies</li> <li>ValidatingAdmissionPolicyBindings</li> </ul>"},{"location":"concepts/apis/built-in/#apiextensionsk8siov1","title":"apiextensions.k8s.io/v1","text":"<ul> <li>CustomResourceDefinitions</li> </ul>"},{"location":"concepts/apis/built-in/#authenticationk8siov1","title":"authentication.k8s.io/v1","text":"<ul> <li>TokenReviews</li> </ul>"},{"location":"concepts/apis/built-in/#authorizationk8siov1","title":"authorization.k8s.io/v1","text":"<ul> <li>LocalSubjectAccessReviews</li> <li>SelfSubjectAccessReviews</li> <li>SelfSubjectRulesReviews</li> <li>SubjectAccessReviews</li> </ul>"},{"location":"concepts/apis/built-in/#certificatesk8siov1","title":"certificates.k8s.io/v1","text":"<ul> <li>CertificateSigningRequests</li> </ul>"},{"location":"concepts/apis/built-in/#coordinationk8siov1","title":"coordination.k8s.io/v1","text":"<ul> <li>Leases</li> </ul>"},{"location":"concepts/apis/built-in/#eventsk8siov1","title":"events.k8s.io/v1","text":"<ul> <li>Events</li> </ul>"},{"location":"concepts/apis/built-in/#flowcontrolapiserverk8siov1beta1-temporarily-removed","title":"flowcontrol.apiserver.k8s.io/v1beta1 (temporarily removed)","text":"<ul> <li>FlowSchemas</li> <li>PriorityLevelConfigurations</li> </ul>"},{"location":"concepts/apis/built-in/#rbacauthorizationk8siov1","title":"rbac.authorization.k8s.io/v1","text":"<ul> <li>ClusterRoleBindings</li> <li>ClusterRoles</li> <li>RoleBindings</li> <li>Roles</li> </ul> <p>Notably, workload-related APIs (Pods, ReplicaSets, Deployments, Jobs, CronJobs, StatefulSets), cluster-related APIs ( Nodes), storage-related APIs (PersistentVolumes, PersistentVolumeClaims) are all missing - kcp does not include these, and it instead relies on workload clusters to provide this functionality.</p>"},{"location":"concepts/apis/exporting-apis/","title":"Exporting APIs","text":"<p>If you're looking to provide APIs that can be consumed by multiple workspaces, this section is for you!</p> <p>kcp adds new APIs that enable this new model of publishing and consuming APIs. The following diagram shows a workspace that an API provider would use to export their <code>widgets</code> API. The provider uses kcp's new <code>APIResourceSchema</code> type to define the schema for <code>widgets</code>, and the new <code>APIExport</code> type to export <code>widgets</code> to other workspaces.</p> <p>2 separate consumer workspaces use kcp's new <code>APIBinding</code> type to add the <code>widgets</code> API to their workspaces. They can then proceed to CRUD <code>widgets</code> in their workspaces, just like any other API type (e.g. <code>namespaces</code>, <code>configmaps</code>, etc.).</p> <pre><code>                                     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                                     \u2502  Consumer Workspace   \u2502\n                                     \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n                                     \u2502                       \u2502\n                                \u250c\u2500\u2500\u2500\u2500\u253c\u2500 Widgets APIBinding   |\n                                \u2502    \u2502                       \u2502\n                                \u2502    \u2502  Widget A             \u2502\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510   \u2502    \u2502  Widget B             \u2502\n\u2502  API Provider Workspace   \u2502   \u2502    \u2502  Widget C             \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524   \u2502    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\u2502                           \u2502   \u2502\n\u2502     Widgets APIExport \u25c4\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2524\n\u2502             \u2502             \u2502   \u2502\n\u2502             \u25bc             \u2502   \u2502\n\u2502 Widgets APIResourceSchema \u2502   \u2502    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518   \u2502    \u2502  Consumer Workspace   \u2502\n                                \u2502    \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n                                \u2502    \u2502                       \u2502\n                                \u2514\u2500\u2500\u2500\u2500\u253c\u2500 Widgets APIBinding   \u2502\n                                     \u2502                       \u2502\n                                     \u2502  Widget A             \u2502\n                                     \u2502  Widget B             \u2502\n                                     \u2502  Widget C             \u2502\n                                     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>Diagram: 1 APIExport consumed by 2 different workspaces (source)</p> <p>Above we talked about needing 1 controller instance per workspace, as workspace is roughly synonymous with cluster, and most/all controllers out there currently only support a single cluster. But because multiple workspaces coexist in kcp, we can be much more efficient and have 1 controller handle <code>widgets</code> in multiple workspaces!</p> <p>We achieve this using a specific URL just for your controller for your <code>APIExport</code>.</p> <p>You'll need to do a few things:</p> <ol> <li>Define 1 or more <code>APIResourceSchema</code> objects, 1 per API resource (just like you'd do with CRDs).</li> <li>Define an <code>APIExport</code> object that references all the <code>APIResourceSchema</code> instances you want to export.</li> <li>Write controllers that are multi-workspace aware.</li> </ol> <p>Let's look at each of these in more detail.</p>"},{"location":"concepts/apis/exporting-apis/#define-apiresourceschemas","title":"Define APIResourceSchemas","text":"<p>An <code>APIResourceSchema</code> defines a single custom API type. It is almost identical to a CRD, but creating an <code>APIResourceSchema</code> instance does not add a usable API to the server. By intentionally decoupling the schema definition from serving, API owners can be more explicit about API evolution. In the future, we could envision the possibility of a new <code>APIDeployment</code> resource that coordinates rolling out API updates.</p> <p>Here is an example for a <code>widgets</code> resource:</p> <pre><code>apiVersion: apis.kcp.io/v1alpha1\nkind: APIResourceSchema\nmetadata:\n  name: v220801.widgets.example.kcp.io # (1)\nspec:\n  group: example.kcp.io\n  names:\n    categories:\n    - kcp\n    kind: Widget\n    listKind: WidgetList\n    plural: widgets\n    singular: widget\n  scope: Cluster\n  versions:\n  - additionalPrinterColumns:\n    - description: The current phase\n      jsonPath: .status.phase\n      name: Phase\n      type: string\n    - jsonPath: .metadata.creationTimestamp\n      name: Age\n      type: date\n    name: v1alpha1\n    schema:\n      description: 'NOTE: full schema omitted for brevity'\n      type: object\n    served: true\n    storage: true\n    subresources:\n      status: {}\n</code></pre> <ol> <li><code>name</code> must be of the format <code>&lt;some prefix&gt;.&lt;plural resource name&gt;.&lt;group name&gt;</code>. For this example:<ul> <li><code>&lt;some prefix&gt;</code> is <code>v220801</code></li> <li><code>&lt;plural resource name&gt;</code> is <code>widgets</code></li> <li><code>&lt;group name&gt;</code> is <code>example.kcp.io</code></li> </ul> </li> </ol> <p>An <code>APIResourceSchema</code>'s <code>spec</code> is immutable; if you need to make changes to your API schema, you create a new instance.</p> <p>Once you've created at least one <code>APIResourceSchema</code>, you can proceed with creating your <code>APIExport</code>.</p>"},{"location":"concepts/apis/exporting-apis/#define-your-apiexport","title":"Define Your APIExport","text":"<p>An <code>APIExport</code> is the way an API provider makes one or more APIs (coming from <code>APIResourceSchemas</code>) available to workspaces.</p> <p>Here is an example <code>APIExport</code> called <code>example.kcp.io</code> that exports 1 resource: <code>widgets</code>.</p> <pre><code>apiVersion: apis.kcp.io/v1alpha1\nkind: APIExport\nmetadata:\n  name: example.kcp.io\nspec:\n  latestResourceSchemas:\n  - v220801.widgets.example.kcp.io\n</code></pre> <p>At a minimum, you specify the names of the <code>APIResourceSchema</code>s you want to export in the <code>spec.latestResourceSchemas</code> field. The <code>APIResourceSchemas</code> must be in the same workspace as the <code>APIExport</code> (and therefore no workspace name or path is required here).</p> <p>You can optionally configure the following additional aspects of an <code>APIExport</code>:</p> <ul> <li>its identity</li> <li>what permissions consumers of your exported APIs are granted</li> <li>API resources from other sources (built-in types and/or from other <code>APIExport</code>s) that your controllers need to access   for your service to function correctly</li> </ul> <p>We'll talk about each of these next.</p>"},{"location":"concepts/apis/exporting-apis/#apiexport-identity","title":"APIExport Identity","text":"<p>Each API resource type is defined by an API group name and a resource name. Each API resource type can further be distinguished by an API version. For example, the <code>roles.rbac.authorization.k8s.io</code> resource that we frequently interact with is in the <code>rbac.authorization.k8s.io</code> API group and its resource name is <code>roles</code>.</p> <p>In a Kubernetes cluster, it is impossible to define the same API <code>&lt;group&gt;,&lt;resource&gt;,&lt;version&gt;</code> multiple times; i.e., it does not support competing definitions for <code>rbac.authorization.k8s.io,roles,v1</code> or any other API resource type.</p> <p>In kcp, however, it is possible for multiple API providers to each define the same API <code>&lt;group&gt;,&lt;resource&gt;,&lt;version&gt;</code>, without interference! This is where <code>APIExport</code> \"identity\" becomes critical.</p> <p>When you create an <code>APIExport</code> and you don't specify its identity (as is the case in the <code>example.kcp.io</code> example above), kcp automatically generates one for you. The identity is always stored in a secret in the same workspace as the <code>APIExport.</code> By default, it is created in the <code>kcp-system</code> namespace in a secret whose name is the name of the <code>APIExport</code>.</p> <p>An <code>APIExport</code>'s identity is similar to a private key; you should never share it with anyone.</p> <p>The identity's hash is similar to a public key; it is not private and there are times when other consumers of your exported APIs need to know and reference it.</p> <p>Given 2 Workspaces, each with its own <code>APIExport</code> that exports <code>widgets.example.kcp.io</code>, kcp uses the identity hash (in a mostly transparent manner) to ensure the correct instances associated with the appropriate <code>APIResourceSchema</code> are served to clients. See Run Your Controller for more information.</p>"},{"location":"concepts/apis/exporting-apis/#permission-claims","title":"Permission Claims","text":"<p>When a consumer creates an <code>APIBinding</code> that binds to an <code>APIExport</code>, the API provider who owns the <code>APIExport</code> implicitly has access to instances of the exported APIs in the consuming workspace. There are also times when the API provider needs to access additional resource data in a consuming workspace. These resources might come from other APIExports the consumer has created <code>APIBindings</code> for, or from APIs that are built in to kcp. The API provider  requests access to these additional resources by adding <code>PermissionClaims</code> for the desired API's group, resource, and  identity hash to their <code>APIExport</code>. Let's take the example <code>APIExport</code> from above and add permission claims for  <code>ConfigMaps</code> and <code>Things</code>:</p> <pre><code>apiVersion: apis.kcp.io/v1alpha1\nkind: APIExport\nmetadata:\n  name: example.kcp.io\nspec:\n  latestResourceSchemas:\n  - v220801.widgets.example.kcp.io\n  permissionClaims:\n  - group: \"\" # (1)\n    resource: configmaps\n    resourceSelector: # (2)\n    - namespace: example-system\n      name: my-setup\n  - group: somegroup.kcp.io\n    resource: things\n    identityHash: 5fdf7c7aaf407fd1594566869803f565bb84d22156cef5c445d2ee13ac2cfca6 # (3)\n    all: true # (4)\n</code></pre> <ol> <li>This is how you specify the core API group</li> <li>You can claim access to one or more resource instances by namespace and/or name</li> <li>To claim another exported API, you must include its <code>identityHash</code></li> <li>If you aren't claiming access to individual instances, you must specify <code>all</code> instead</li> </ol> <p>This is essentially a request from the APIProvider, asking each consumer to grant permission for the claimed  resources. If the consumer does not accept a permission claim, the API Provider is not allowed to access the claimed resources. Consumer acceptance of permission claims is part of the <code>APIBinding</code> spec. For more details, see the  section on APIBindings.</p>"},{"location":"concepts/apis/exporting-apis/#maximal-permission-policy","title":"Maximal Permission Policy","text":"<p>If you want to set an upper bound on what is allowed for a consumer of your exported APIs. you can set a \"maximal permission policy\" using standard RBAC resources. This is optional; if the policy is not set, no upper bound is applied, and a consuming user is authorized based on the RBAC configuration in the consuming workspace.</p> <p>The maximal permission policy consists of RBAC <code>(Cluster)Roles</code> and <code>(Cluster)RoleBindings</code>. Incoming requests to a  workspace that binds to an APIExport are additionally checked against these rules, with the username and groups prefixed with <code>apis.kcp.io:binding:</code>.</p> <p>For example: we have an <code>APIExport</code> in the <code>root</code> workspace called <code>tenancy.kcp.io</code> that provides APIs for <code>workspaces</code> and <code>workspacetypes</code>:</p> <pre><code>apiVersion: apis.kcp.io/v1alpha1\nkind: APIExport\nmetadata:\n  name: tenancy.kcp.io\nspec:\n  latestResourceSchemas:\n  - v230110-89146c99.workspacetypes.tenancy.kcp.io\n  - v230116-832a4a55d.workspaces.tenancy.kcp.io\n  maximalPermissionPolicy:\n    local: {} # (1)\n</code></pre> <ol> <li><code>local</code> is the only supported option at the moment. \"Local\" means the RBAC policy is defined in the same     workspace as the <code>APIExport</code>.</li> </ol> <p>We don't want users to be able to mutate the <code>status</code> subresource, so we set up a maximal permission policy to limit what users can do:</p> Tenancy APIExport Maximal Permission Policy ClusterRole<pre><code>apiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRole\nmetadata:\n  name: system:kcp:apiexport:tenancy:maximal-permission-policy\nrules:\n- apiGroups: [\"tenancy.kcp.io\"]\n  verbs: [\"*\"] # (1)\n  resources:\n  - workspaces\n  - workspacetypes\n- apiGroups: [\"tenancy.kcp.io\"]\n  verbs: [\"list\",\"watch\",\"get\"] # (2)\n  resources:\n  - workspaces/status\n  - workspacetypes/status\n</code></pre> <ol> <li>Users can perform any/all actions on the main resources</li> <li>Users can only get/list/watch the status subresources</li> </ol> Tenancy APIExport Maximal Permission Policy ClusterRoleBinding<pre><code>apiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRoleBinding\nmetadata:\n  name: system:kcp:authenticated:apiexport:tenancy:maximal-permission-policy\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: ClusterRole\n  name: system:kcp:apiexport:tenancy:maximal-permission-policy\nsubjects:\n- apiGroup: rbac.authorization.k8s.io\n  kind: Group\n  name: apis.kcp.io:binding:system:authenticated # (1)\n</code></pre> <ol> <li>Note the <code>apis.kcp.io:binding:</code> prefix; this identifies this <code>ClusterRoleBinding</code> as part of the maximal     permission policy. It applies to <code>system:authenticated</code>.</li> </ol> <p>Now imagine a user named <code>unicorn</code> with group <code>system:authenticated</code>. They create a workspace named  <code>magic</code> and bind to the <code>tenancy.kcp.io</code> APIExport from the workspace <code>root</code>. What actions <code>unicorn</code> is allowed to  perform in the <code>magic</code> workspace must be granted by both:</p> <ol> <li>the standard RBAC authorization flow in the <code>magic</code> workspace; i.e., <code>(Cluster)Roles</code> and <code>(Cluster)RoleBindings</code>     in the <code>magic</code> workspace itself, and</li> <li>the maximal permission policy RBAC settings configured in the <code>root</code> workspace for the <code>tenancy</code> APIExport</li> </ol>"},{"location":"concepts/apis/exporting-apis/#build-your-controller","title":"Build Your Controller","text":"<p>Controllers to reconcile resources backed by <code>APIExports</code> can be developed with kcp's controller-runtime fork. The fork follows upstream and allows to write both kcp-aware and vanilla Kubernetes controllers at the same time. There is an example controller that serves as reference for implementations.</p> <p>When reconciling exported APIs, controllers usually interact with the API resources of said <code>APIExport</code> through a virtual workspace. Because kcp can be sharded, it is possible that there are multiple virtual workspaces (across the different shards) that show a subset of API resources created across a sharded kcp instance.</p>"},{"location":"concepts/apis/exporting-apis/#endpoint-slices","title":"Endpoint Slices","text":"<p>An API provider that implements a controller should also create something called an <code>APIExportEndpointSlice</code> alongside their <code>APIExport</code>. This will \"slice and dice\" the list of available virtual workspace endpoints. For example, a Partition can be provided to restrict provided virtual workspace URLs to a part of the sharded kcp setup.</p> <p>This is what the resource looks like:</p> APIExportEndpointSlice for a partition's virtual workspace endpoints<pre><code>kind: APIExportEndpointSlice\napiVersion: apis.kcp.io/v1alpha1\nmetadata:\n    name: example-gcp-europe\nspec:\n    export:\n        path: root # (1)\n        name: example.kcp.io\n    # optional\n    partition: cloud-region-gcp-europe-xdfgs\n</code></pre> <ol> <li>The workspace path at which the <code>APIExport</code> sits. This can be in a different workspace than the <code>APIExportEndpointSlice</code></li> </ol> <p>Based on this, only virtual workspace URLs for the <code>cloud-region-gcp-europe-xdfgs</code> partition will be populated into this endpoint slice. If you wish to get a full list of virtual workspace endpoints, just omit the <code>spec.partition</code> field from the example above.</p> <p>Once created, the <code>status</code> field of the <code>APIExportEndpointSlice</code> will be populated according to its specification:</p> <pre><code>$ kubectl get apiexportendpointslice example-gcp-europe -o yaml\nkind: APIExportEndpointSlice\napiVersion: apis.kcp.io/v1alpha1\nmetadata:\n    name: example-gcp-europe\n...\nstatus:\n    endpoints\n        - url: https://host1:6443/services/apiexport/root/example.kcp.io\n        - url: https://host2:6443/services/apiexport/root/example.kcp.io\n...\n</code></pre> <p>Controller implementations need to watch the <code>APIExportEndpointSlice</code> and use all endpoints provided by the status. Each virtual workspace endpoint is its own Kubernetes-like API endpoint. This means that it's likely necessary to spin up managers that each \"own\" one of the endpoints and process them.</p>"},{"location":"concepts/apis/exporting-apis/#authorization","title":"Authorization","text":"<p>Controllers need to run with proper authorization. That means they need to be able to access the <code>APIExportEndpointSlice</code> above (read-only access is sufficient). The <code>ClusterRole</code> in the workspace hosting the <code>APIExportEndpointSlice</code> could look like this:</p> ClusterRole for APIExportEndpointSlice access<pre><code>kind: ClusterRole\napiVersion: rbac.authorization.k8s.io/v1\nmetadata:\n  name: example.kcp.io:access-endpointslice\nrules:\n- apiGroups: [\"apis.kcp.io\"]\n  verbs: [\"list\", \"watch\", \"get\"]\n  resources:\n  - apiexportendpointslices\n</code></pre> <p>In addition, the user used by the controller needs to be authorized to access the content of the <code>APIExport</code> virtual workspace. For that, authorization needs to be given for the <code>content</code> subresource of the <code>APIExport</code> resource (which might exist in a different workspace than the <code>APIExportEndpointSlice</code>). A suitable <code>ClusterRole</code> would look like this:</p> ClusterRole for APIExport content access<pre><code>kind: ClusterRole\napiVersion: rbac.authorization.k8s.io/v1\nmetadata:\n  name: example.kcp.io:access-content\nrules:\n- apiGroups: [\"apis.kcp.io\"]\n  verbs: [\"*\"] # (1)\n  resources:\n  - apiexports/content\n  resourceNames:\n  - example.kcp.io\n</code></pre> <ol> <li>Allows read and write access to resources created from this <code>APIExport</code> across all workspaces</li> </ol>"},{"location":"concepts/apis/exporting-apis/#bind-to-exported-apis","title":"Bind to Exported APIs","text":""},{"location":"concepts/apis/exporting-apis/#apibinding","title":"APIBinding","text":"<p>TODO - \"imports\" all the <code>APIResourceSchemas</code> defined in an <code>APIExport</code> into its workspace - Also provides access to the group/resources defined in an <code>APIExport</code>'s <code>PermissionClaims</code> slice - APIs are \"imported\" by accepting <code>PermissionClaims</code> within the <code>APIBinding</code> for each of the <code>APIExport</code>'s group/resources - The consumer of the <code>APIExport</code> must be granted permission to the <code>bind</code> verb on that <code>APIExport</code> in order to create an <code>APIBinding</code> - An <code>APIBinding</code> is bound to a specific <code>APIExport</code> and associated <code>APIResourceSchema</code>s via the <code>APIBinding.Status.BoundResources</code> field, which will hold the identity information to precisely identify relevant objects. - how do I correctly reference an APIExport?</p>"},{"location":"concepts/apis/rest-access-patterns/","title":"REST Access Patterns","text":"<p>This describes the various REST access patterns the kcp apiserver supports.</p>"},{"location":"concepts/apis/rest-access-patterns/#typical-requests-for-resources-in-a-workspace","title":"Typical requests for resources in a workspace","text":"<p>These requests are all prefixed with <code>/clusters/&lt;workspace path | logical cluster name&gt;</code>. Here are some example URLs:</p> <ul> <li><code>GET /clusters/root/apis/tenancy.kcp.io/v1alpha1/workspaces</code> - lists all kcp Workspaces in the    <code>root</code> workspace.</li> <li><code>GET /clusters/root:compute/api/v1/namespaces/test</code> - gets the namespace <code>test</code> from the <code>root:compute</code> workspace</li> <li><code>GET /clusters/yqzkjxmzl9turgsf/api/v1/namespaces/test</code> - same as above, using the logical cluster name for    <code>root:compute</code></li> </ul>"},{"location":"concepts/apis/rest-access-patterns/#typical-requests-for-resources-through-the-apiexport-virtual-workspace","title":"Typical requests for resources through the APIExport virtual workspace","text":"<p>An APIExport provides a view into workspaces that contain APIBindings that are bound to the APIExport. This allows  the service provider - the owner of the APIExport - to access data in its consumers' workspaces. Here is an example  APIExport virtual workspace URL:</p> <pre><code>/services/apiexport/root:my-ws/my-service/clusters/root:consumer-1/apis/example.kcp.io/v1/widgets\n</code></pre> <p>Let's break down the segments in the URL path:</p> Segment Description /services/apiexport Prefix for every APIExport virtual workspace root:my-ws Workspace where the APIExport lives my-service Name of the APIExport clusters/root:consumer-1 Consumer workspace path apis/example.kcp.io/v1/widgets Normal API path"},{"location":"concepts/apis/rest-access-patterns/#setting-up-shared-informers-for-a-virtual-workspace","title":"Setting up shared informers for a virtual workspace","text":"<p>A virtual workspace typically allows the service provider to set up shared informers that can list and watch  resources across all the consumer workspaces bound to or supported by the virtual workspace. For example, the  APIExport virtual workspace lets you inform across all workspaces that have an APIBinding to your APIExport. The  syncer virtual workspace lets a syncer inform across all workspaces that have a Placement on the syncer's associated  SyncTarget.</p> <p>To set up shared informers to span multiple workspaces, you use a special cluster called the wildcard cluster,  denoted by <code>*</code>. An example URL you would use when constructing a shared informer in this manner might be:</p> <pre><code>/services/apiexport/root:my-ws/my-service/clusters/*/api/v1/configmaps\n</code></pre> <p>or, to set up an entire shared informer factory, you give it the wildcard base:</p> <pre><code>/services/apiexport/root:my-ws/my-service/clusters/*\n</code></pre>"},{"location":"concepts/authorization/","title":"Authorization","text":"<p>Within workspaces, KCP implements the same RBAC-based authorization mechanism as Kubernetes. Other authorization schemes (i.e. ABAC) are not supported. Generally, the same (cluster) role and (cluster) role binding principles apply exactly as in Kubernetes.</p> <p>In addition, additional RBAC semantics is implemented cross-workspaces, namely the following:</p> <ul> <li>Workspace Content access: the user needs <code>access</code> permissions to a workspace or be even admin.</li> <li>for some resources, additional permission checks are performed, not represented by local or Kubernetes standard RBAC rules; for example<ul> <li>workspace creation checks for organization membership (see above).</li> <li>workspace creation checks for <code>use</code> verb on the <code>WorkspaceType</code>.</li> <li>API binding via APIBinding objects requires verb <code>bind</code> access to the corresponding <code>APIExport</code>.</li> </ul> </li> <li>System Workspaces access: system workspaces are prefixed with <code>system:</code> and are not accessible by users.</li> </ul> <p>The details of the authorizer chain are documented in Authorizers.</p>"},{"location":"concepts/authorization/#pages","title":"Pages","text":""},{"location":"concepts/authorization/#authorizers","title":"Authorizers","text":"<p>How to authorize requests to kcp.</p>"},{"location":"concepts/authorization/authorizers/","title":"Authorizers","text":"<p>In kcp, a request has four different ways of being admitted:</p> <ul> <li>It can be made to one of the preconfigured paths that do not require authorization, like <code>/healthz</code>.</li> <li>It can be performed by a user in one of the configured always-allow groups, by default <code>system:masters</code>.</li> <li>It can pass through the RBAC chain and match configured Roles and ClusterRoles.</li> <li>It can be permitted by an external HTTPS webhook backend.</li> </ul> <p>They are related in the following way: <pre><code>graph TD\n    start(Request):::state --&gt; main_alt[/one of\\]:::or\n    main_alt --&gt; aaga[Always Allow Groups Auth]\n    main_alt --&gt; aapa[Always Allow Paths Auth]\n    main_alt --&gt; rga[Required Groups Auth]\n    main_alt --&gt; wa[Webhook Auth]\n\n\n    aaga --&gt; decision(Decision):::state\n    aapa --&gt; decision\n\n    subgraph \"RBAC\"\n    rga --&gt; wca[Workspace Content Auth]\n    wca --&gt; scrda[System CRD Auth]\n    scrda --&gt; mppa[Max. Permission Policy Auth]\n\n    mppa --- mppa_alt[/one of\\]:::or\n    mppa_alt --&gt; lpa[Local Policy Auth]\n    mppa_alt --&gt; gpa[Global Policy Auth]\n    mppa_alt --&gt; bpa[Bootstrap Policy Auth]\n    end\n\n    lpa --&gt; decision\n    gpa --&gt; decision\n    bpa --&gt; decision\n    wa --&gt; decision\n\n    classDef state color:#F77\n    classDef or fill:none,stroke:none</code></pre></p> <p>View graph on Kroki</p>"},{"location":"concepts/authorization/authorizers/#always-allow-paths-authorizer","title":"Always Allow Paths Authorizer","text":"<p>Like in vanilla Kubernetes, this authorizer always grants access to the configured URL paths. This is used for the health and liveness checks of kcp.</p>"},{"location":"concepts/authorization/authorizers/#always-allow-groups-authorizer","title":"Always Allow Groups Authorizer","text":"<p>This authorizer always permits access if the user is in one of the configured groups. By default this only includes the <code>system:masters</code> group.</p>"},{"location":"concepts/authorization/authorizers/#rbac-chain","title":"RBAC Chain","text":"<p>The primary authorization flow is handled by a sequence of RBAC-based authorizers that a request must satisfy all in order to be granted access.</p> <p>The following authorizers work together to implement RBAC in kcp:</p> Authorizer Description Workspace content authorizer validates that the user has <code>access</code> permission to the workspace Required groups authorizer validates that the user is in the annotation-based list of groups required for a workspace System CRD authorizer prevents undesired updates to certain core resources, like the status subresource on APIBindings Maximal permission policy authorizer validates the maximal permission policy RBAC policy in the API exporter workspace Local Policy authorizer validates the RBAC policy in the workspace that is accessed Global Policy authorizer validates the RBAC policy in the workspace that is accessed across shards Kubernetes Bootstrap Policy authorizer validates the RBAC Kubernetes standard policy"},{"location":"concepts/authorization/authorizers/#required-groups-authorizer","title":"Required Groups Authorizer","text":"<p>A <code>authorization.kcp.io/required-groups</code> annotation can be added to a LogicalCluster to specify additional groups that are required to access a workspace for a user to be member of. The syntax is a disjunction (separator <code>,</code>) of conjunctions (separator <code>;</code>).</p> <p>For example, <code>&lt;group1&gt;;&lt;group2&gt;,&lt;group3&gt;</code> means that a user must be member of <code>&lt;group1&gt;</code> AND <code>&lt;group2&gt;</code>, OR of <code>&lt;group3&gt;</code>.</p> <p>The annotation is copied onto sub-workspaces during workspace creation, but is then not updated automatically if it's changed.</p>"},{"location":"concepts/authorization/authorizers/#workspace-content-authorizer","title":"Workspace Content Authorizer","text":"<p>The workspace content authorizer checks whether the user is granted access to the workspace. Access is granted access through <code>verb=access</code> non-resource permission to <code>/</code> inside of the workspace.</p> <p>The ClusterRole <code>system:kcp:workspace:access</code> is pre-defined which makes it easy to give a user access through a ClusterRoleBinding inside of the workspace.</p> <p>For example, to give a user <code>user1</code> access, create the following ClusterRoleBinding:</p> <pre><code>apiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRoleBinding\nmetadata:\n  name: example-access\nsubjects:\n- kind: User\n  name: user1\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: ClusterRole\n  name: system:kcp:workspace:access\n</code></pre> <p>To give a user <code>user1</code> admin access, create the following ClusterRoleBinding:</p> <pre><code>apiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRoleBinding\nmetadata:\n  name: example-admin\nsubjects:\n- kind: User\n  name: user1\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: ClusterRole\n  name: cluster-admin\n</code></pre> <p>A service-account defined in a workspace implicitly is granted access to it.</p> <p>A service-account defined in a different workspace is NOT given access to it.</p> <p>Note</p> <p>By default, workspaces are only accessible to a user if they are in <code>Ready</code> phase. Workspaces that are initializing can be accessed only by users that are granted <code>admin</code> verb on the <code>workspaces/content</code> resource in the parent workspace.</p> <p>Service accounts declared within a workspace don't have access to initializing workspaces.</p>"},{"location":"concepts/authorization/authorizers/#system-crd-authorizer","title":"System CRD Authorizer","text":"<p>This small authorizer simply prevents updates to the <code>status</code> subresource on APIExports or APIBindings. Note that this authorizer does not validate changes to the CustomResourceDefitions themselves, but to objects from those CRDs instead.</p>"},{"location":"concepts/authorization/authorizers/#maximal-permission-policy-authorizer","title":"Maximal Permission Policy Authorizer","text":"<p>If the requested resource type is part of an API binding, then this authorizer verifies that the request is not exceeding the maximum permission policy of the related API export. Currently, the \"local policy\" maximum permission policy type is supported.</p>"},{"location":"concepts/authorization/authorizers/#local-policy","title":"Local Policy","text":"<p>The local maximum permission policy delegates the decision to the RBAC of the related API export. To distinguish between local RBAC role bindings in that workspace and those for this these maximum permission policy, every name and group is prefixed with <code>apis.kcp.io:binding:</code>.</p> <p>Example: Given an APIBinding for type <code>foo</code> declared in workspace <code>consumer</code> that refers to an APIExport declared in workspace <code>provider</code> and a user <code>user-1</code> having the group <code>group-1</code> requesting a <code>create</code> of <code>foo</code> in the <code>default</code> namespace in the <code>consumer</code> workspace, this authorizer verifies that <code>user-1</code> is allowed to execute this request by delegating to <code>provider</code>'s RBAC using prefixed attributes.</p> <p>Here, this authorizer prepends the <code>apis.kcp.io:binding:</code> prefix to the username and all groups the user belongs to. Using prefixed attributes prevents RBAC collisions i.e. if <code>user-1</code> is granted to execute requests within the <code>provider</code> workspace directly.</p> <p>For the given example RBAC request looks as follows:</p> <ul> <li>Username: <code>apis.kcp.io:binding:user-1</code></li> <li>Groups: [<code>apis.kcp.io:binding:group-1</code>]</li> <li>Resource: <code>foo</code></li> <li>Namespace: <code>default</code></li> <li>Workspace: <code>provider</code></li> <li>Verb: <code>create</code></li> </ul> <p>The following Role and RoleBinding declared within the <code>provider</code> workspace will grant access to the request:</p> <pre><code>apiVersion: rbac.authorization.k8s.io/v1\nkind: Role\nmetadata:\n  name: foo-creator\n  clusterName: provider\nrules:\n- apiGroups:\n  - foo.api\n  resources:\n  - foos\n  verbs:\n  - create\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: RoleBinding\nmetadata:\n  name: user-1-foo-creator\n  namespace: default\n  clusterName: provider\nsubjects:\n- kind: User\n  name: apis.kcp.io:binding:user-1\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: Role\n  name: foo-creator\n</code></pre> <p>Note</p> <p>The same authorization scheme is enforced when executing the request of a claimed resource via the virtual APIExport API server, i.e. a claimed resource is bound to the same maximal permission policy. Only the actual owner of that resources can go beyond that policy.</p> <p>TBD: Example</p>"},{"location":"concepts/authorization/authorizers/#local-policy-authorizer","title":"Local Policy Authorizer","text":"<p>This authorizer ensures that RBAC rules contained within a workspace are being applied and work just like in a regular Kubernetes cluster.</p> <p>It is possible to bind to Roles and ClusterRoles in the bootstrap policy from a local policy's <code>RoleBinding</code> or <code>ClusterRoleBinding</code>, for example the <code>system:kcp:workspace:access</code> ClusterRole exists in the <code>system:admin</code> logical cluster, but can still be bound from without any other logical cluster.</p>"},{"location":"concepts/authorization/authorizers/#global-policy-authorizer","title":"Global Policy Authorizer","text":"<p>This authorizer works identically to the Local Policy Authorizer, just with the difference that it uses a global (i.e. across shards) getter for Roles and RoleBindings.</p>"},{"location":"concepts/authorization/authorizers/#bootstrap-policy-authorizer","title":"Bootstrap Policy Authorizer","text":"<p>The bootstrap policy authorizer works just like the local authorizer but references RBAC rules defined in the <code>system:admin</code> system workspace. This workspace is where the classic Kubernetes RBAC like the <code>cluster-admin</code> ClusterRole is being defined and the policy defined in this workspace applies to every workspace in a kcp shard.</p>"},{"location":"concepts/authorization/authorizers/#webhook-authorizer","title":"Webhook Authorizer","text":"<p>This authorizer can be enabled by providing the <code>--authorization-webhook-config-file</code> flag to the kcp process and works identically to how it works in vanilla Kubernetes.</p> <p>The given configuration file must be of the kubeconfg format and point to an HTTPS server, potentially including certificate information as needed:</p> <pre><code>apiVersion: v1\nkind: Config\nclusters:\n  - name: webhook\n    cluster:\n      server: https://localhost:8080/\ncurrent-context: webhook\ncontexts:\n  - name: webhook\n    context:\n      cluster: webhook\n</code></pre> <p>The webhook will receive every authorization request made in kcp, including internal ones. This means if the webhook is badly configured, it can even prevent kcp from starting up successfully, but on the other hand this allows a lot of influence over the authorization in kcp.</p> <p>The webhook will receive JSON-marshalled <code>SubjectAccessReview</code> objects, that (compared to vanilla Kubernetes) include the name of target logical cluster as an <code>extra</code> field, like so:</p> <pre><code>{\n  \"apiVersion\": \"authorization.k8s.io/v1beta1\",\n  \"kind\": \"SubjectAccessReview\",\n  \"spec\": {\n    \"resourceAttributes\": {\n      \"namespace\": \"kittensandponies\",\n      \"verb\": \"get\",\n      \"group\": \"unicorn.example.org\",\n      \"resource\": \"pods\"\n    },\n    \"user\": \"jane\",\n    \"group\": [\n      \"group1\",\n      \"group2\"\n    ],\n    \"extra\": {\n      \"authorization.kubernetes.io/cluster-name\": [\"root\"]\n    }\n  }\n}\n</code></pre> <pre><code>The extra field will contain the logical cluster _name_ (e.g. o43u2gh528rtfg721rg92), not the human-readable path. Webhooks need to resolve the name to a path themselves if necessary.\n</code></pre>"},{"location":"concepts/authorization/authorizers/#authorizer-order","title":"Authorizer Order","text":"<p>By default, the authorizers are evaluated in the following order:</p> <ol> <li>Always Allow Groups Authorizer (<code>AlwaysAllowGroups</code>)</li> <li>Always Allow Paths Authorizer (<code>AlwaysAllowPaths</code>)</li> <li>RBAC Chain (<code>RBAC</code>)</li> <li>Webhook Authorizer (<code>Webhook</code>)</li> </ol> <p>The order in which authorizers are evaluated can be configured. This allows administrators to customize the authorization flow. It can be done by setting <code>--authorization-order</code> flag in the kcp server. This flag accepts a comma-separated list of authorizer names, which will be evaluated in the specified order.</p> <p>Here is an example on how to enable the Webhook to be the first one in the authorizers chain: <pre><code>--authorization-order=Webhook,AlwaysAllowGroups,AlwaysAllowPaths,RBAC\n</code></pre></p>"},{"location":"concepts/authorization/authorizers/#scopes","title":"Scopes","text":"<p>Scopes are a way to limit the access of a user to a specific logical cluster. Scopes are (optionally) attached to the user identity by setting the <code>authentication.kcp.io/scopes: cluster:&lt;logical-cluster&gt;,...</code> extra field. The scope is then checked by the authorizers. For example:</p> <p><pre><code>user: user1\ngroups: [\"group1\"]\nextra:\n  authentication.kcp.io/scopes:\n  - cluster:logical-cluster-1\n</code></pre> This user will only be allowed to access resources in <code>logical-cluster-1</code>, falling back to be considered as user <code>system:anonymous</code> with group <code>system:authenticated</code> in all other logical clusters.</p> <p>Each extra field can contain multiple scopes, separated by a comma: <pre><code>user: user1\ngroups: [\"group1\"]\nextra:\n  authentication.kcp.io/scopes:\n  - cluster:logical-cluster-1,cluster:logical-cluster-2\n</code></pre> This user is allowed to operate in both <code>logical-cluster-1</code> and <code>logical-cluster-2</code>, falling back to be considered as user <code>system:anonymous</code> with group <code>system:authenticated</code> in all other logical clusters.</p> <p>If multiple <code>authentication.kcp.io/scopes</code> values are set, the intersection is taken: <pre><code>user: user1\ngroups: [\"group1\"]\nextra:\n  authentication.kcp.io/scopes:\n  - cluster:logical-cluster-1,cluster:logical-cluster-2\n  - cluster:logical-cluster-2,cluster:logical-cluster-3\n</code></pre> This user is only allowed to operate in <code>logical-cluster-2</code>, falling back to be considered as user <code>system:anonymous</code> with group <code>system:authenticated</code> in all other logical clusters.</p> <p>The intersection can be empty, in which case it falls back in every logical cluster.</p> <p>When impersonating a user in a logical cluster, the resulting user identity is scoped to the logical cluster the impersonation is happening in.</p> <p>A scope mismatch does not invalidate the warrants (see next section) of a user.</p>"},{"location":"concepts/authorization/authorizers/#warrants","title":"Warrants","text":"<p>Warrants are a way to grant extra access to a user. It can be limited by the scope of a logical cluster. A warrant is attached by adding a <code>authorization.kcp.io/warrant</code> extra field to the user identity with a JSON-encoded user info, and the limiting logical cluster set as <code>authentication.kcp.io/scopes: cluster:&lt;logical-cluster&gt;</code>. in the embedded user info's extra. The warrant is then checked by the authorizers in the chain of every step if the primary users is not allowed. For example:</p> <pre><code>user: user1\ngroups: [\"group1\"]\nextra:\n  authorization.kcp.io/warrant: |\n    {\n      \"user\": \"user2\",\n      \"groups\": [\"group2\"],\n      \"extra\": {\n        \"authentication.kcp.io/scopes\": \"cluster:logical-cluster-1\"\n      }\n    }\n</code></pre> <p>This warrant allows <code>user1</code> to act under the permissions of <code>user2</code> in <code>logical-cluster-1</code> if <code>user1</code> is not allowed to act as <code>user2</code> in the first place.</p> <p>Note that a warrant only allow to act under the permissions of the warrant user, but not to act as the warrant user itself. E.g. in auditing or admission control, the primary user is still the one that is acting.</p> <p>Warrants can be nested, i.e. a warrant can contain another warrant.</p>"},{"location":"concepts/miscellaneous/","title":"Miscellaneous","text":""},{"location":"concepts/miscellaneous/#pages","title":"Pages","text":""},{"location":"concepts/miscellaneous/#architecture-a-brain-dump","title":"Architecture \u2013 A Brain Dump","text":"<p>A brain dump of thoughts behind KCP's architecture.</p>"},{"location":"concepts/miscellaneous/braindump/","title":"Architecture \u2013 A Brain Dump","text":"<p>Note</p> <p>This document is a brain dump of thoughts behind KCP's architecture. It's a work in progress and may contain incomplete or unpolished ideas. It was recorded through ChatGPT (not generated), and hence might have a conversational tone (GPT's summarizing responses have been removed) and might contain mistakes.</p>"},{"location":"concepts/miscellaneous/braindump/#kcp-overview","title":"KCP Overview","text":"<p>KCP is an extension or a fork of the KubeAPI server, it's adding a concept called a logical cluster, or merely it's called a workspace, a workspace concept to one instance of KCP. When I talk about one instance, it's actually one shard, and there can be multiple shards in a system. And the paper will be about the architecture to make that possible while giving the user</p>"},{"location":"concepts/miscellaneous/braindump/#every-workspace-looks-like-a-kubernetes-cluster-from-the-outside","title":"Every workspace looks like a Kubernetes cluster from the outside.","text":"<p>Every workspace looks like a Kubernetes cluster from the outside, at least when looking at the main APIs which are independent of container management. For example, every workspace has its own discovery information, its own OpenAPI spec, and of course, its own API groups implementation. Every workspace has certain kinds of APIs which are used to from Kubernetes, APIs which are like namespaces, config maps, secrets, and many more. And it has the semantics you would expect, like namespace deletion is implemented, garbage collection is implemented, airbag permission management is implemented the same way as in Kubernetes. The big difference to Kubernetes is that one instance of KCP, we call that one shard in this context, can host an arbitrary number of workspaces, each being logically independent.</p> <p>If you look on one shard, one instance of KCP and a number of workspaces hosted by that shard. Between workspaces, there can be interactions. Interactions in the sense of one workspace can export APIs and another workspace can bind to those APIs. And the objects to define those two concepts in KCP are named like that, API export and API binding. With a small number of exceptions, everything in KCP is implemented using those API export, API binding concepts. Even the workspace concept itself is based on an API export. The workspaces of KCP have a structure, like they live in KCP as a system, as a platform. They are ordered in a hierarchy, so they are placed in a hierarchy in a tree-like structure, similar to directories in a Linux or Windows file system. Every directory is here a workspace. Exports and bindings connect those. There's one very special workspace called the root workspace. The root workspace hosts the API exports of the main KCP APIs. For example, the tendency KCP or API group, with the workspace object, the workspace kind as a primary type, is exported from the root workspace. So the root workspace plays a crucial role in bootstrapping a KCP instance.</p> <p>The workspace hierarchy is not established through exports and bindings. The workspace hierarchy is established by defining a child workspace within a parent workspace by creating a workspace object. So in this sense, this is again very similar to a file system hierarchy in Linux.</p> <p>I want to dive a little bit into the Workspace concept, what is behind it, how it's implemented. As I described, Workspace objects within parent Workspaces define the hierarchy. Within the hierarchy, you get a path, a path like a file system path. In KCP, we use a colon as separator. A normal example of a path is starting with root, colon, team name, colon, and, for example, application name. So it's root, team A, application Z, as an example. The path is constructed and also reconstructed just by this nesting of Workspaces. It's not inherent in how the data is stored, stored in the storage layer in etcd or in the kind-based SQL database. Behind the scenes, every Workspace path is mapped to a logical cluster. A logical cluster is identified by some hash value. So there's a hash, some random character string. And this is unique. So it's like UID. It's unique throughout the KCP system. And that key is used within the etcd or the kind-based key structure. So it's part of the keys in the storage. This is used to separate values, so objects, of Workspaces which live in different logical clusters. When you talk to KCP, you as a client from outside, so similarly as you would talk to a kube cluster, but now you talk to a Workspace, you can talk to it through the path, or you can talk to it through the logical cluster, identify as this random UID string.</p> <p>The mapping from a workspace path to the Logical Cluster ID is done through reading the leaf object of the path, so the last workspace object, the last component of the path. Inside of that, the UID of the Logical Cluster is stored. So if you know all workspace objects in KCP, you can resolve every path which exists which points to a Logical Cluster by going through the workspace objects one by one up to the leaf, reading the Logical Cluster ID, and then accessing.</p> <p>I want to talk about the consistency guarantees in KCP. A workspace is a main unit of a consistent API space, like a Kubernetes cluster. So in a workspace, you have similar guarantees as in a Kubernetes cluster, which means per object kind, you have resource versions, which order changes to objects. Cross workspaces, you don't necessarily have that. If you look on objects in two workspaces, in two logical clusters technically, and you look on objects and resource versions, there does not have to be a linear order between them. It can be, but it doesn't have to.</p> <p>The fact that objects in one KCP shard are stored in the same storage, the same etcd or kine storage, this can be exploited by listing objects across workspaces, across logical clusters which are stored on the same shard. There is a request type we call a wildcard request. And that request returns objects across logical clusters of a shard. And inside of that list, all objects, the resource versions of those objects, they follow the same linear ordering guarantees as in one Kubernetes cluster, which means you can wildcard list objects for all logical clusters on a shard, and you can use that information to back an informer, so a reflector-based informer with list and watch support. And we can use the same informer infrastructure of Kubernetes to have informers for one shard. Now we will go into details of a multi-shard architecture soon, but in that case, this guarantee will not be there. This guarantee only holds for one shard.</p> <p>I want to talk about API exports a little bit in the sense of how they are stored and what an API export actually defines. An API export is a little bit like a CRD or multiple CRDs, so it defines resources with things like resource name, of course, the kind name, and the list type name. If you have multiple workspaces, in theory, each could export the same kind in the same API group. So there's a problem of how to distinguish those. Different exports, same kind of group. What KCP is introducing, basically adding to a CRD to an export, is a concept called an identity of an export. The identity is a secret that the owner of the API export knows. When you know that secret, you can export the same object and workspaces which use a bind to that API will then be actually bound to that export with that identity. So if you give away the secret, you leak the secret to an attacker, in theory, that attacker could export the same kind with the same identity and steal information. What we do to make use of the identity, we take the hash. So it's a short 256 hash of the identity, and that string is part of a binding. If you want to bind to an API export, you point the binding to the export by path, or in different ways in the future, and you will use the identity hash to make sure you really bind to the right export. The identity hash is also used within etcd. So to make sure that the API export owner can access objects of all tenants or users of that export, to make sure that this owner can only see the objects belonging to the same export, again, identified by the identity hash string. To make sure this is safe and nothing leaks, the identity hash is part of the key in the storage. So while on the wire, when you do requests against the workspace, you will just use the resource name as-is, but the binding carries the identity hash, and the system will make sure that the data, the objects are stored in the key which has identity hash as part of it. That way, if we come back to the wildcard request, that way we can make sure that the owner of the export can see all objects in all logic clusters on the shard of their export with their identity hash. And the wildcard request will include the identity hash as part of the request resource name. That way, we can segregate wildcard requests of different exports of the same resource and group name. That way, we get a safe system.</p>"},{"location":"concepts/miscellaneous/braindump/#api-exports-definition-and-storage","title":"API Exports Definition and Storage","text":"<p>An API export in KCP is similar to a CRD and defines resources by specifying details like the resource name, kind name, and list type name. In scenarios where multiple workspaces could export the same kind in the same API group, KCP introduces a way to distinguish between different exports of the same kind and group.</p>"},{"location":"concepts/miscellaneous/braindump/#identity-of-an-export","title":"Identity of an Export","text":"<p>KCP adds a concept called the identity of an export. This identity is essentially a secret known only to the owner of the API export. This secret, when known, allows for the exporting of the same object. Workspaces that bind to this API are then actually bound to that export with the specified identity. The security implication here is that if this secret is leaked, an attacker could potentially export the same kind with the same identity and intercept information. Utilizing the Identity Hash:</p> <p>To secure this system, KCP uses an SHA-256 hash of the identity as a critical part of a binding. When a workspace wants to bind to an API export, it points the binding to the export by path (or other methods in the future) and uses the identity hash to ensure binding to the correct export. Within the etcd storage system, this identity hash is used to ensure that the API export owner can only access objects that belong to the same export, identified by this hash. Security and Data Segregation in Storage:</p> <p>The identity hash becomes part of the key in the storage system. While client requests to a workspace use the resource name asynchronously, the binding carries the identity hash, and KCP ensures that data objects are stored in keys incorporating this hash. For wildcard requests, this system allows the export owner to see all objects across all logical clusters on a shard that pertain to their export, using their identity hash. The wildcard request includes the identity hash as part of the request resource name, allowing segregation of wildcard requests for different exports of the same resource and group name. This design presents a robust and secure system for managing API exports in KCP, ensuring that only authorized entities can access the relevant data and preventing unauthorized access or data leaks. This approach to API export management in KCP is both intricate and vital for the system's overall security and functionality.</p>"},{"location":"concepts/miscellaneous/braindump/#wildcard-requests","title":"Wildcard Requests","text":"<p>A note about wildcard requests. Wildcard requests are the low-level tool or enabler to make multi-workspace multi-logic cluster access possible. In practice, the use of the wildcard request type is a privileged operation which is only possible to do if you have the system master's group. An API export owner will not have that group. To enable the API export owner to access the objects of their export in all logic clusters on the shard. There is a proxy in front of the wildcard requests. We call that proxy a virtual API server, a virtual workspace API server. The API export object has a URL field in the status where the export owner can access the objects on a shard across logic clusters. This API service looks like the wildcard request we talked about earlier, but it is protected. For example, on that endpoint, the API export owner does not have to pass the identity hash. This is automatically added behind the scenes when proxying the request from the API export owner to the actual KCP instance via the wildcard request we talked about earlier. Virtual workspace API servers are a crucial tool. Here, we see them the first time for the first use of them in the system, but there are many more. We have built that in KCP, but in theory, virtual workspace API servers can be built by third parties and add further functionality which goes beyond a simple API service of one block.</p>"},{"location":"concepts/miscellaneous/braindump/#sharding","title":"Sharding","text":"<p>Now it is time to extend the mental model of KCP, which we described until now, to extend it to multiple shards. Imagine you have multiple instances of KCP running. Let's say we have two, A and B. Let's call the first one, let's call it the root shard. So the A shard is the root shard. The root shard hosts the root workspace. By the way, small note, the root workspace is the only workspace which has a logical cluster, the identifier of its logical cluster, which matches the workspace name. So the logical cluster UID of the root workspace is root. And the root shard is the one hosting the root logical cluster. The root logical cluster is a singleton in the KCP platform. On that root shard, there can be many more workspaces, many more logical clusters. The workspaces, or merely the logical clusters behind the workspace path, they are hosted on the shards of the KCP system. If they are multiple, for every logical cluster, when you want to access it, you have to know on which shard that logical cluster is stored. Every logical cluster object, or let's go into some detail here, a logical cluster, as we described, is identified by a logical cluster UID. That UID is used in the resource path in the storage. To make a logical cluster existent, there is a logical cluster object which lives in that logical cluster. It lives in its own. It's a bit like the dot object, the dot file in a Linux file system. It tells the system, when watching all logical cluster objects across the shard, so across one shard, if you list all logical cluster objects, you will know all the logical clusters on that shard. Side note, in theory, a system master user can create objects like config maps in a logical cluster without a logical cluster object. A user which is not privileged, including admin users, cannot do that. There is a mission in place which makes sure that objects like config maps can only be stored in logical clusters which exist. Existence again is realized by creating the logical cluster object. The logical cluster object is always called cluster. There's just one name, one singleton per workspace, per logical cluster.</p> <p>When sending a request to a multi-shard KCP, that request must be routed to the right shard. As we have seen, the existence of the Logical Cluster object called Cluster tells the system that the given Logical Cluster identified by the UID lives on that shard. In other words, a front proxy, as we call it in KCP, a component sitting in front of the KCP system, if it watches all Logical Cluster objects on all shards, it knows how to route requests. Combining that with the resolution of WorkspacePath, as we have seen before, this front proxy watches Workspace objects and Logical Cluster objects. When resolving a request to a WorkspacePath, it will first go Workspace object by Workspace object to resolve the leave of the WorkspacePath, to map it to the UID of the underlying Logical Cluster. In the last step, it will use the informer of the Logical Cluster objects, and it will store a mapping from a Logical Cluster UID to the respective shard, each with a stored one. So the front proxy will resolve, in the last step, the Logical Cluster UID to the shard and send the request there. The shard itself will check the existence of the Logical Cluster object called Cluster to make sure the request it receives from the front proxy is valid. And it will process the request and will eventually get the object the user requested, send it back to the front proxy and the front proxy sends it back to the user.</p> <p>I want to talk about controllers which run cross-workspace, which implement semantics, which require multiple workspaces. As an example, take the API binding controller. The controller sees an API binding pointing to an API export on a different shard.</p> <p>To implement this API binding logic, the controller has to access the API export, and the API export can live on different shards, as we said. In that case, it has to access a different shard to implement the functionality of the API. This is not a desired behavior, because if the target shard is unavailable for some time for reasons, all the other shards won't be able to bind APIs anymore. To solve that, KCP introduced a concept called a cache server. A cache server stores objects which are needed to implement cross-shard functionality of APIs. For example, for the API binding process, the API exports are needed. What happens is, there is a second controller next to the API binding controller, and we call that an API export replication controller, which synchronizes all API exports from a local shard to a shared cache server. The whole system will have a number one or a number of cache servers, and each shard will replicate data to the cache server or to the cache servers, if they are multiple. In this example, all API exports not only live on the local shard where the workspace is living on, but they are replicated, so the API binding controller, in our example, will have an informer both against the local objects and against the cached objects. When it wants to bind, it will look first for the local API exports, because of course this export can also live potentially on the local shard, but it will also watch on the cache server, and it will merge those two informers into one. There might be more objects necessary. A typical example here is, again, the API export, the binding process. For binding, you have to authorize. To authorize, you need the ABAC objects, which are living next to the API export. The API export application controller will check all ABAC objects, verify whether they are necessary to authorize a binding, and in that case, it will label the ABAC objects with a certain label for application, and application happens. This is a general pattern, which we will use for multi or cross workspace semantics of APIs.</p> <p>The concept of a cache server introduces constraints. The cache server in KCP is a regular Kube API server with workspace support. So it is bound to the scaling targets of Kube itself, the Kube API server itself. So imagine you have a giant multi-talent KCP installation. The cache server has to hold all the exports. This means that the cache server has to be able to store all exports in the system in roughly eight gigabytes of storage memory, in the case of etcd, including the airbag objects as well. While for API exports, this number doesn't seem to be a problem. You can imagine other kind of APIs, whereas cardinality might be much bigger, and APIs which also should be cross-workspace. In that case, you have to think about more scalable cache server topologies. In such a setup, very likely you will have many tenants. A tenant is probably a good partitioning, but it offers a way to partition the caching infrastructure. So everything which is within one tenant could be contained by default at least, which means you need only one cache server per tenant. Only those things like APIs which are not contained, which are explicitly shared among users, would have to be replicated into the whole hierarchy of cache servers. I mentioned the word hierarchy here because this might be a caching hierarchy which we want to use. Think of this as an API app store-like thing, where companies' tenants can offer services to other tenants. In such a platform, they would opt in into sharing their API export to the world. In such a setup, the cache server scalability would only be a limit for the APIs which are shared in that app store-like way. This doesn't seem to be a limit which limits the applicability of KCP, because there will never be so many exports that eight gigabyte is not enough.</p>"},{"location":"concepts/miscellaneous/braindump/#multi-shard-architecture-and-controllers-in-a-multi-shard-setup","title":"Multi-Shard Architecture and Controllers In a multi-shard setup","text":"<p>You need multiple instances of a controller or alternatively make a controller aware of multiple instances of KCP. A controller would in the second case have multiple wildcard watches as described before not against the shards themselves but against the virtual workspace API server for API exports. And they would watch multiple of them at the same time and give behavior semantics to the API objects on all of those shards. And you can imagine that you want some kind of partitioning of course if the number of shards grows. But it's pretty clear that some kind of awareness of KCP is necessary to run multi-workspace, multi-cluster controllers. In particular for the API exports, we talked about having the URL of the virtual workspace API server in the status. In reality, this list of URLs or this is a list of URLs. It's not just one. There are multiple URLs to multiple virtual workspace API servers, one per shard. At least one per shard which has at least one workspace which binds against that export. That way a controller which is KCP-enabled would have to watch the API export status and spawn another instance either of an informer or even the whole controller per URL which pops up in the status of the export.</p>"},{"location":"concepts/miscellaneous/braindump/#multi-shard-setups-and-the-workspace-hierarchy","title":"Multi-shard setups, and the workspace hierarchy","text":"<p>When creating a workspace object within another parent workspace, a WorkspaceScheduler will choose a shard and create a Logical Cluster object named Cluster on that shard under a random Logical Cluster identity hash. The choice of the shard to be used for scheduling follows different logics. In a single shard setup, this will always be the same shard. In a multi-shard setup, this could be random. Or it could be that it will choose a shard in the same region by default. Or there could be shards which belong to certain tenants. There could also be ways where a user could specify a different region for a workspace. The parent workspace, for example, would live in US-West-1 and the sub-workspace would live in EU-West-1 because it will host information necessary in Europe. So the result of all of those scheduling alternatives would be that the hierarchy is basically scattered across shards. Multiple shards from multiple regions potentially would be needed to access a certain deeply nested workspace. The naive implementation today of the front proxy is just watching all shards for workspace objects and for Logical Cluster objects. Clearly, this is not necessary that we implement it that way. We could have a highly available and distributed storage as an alternative to have an eventually consistent picture of all workspaces and Logical Clusters to implement the right routing of requests. In such a world, resolving a path would not necessarily couple shards from different regions or different cloud providers or anything like that. But it would be highly available even when the region goes down, which is used to store an intermediate workspace in the workspace path. Only the last component actually matters. If you can resolve the workspace path to the Logical Cluster UID and the shard name, the request can be routed, although parts of the workspace path may be unlocked.</p>"},{"location":"concepts/miscellaneous/braindump/#workspace-access-authorization","title":"Workspace Access Authorization","text":"<p>To access a workspace, the KCP instance which receives the request, potentially sent by the front proxy, will do an authentication of the user, whether this user is able to access the workspace as a whole, and in particular, the object which is accessed, whether it is allowed to access that using normal local airbag semantics. The second part is pure Kube. The first part is done through authorization via airbag objects in the logical cluster, not outside, inside, using a virtual fake resource, or more correctly, a certain verb against the logic cluster object. The user is authorized to access that workspace, and access means basically system-authenticated access, which might be just discovery in the minimal case. But there is this first step of workspace as a whole authorization, and it's done by checking that the user has verb access permission against logical cluster object, which as we described before is called cluster. To delegate access to that workspace, to a workspace, you will create a cluster role for the logical cluster object with a verb access, and you will bind the user or some groups the user has with a cluster role binding to that cluster role. Again, very important, all this happens within the workspace which is authorized. There is no airbag logic at the parent level. The reason is having logic at the parent level would mean would have to cache the airbag objects which are involved in the cache server. This is just heavy and can be avoided by this architecture.</p>"},{"location":"concepts/miscellaneous/braindump/#security-and-scheduling-of-workspaces","title":"Security and scheduling of workspaces","text":"<p>When scheduling a workspace, a shard is chosen to host the logic cluster. We talked about variations of that logic before. The scheduler, which does that work, runs on the shard, which hosts the parent workspace. For it to be able to schedule, it means two things. First, what I just described, choosing the shard, choosing a UID. And second, we have to create the logic cluster object on that chosen shard. Remember, existence of a logic cluster means the creation of the logic cluster object. For this to work, the shard which hosts the parent workspace needs access to the target shard of the child. Access which allows to create a logic cluster object, although the logic cluster it's created in is not existing yet. So this process of scheduling today needs privileged access, a privileged user which can create logic clusters, skipping the check whether the logic cluster actually exists.</p>"},{"location":"concepts/miscellaneous/braindump/#bootstrapping-a-kcp-platform-and-in-particular-a-kcp-shard-in-the-light-of-multiple-shards","title":"Bootstrapping a KCP platform and, in particular, a KCP shard in the light of multiple shards","text":"<p>Bootstrapping a KCP system, and in particular, a KCP shard. Bootstrapping a single shard KCP means to create the root workspace, create the API shards for the main API groups, and that's basically it. A multi-shard setup requires to bootstrap the root shard, which is basically equivalent to what I just described, and then bootstrapping further shards in addition. When we talked about API exports and API bindings, we talked about the identity of an export. The identity, or more completely, the hash of the identity string, is an important part of a resource, especially the resources which are defined in the root shards, the root workspace API exports. And those are particularly risky, so we really want this security feature of identities and identity hashes for those, because they are central for KCP, for the security of KCP, of the whole platform. By bootstrapping a KCP shard, it will need the identity hash of those exports. And to do that, the KCP shard needs a bootstrapping root shard user, which is able to read the identity hashes of the API exports. Which means, when bootstrapping a new shard for the first time the shard has started, it will need access to the root shard. After that, every start of the KCP instance of that shard, the KCP process, that root user is actually not needed anymore if no new exports are added or something like that. Every shard caches its identities, its identity hashes, in a local contract map. So that contract maps allows to restart every KCP shard, even when the root shard is down. What makes this bootstrapping tricky is that to make, to start up the KCP shard and the core controllers, for example, the API binding controller, it has to start certain informers. And to start an informer, you have to know the identity hash of the exports of the resources you want to watch. So in the bootstrapping phase, before a shard is ready, it will need this root shard information, which is cached in the contract map, or if the contract map is not there, or incomplete, there's access to the root shard. When those informers are up, the core controllers of the KCP shard are started and the KCP shard is ready to serve requests.</p>"},{"location":"concepts/miscellaneous/braindump/#definitions-of-shards","title":"Definitions of Shards","text":"<p>Shards are defined by creating Shard objects in the root workspace. When a Shard starts up, it will attempt to create its object there and tries to keep status of that object up-to-date with the controller. In the future, this object might also give load information, which might control scheduling of new workspaces. Today, this is very simplistic and not really crucial for the architecture of KCP.</p>"},{"location":"concepts/miscellaneous/braindump/#logical-cluster-path-annotation","title":"Logical cluster path annotation","text":"<p>A logical cluster has one unique longest path which points to it. In the simplest case where a logical cluster has no parent, that path is equal to the logical cluster ID. But if there is a parent, the parent longest and unique path plus colon plus the workspace name of the child is the longest path for that child. Some controllers need that information. To make that available, there is a kcp.io path annotation on the logical cluster object. If you have that object, you can know the path as well, not only the logical cluster itself, the logical cluster UID. There are APIs which allow bypass references, like API binding, for example. To make that possible, the API export, which is cached in the cache server, will also hold that path annotation. But this is a custom feature of the API export object, the API export API. There must be a controller which adds that annotation to the API export, which then ends up on the cache server to resolve an API binding. The API binding will maintain an index of the informer of API exports by that path annotation, so it can quickly find, bypass the corresponding export if it exists. There might be other APIs which use the same trick to allow bypass references across workspaces.</p>"},{"location":"concepts/miscellaneous/braindump/#multiple-routes-in-the-workspace-hierarchy","title":"Multiple Routes in the Workspace Hierarchy","text":"<p>We described before that the only workspace which has a logical cluster name that resembles the longest workspace path as the root workspace. We will keep this invariant, but we will extend the hierarchy by having more than one root, like a tree, not a tree, a root of trees. The main tree is starting at root, as we described before. Imagine you have a logical cluster which carries a path annotation which is referencing logically a parent which does not exist. We allow that. As an example, we take a user hierarchy. Imagine there are user workspaces. Every user gets a workspace. What we want to have is user colon username as a workspace. But we will not have a workspace or a logical cluster called user or users, plural. We can do that by creating logical clusters with a path annotation, users colon username, and no workspace which points to them. No workspace in the parent, because the parent doesn't exist. The front proxy will take the path annotation, and it will resolve the path from users colon username just by following the annotation. The annotation is enough to create a new root in the system. That way, we have rootless user home workspaces. In a previous iteration of KCP, we had a hierarchy of user home workspaces in the main hierarchy, which means we had to apply a multilevel hierarchy with a first letter or some hash of usernames and then multiple layers to guarantee that millions of users can store the workspace objects in the same parent. As you know in Kubernetes, there's a logical or technical limit of the number of objects of cluster wide cluster scope objects, which is probably in the 10,000 or something like that. All this complexity of a multilayer hierarchy for home workspaces goes away by having rootless workspaces. They can live anywhere in the KCP system on every shard, and the front proxy implements them just by the path annotation. To implement them, you need a privileged user which can create logical cluster objects in non-existing logical clusters, so very similar to the scheduler we talked about before. The user home workspace here is just an example. You could have rootless logical clusters for tenants, <code>tenant:colon:tenant-name</code>,  for example, or anything else you want to have as a start of a new hierarchy.</p>"},{"location":"concepts/miscellaneous/braindump/#system-masters-versus-cluster-admin-users","title":"System Masters versus Cluster Admin Users","text":"<p>In a single-shard setup, when you launch KCP via KCP Start, for convenience, an admin user is created. An admin user has star access, wildcard access, to all resources and all verbs. In Kubernetes, there is a System Masters group in addition. System Masters is more than Cluster Admin. System Masters means that authorization and admission is skipped. Skipped means safety checks are not executed for System Master requests. As an example, the scheduler needs a System Masters user in order to create notary cluster objects in non-existing notary clusters. It is important to understand that every user in front of the front proxy, so a real user including cluster admins, should not be System Masters because System Masters can destroy the workspace hierarchy, bring it into an inconsistent state. Hence, System Masters users should only be used for very specific high-risk operations on a shard. The KCP process will create a shard admin for that purpose. It's a shard local System Masters user. The admin user which is created is not like that. The KCP Start command in a single shard setup will create a token-based admin user. That token is only valid during runtime. It's not completely correct. That token is also stored locally, but the idea is that that user is just for convenience and a single shard setup. If you want to have a multi-shard setup, you have to create an admin user outside the bootstrapping process. There's a flag for the KCP Start command to skip the admin user creation. You can use, for example, a client certificate which adds cluster admin permissions to a user and makes that client certificate accepted by all shards by passing the right client cert flags to the process. To summarize, the bootstrapping of KCP in a multi-shard setup is a multistep process. The KCP Start command alone is without any special parameters. It's really meant for a single shard and for that reason, pretty simplistic setup. This is intentional. The admin user must be created out of scope in a step on its own.</p>"},{"location":"concepts/miscellaneous/braindump/#mobility-of-workspaces","title":"Mobility of Workspaces","text":"<p>There are reasons why workspaces or more correctly their logical clusters should be moved to another shard, for example to escape from some noisy neighbour, or because a shard is supposed to be replaced and drained. This is future work. But let's collect some thoughts:</p> <ol> <li>Moving a logical cluster means to copy the key-values from storage to another    storage. This is obviously not an instant process but might take minutes, at    least with etcd (maybe a kine based storage could be faster).</li> <li>While moving, controllers need some transactional migration to the new shard.</li> <li>Moving could be with downtime, or it can be continuous. There are ideas to use    some kind of progressive move-stone (like tomb-stones) logic, but this hasn't    been implemented.</li> <li>Controllers would see deletion as deletions and do what they have to do.    Even if we make the logical cluster immutable during deletion, there might be    cross-workspace controllers leading to surprising effects of these deletion    events that actually are no deletions. It might be that extending the    wildcard watch protocol with MOVE events would help here. After all this    protocol is under kcp control as wildcard request semantics is not part of    Kubernetes conformance anyway.</li> </ol>"},{"location":"concepts/sharding/","title":"Sharding","text":""},{"location":"concepts/sharding/#pages","title":"Pages","text":""},{"location":"concepts/sharding/#shards","title":"Shards","text":"<p>Horizontal Scaling of kcp through sharding</p>"},{"location":"concepts/sharding/#partitions","title":"Partitions","text":"<p>How to create shard partitions.</p>"},{"location":"concepts/sharding/#cache-server","title":"Cache Server","text":"<p>The cache server is a regular, Kubernetes-style CRUD API server with support of LIST/WATCH semantics.</p>"},{"location":"concepts/sharding/cache-server/","title":"Cache Server","text":""},{"location":"concepts/sharding/cache-server/#purpose","title":"Purpose","text":"<p>The primary purpose of the cache server is to support cross-shard communication. Shards need to communicate with each other in order to enable, among other things, migration and replication features. Direct shard-to-shard communication is not feasible in a larger setup as it scales with n*(n-1). The web of connections would be hard to reason about, maintain and troubleshoot. Thus, the cache server serves as a central place for shards to store and read common data. Note, that the final topology can have more than one cache server, i.e. one in some geographic region.</p>"},{"location":"concepts/sharding/cache-server/#high-level-overview","title":"High-level Overview","text":"<p>The cache server is a regular, Kubernetes-style CRUD API server with support of LIST/WATCH semantics. Conceptually it supports two modes of operations. The first mode is in which a shard gets its private space for storing its own data. The second mode is in which a shard can read other shards' data.</p> <p>The first mode of operation is implemented as a write controller that runs on a shard. It holds some state/data from that shard in memory and pushes it to the cache server. The controller uses standard informers for getting required resources both from a local kcp instance and remote cache server. Before pushing data it has to compare remote data to its local copy and make sure both copies are consistent.</p> <p>The second mode of operation is implemented as a read controller(s) that runs on some shard. It holds other shards' data in an in-memory cache using an informer, effectively pulling data from the central cache. Thanks to having a separate informer for interacting with the cache server a shard can implement a different resiliency strategy. For example, it can tolerate the unavailability of the secondary during startup and become ready.</p>"},{"location":"concepts/sharding/cache-server/#running-the-server","title":"Running the Server","text":"<p>The cache server can be run as a standalone binary or as part of a kcp server.</p> <p>The standalone binary is in https://github.com/kcp-dev/kcp/tree/main/cmd/cache-server and can be run by issuing <code>go run ./cmd/cache-server/main.go</code> command.</p> <p>To run it as part of a kcp server, pass <code>--cache-url</code> flag to the kcp binary.</p>"},{"location":"concepts/sharding/cache-server/#client-side-functionality","title":"Client-side Functionality","text":"<p>In order to interact with the cache server from a shard, the https://github.com/kcp-dev/kcp/tree/main/pkg/cache/client repository provides the following client-side functionality:</p> <p>ShardRoundTripper, a shard aware wrapper around <code>http.RoundTripper</code>. It changes the URL path to target a shard from the context.</p> <p>DefaultShardRoundTripper is a <code>http.RoundTripper</code> that sets a default shard name if not specified in the context.</p> <p>For example, in order to make a client shard aware, inject the <code>http.RoundTrippers</code> to a <code>rest.Config</code></p> <pre><code>import (\n  cacheclient \"github.com/kcp-dev/kcp/pkg/cache/client\"\n  \"github.com/kcp-dev/kcp/pkg/cache/client/shard\"\n)\n\n// adds shards awareness to a client with a default `shard.Wildcard` name.\nNewForConfig(cacheclient.WithShardRoundTripper(cacheclient.WithDefaultShardRoundTripper(serverConfig.LoopbackClientConfig, shard.Wildcard)))\n\n// and then change the context when you need to access a specific shard and pass is when making a HTTP request\nctx = cacheclient.WithShardInContext(ctx, shard.New(\"cache\"))\n</code></pre>"},{"location":"concepts/sharding/cache-server/#authorizationauthentication","title":"Authorization/Authentication","text":"<p>Not implemented at the moment</p>"},{"location":"concepts/sharding/cache-server/#built-in-resources","title":"Built-in Resources","text":"<p>Out of the box, the server supports the following resources:</p> <ul> <li><code>apiresourceschemas</code></li> <li><code>apiexports</code></li> <li><code>shards</code></li> </ul> <p>All those resources are represented as CustomResourceDefinitions and stored in <code>system:cache:server</code> shard under <code>system:system-crds</code> cluster.</p>"},{"location":"concepts/sharding/cache-server/#adding-new-resources","title":"Adding New Resources","text":"<p>Not implemented at the moment. Our near-term plan is to maintain a list of hard-coded resources that we want to keep in the cache server. In the future, we will use the ReplicationClam which will describe schemas that need to be exposed by the cache server.</p>"},{"location":"concepts/sharding/cache-server/#deletion-of-data","title":"Deletion of Data","text":"<p>Not implemented at the moment. Only deleting resources explicitly is possible. In the future, some form of automatic removal will be implemented.</p>"},{"location":"concepts/sharding/cache-server/#design-details","title":"Design Details","text":"<p>The cache server is implemented as the <code>apiextensions-apiserver</code>. It is based on the same fork used by the kcp server, extended with shard support.</p> <p>Since the server serves as a secondary replica it doesn't support versioning, validation, pruning, or admission. All resources persisted by the server are deprived of schema. That means the schema is implicit, maintained, and enforced by the shards pushing/pulling data into/from the server.</p>"},{"location":"concepts/sharding/cache-server/#on-the-http-level","title":"On the HTTP Level","text":"<p>The server exposes the following path:</p> <p><code>/services/cache/shards/{shard-name}/clusters/{cluster-name}/apis/group/version/namespaces/{namespace-name}/resource/{resource-name}</code></p> <p>Parameters:</p> <p><code>{shard-name}</code>: a required string holding a shard name, a wildcard is also supported indicating a cross-shard request</p> <p><code>{cluster-name}</code>: a required string holding a cluster name, a wildcard is also supported indicating a cross-cluster request.</p> <p><code>{namespace-name}</code>: an optional string holding a namespace the given request is for, not all resources are stored under a namespace.</p> <p><code>{resource-name}</code>: an optional string holding the name of a resource</p> <p>For example:</p> <p><code>/services/cache/shards/*/clusters/*/apis/apis.kcp.io/v1alpha1/apiexports</code>: for listing apiexports for all shards and clusters</p> <p><code>/services/cache/shards/amber/clusters/*/apis/apis.kcp.io/v1alpha1/apiexports</code>: for listing apiexports for amber shard for all clusters</p> <p><code>/services/cache/shards/sapphire/clusters/system:sapphire/apis/apis.kcp.io/v1alpha1/apiexports</code>: for listing apiexports for sapphire shard stored in system:sapphire cluster</p>"},{"location":"concepts/sharding/cache-server/#on-the-storage-layer","title":"On the Storage Layer","text":"<p>All resources stored by the cache server are prefixed with <code>/cache</code>. Thanks to that the server can share the same database with the kcp server.</p> <p>Ultimately a shard aware resources end up being stored under the following key:</p> <p><code>/cache/group/resource/{shard-name}/{cluster-name}/{namespace-name}/{resource-name}</code></p> <p>For more information about inner working of the storage layer, please visit: TODO: add a link that explains the mapping of a URL to a storage prefix.</p>"},{"location":"concepts/sharding/partitions/","title":"Partitions","text":"<p><code>Partitions</code> and <code>PartitionSets</code> build an API that allows service providers and cluster administrators to create shard Partitions. These partitions can then be leveraged:</p> <ul> <li>for the deployment of the controllers of the service providers and other components. These APIs provide the information on shard topology required for the scheduling.</li> <li>for grouping APIExport endpoints of multiple shards in common buckets, that is <code>Partitions</code>. These buckets can the be used to achieve geo proximity: controllers should communicate with API servers of shards in the same region. They can also be leveraged for scalability and load distribution, that is to adapt the number of deployment/ processes to the load pattern specific to the controllers of a service provider.</li> </ul>"},{"location":"concepts/sharding/partitions/#partitions_1","title":"Partitions","text":"<p>The partitioning of <code>Shards</code> is done through labels and label selectors.</p> <p>Here is an example of a <code>Partition</code>:</p> <pre><code>kind: Partition\napiVersion: topology.kcp.io/v1alpha1\nmetadata:\n    name: cloud-region-gcp-europe-xdfgs\n    ownerReferences:\n    ...\nspec:\n    selector:\n     matchLabels:\n       cloud: gcp\n       region: europe\n     matchExpressions:\n     - key: country\n       operator: NotIn\n       values:\n       - LI\n</code></pre> <p><code>Partitions</code> can be referenced in <code>APIExportEndpointSlices</code>.</p>"},{"location":"concepts/sharding/partitions/#partitionsets","title":"PartitionSets","text":"<p><code>PartitionSets</code> is  an API for convenience. <code>PartitionSet</code> can be used to get <code>Partitions</code> automatically created based on dimensions that match the shard label keys. The <code>Partitions</code> are created in the same workspace as the <code>PartitionSet</code>. They can then be copied to the desired workspace for consumption, for instance, by an <code>APIExportEndpointSlice</code>.</p> <p>Here is an example of a <code>PartitionSet</code>:</p> <pre><code>kind: PartitionSet\napiVersion: topology.kcp.io/v1alpha1\nmetadata:\n    name: cloud-region\nspec:\n   dimensions:\n   - region\n   - cloud\n   selectors:\n     matchExpressions:\n     - key: region\n       operator: NotIn\n       values:\n       - Antarctica\n       - Greenland\n     - key: country\n       operator: NotIn\n       values:\n       - NK\nstatus:\n   count: 10\n...\n</code></pre> <p>It is to note that a <code>Partition</code> is created only if it matches at least one shard. With the provided example if there is no shard in the cloud provider <code>aliyun</code> in the region <code>europe</code> no <code>Partition</code> will be created for it.</p> <p>An example of a <code>Partition</code> generated by this <code>PartitionSet</code> can be found above. The <code>dimensions</code> are translated into <code>matchLabels</code> with values specific to each <code>Partition</code>. An owner reference of the <code>Partition</code> will be set to the <code>PartitionSet</code>.</p>"},{"location":"concepts/sharding/shards/","title":"Shards","text":"<p>Every kcp shard is hosting a set of logical clusters. A logical cluster is identified by a globally unique identifier called a cluster name. A shard serves the logical clusters under <code>/clusters/&lt;cluster-name&gt;</code>.</p> <p>A set of known shards comprises a kcp installation.</p>"},{"location":"concepts/sharding/shards/#root-logical-cluster","title":"Root Logical Cluster","text":"<p>Every kcp installation has one logical cluster called <code>root</code>. The <code>root</code> logical cluster holds administrational objects. Among them are shard objects.</p>"},{"location":"concepts/sharding/shards/#shard-objects","title":"Shard Objects","text":"<p>The set of shards in a kcp installation is defined by <code>Shard</code> objects in <code>core.kcp.io/v1alpha1</code>.</p> <p>A shard object specifies the network addresses, one for external access (usually  some worldwide load balancer) and one for direct access (shard to shard).</p>"},{"location":"concepts/sharding/shards/#logical-clusters-and-workspace-paths","title":"Logical Clusters and Workspace Paths","text":"<p>Logical clusters are defined through the existence of a <code>LogicalCluster</code> object \"in themselves\", similar to a <code>.</code> directory defining the existence of a directory  in Unix.</p> <p>Every logical cluster name <code>name</code> is a logical cluster path. Every logical cluster is reachable through <code>/cluster/&lt;path&gt;</code> for every of their paths.</p> <p>A <code>Workspace</code> in <code>tenancy.kcp.io/v1alpha1</code> of name <code>name</code> references a logical cluster specified in <code>Workspace.spec.cluster</code>. If that workspace object resided in a logical cluster reachable via a path <code>path</code>, the referenced logical cluster can be reached via a path <code>path:name</code>.</p>"},{"location":"concepts/sharding/shards/#canonical-paths","title":"Canonical Paths","text":"<p>The longest path a logical cluster is reachable under is called the canonical path. By default, all canonical paths start with <code>root</code>, i.e. they start in root logical cluster.</p> <p>The logical cluster object annotated with <code>kcp.io/path: &lt;canonical-path&gt;</code>.</p> <p>Additional subtrees of the workspace path hierarchy can be defined by creating logical clusters with <code>kcp.io/path</code> annotation not starting in <code>root</code>. E.g. a home workspace hierarchy could start at <code>home:&lt;user-name&gt;</code>. There is no need for the parent (<code>home</code> in this case) to exist.</p>"},{"location":"concepts/sharding/shards/#front-proxy","title":"Front Proxy","text":"<p>A front-proxy is aware of all logical clusters, their shard they live on, their canonical paths and all <code>Workspaces</code>s. Non canonical paths can be  reconstructed from the canonical path prefixes and the worksapce names.</p> <p>Requests to <code>/cluster/&lt;path&gt;</code> are forwarded to the shard via inverse proxying.</p> <p>A front-proxy in its most simplistic (toy) implementation watches all shards. Clearly, that's neither feasible for scale nor good for availability. A front- proxy could alternative be backed by any kind of external database with the right scalability and availability properties.</p> <p>There can be one front-proxy in front of a kcp installation, or many, e.g. one or multiple per region or cloud provider.</p>"},{"location":"concepts/sharding/shards/#consistency-domain","title":"Consistency Domain","text":"<p>Every logical cluster provides a Kubernetes-compatible API root endpoint under <code>/cluster/&lt;path&gt;</code> including its own discovery endpoint and their own set of  API groups and resources.</p> <p>Resources under such an endpoints satisfy the same consistency properties as with a full Kubernetes cluster, e.g. the semantics of the resource versions of one resource matches that of Kubernetes.</p> <p>Across logical clusters the resource versions must be considered as unrelated, i.e. resource versions cannot be compared.</p>"},{"location":"concepts/sharding/shards/#wildcard-requests","title":"Wildcard Requests","text":"<p>The only exception to the upper rule are objects under a \"wildcard endpoint\" <code>/clusters/*/apis/&lt;group&gt;/&lt;v&gt;/[namespaces/&lt;ns&gt;]/resource:&lt;identity-hash&gt;</code> per shard. It serves the objects of the given resource on that shard across  logical-clusters. The annotation <code>kcp.io/cluster</code> tells the consumer which logical cluster each object belongs to.</p> <p>The wildcard endpoint is privileged (requires <code>system:masters</code> group membership). It is only accessible when talking directly to a shard, not through a  front-proxy.</p> <p>Note: for unprivileged access, virtual view apiservers can offer a highly secured and filtered view, usually also per shard, e.g. for owners of APIs.</p>"},{"location":"concepts/sharding/shards/#cross-logical-cluster-references","title":"Cross Logical Cluster References","text":"<p>Some objects reference other logical clusters or objects in other logical clusters. These references can be by logical cluster name, by arbitrary logical cluster path or by canonical path.</p> <p>Referenced logical clusters must be assumed to live on other shard, maybe even on shard of different regions or cloud providers.</p> <p>For scalability reasons, it is usually not adequate to drive controllers by informers that span all shard.</p> <p>In other words, logical involving cross-logical-cluster referenced have a cost higher than in-logical-cluster references and logic must be carefully planned:</p> <p>For example, during workspace creation and scheduling the scheduler running on the shard hosting the <code>Workspace</code> object will access another shard to create the <code>LogicalCluster</code> object initially. It does that by choosing a random logical cluster name (optimistically) and choosing a shard that name maps to (through consistent hashing). It then tries to create the <code>LogicalCluster</code>. On conflict, it can check whether the existing object belong the given <code>Workspace</code> object or  not. If not, another name and shard is chosen, until scheduling succeeds. During initialization the controller on the <code>Workspace</code> hosting shard will keep watching the logical cluster on the other shard, with some exponential backoff. In other words, the <code>Workspace</code> hosting shard does not continuously watch the object on the other shard.</p> <p>Another example is API binding, but it is different than workspace scheduling: a binding controller running on the shard hosting the <code>APIBinding</code> object will be aware of all <code>APIExport</code>s in the kcp installation through caching replication (see next section). What is special is that this controller has all the  information necessary to bind a new API and to keep bound APIs working even if  the shard of the <code>APIExport</code> is unavailable.</p> <p>Note: usually it a bad idea to create logic dependent on the parent workspace. If such logic is desired, for availability and scalability reasons some kind of replication is required. E.g. a child workspace must stay operation even if the parent is not accessible.</p>"},{"location":"concepts/sharding/shards/#cache-server-replication","title":"Cache Server Replication","text":"<p>The cache server is a special API server that can hold replicas of objects that must be available globally in an eventual consistent way. E.g. the <code>APIExport</code>s and <code>APIResourceSchemas</code> are replicated that way and made available to the  corresponding controllers via informers.</p> <p>The cache server holds objects by logical clusters, and it can hold objects from many or all shards in a kcp installation, served through wildcard informers. The resource versions of those objects have no meaning beyond driving the cache  informers running in the shards.</p> <p>Cache servers can be 1:1 with shards, or there can be shared cache servers, e.g. by region.</p> <p>Cache servers can form a cache hierarchy.</p> <p>Controllers that make use of cached objects, will usually have informers against local objects and against the same objects in the cache server. If the former returns a \"NotFound\" error, the controllers will look up in the cache informers.</p> <p>The cache server technique is only useful for APIs whose object cardinality  across all shards does not go beyond the cardinality sensibly storable in a kube-based apiserver.</p> <p>Note that objects like <code>Workspace</code>s and <code>LogicalCluster</code>s fall not into that category. This means that in particular the logical cluster canonical path cannot be derived from cached <code>LogicalCluster</code>s. Instead, the cached objects must hold their own <code>kcp.io/path</code> annotation in order to be indexable by that value. This is crucial to implement cross-logical-cluster references by  canonical path.</p> <p>Note: the <code>APIExport</code> example assumes that there are never more than e.g. 10,000 API exports in a kcp installation. If that is not an acceptable constraint,  other partitioning mechanism would be need to hold the number of <code>APIExport</code> objects per cache server below the critical number. E.g. there could be cache servers per big tenant, and that would hold only public exports and  tenant-internal exports. A more complex caching hierarchy would make sure the right objects are replicated, while the \"really public\" exports would only be a small number.</p>"},{"location":"concepts/sharding/shards/#replication","title":"Replication","text":"<p>Each shard pushes a restricted set of objects to the cache server. As the cache server replication is costly, this set is as minimal as possible. For example, certain RBAC objects are replicated in case they are needed to successfully authorize bindings of an API, or to use a workspace type.</p> <p>By the nature of replication, objects in the cache server can be old and  incomplete. For instance, the non-existence of an object in the cache server does not mean it does not exist in its respective shard. The replication  could be just delayed or the object was not identified to be worth to replicate.</p>"},{"location":"concepts/sharding/shards/#bootstrapping","title":"Bootstrapping","text":"<p>A new shard starting up will run a number of standard controllers (e.g. for workspaces, API bindings and more). These will need a number of standard informers both watching objects locally on that shard through wildcard informers and watching the corresponding cache server.</p> <p>Wildcard informers require the <code>APIExport</code> identity. This identity varies by installation as it is cryptographically created during creation of the <code>APIExport</code>.</p> <p>The core and tenancy APIs have their <code>APIExport</code> in the root logical cluster. A new shard will connect to that root logical cluster in order to extract the identities for these APIs. It will cache these for later use in case of a network partition or unavailability of the root shard. After retrieving these identities the informers can be started.</p>"},{"location":"concepts/workspaces/","title":"Workspaces","text":"<p>Multi-tenancy is implemented through workspaces. A workspace is a Kubernetes-cluster-like HTTPS endpoint, i.e. an endpoint usual Kubernetes client tooling (client-go, controller-runtime and others) and user interfaces (kubectl, helm, web console, ...) can talk to like to a Kubernetes cluster. Workspaces become available under <code>/clusters/&lt;parent-workspace-name&gt;:&lt;cluster-workspace-name&gt;</code>.</p> <p>Workspaces are backed by logical clusters, which means they are persisted in etcd on a shard with disjoint etcd prefix ranges, i.e. they have independent behaviour and no workspace sees objects from other workspaces. In contrast to namespace in Kubernetes, this includes non-namespaced objects, e.g. like CRDs where each workspace can have its own set of CRDs installed.</p> <p>Note</p> <p>For workspaces not backed by storage, check out virtual workspaces that transform other APIs e.g. by projections or by applying visibility filters (e.g. showing all workspaces or all namespaces the current user has access to). Virtual workspaces are not part of the <code>/clusters/</code> path structure.</p> <p>Workspaces are represented to the user via the <code>Workspace</code> kind, e.g.</p> <pre><code>kind: Workspace\napiVersion: tenancy.kcp.io/v1alpha1\nspec:\n  type: Universal\nstatus:\n  url: https://kcp.example.com/clusters/myapp\n</code></pre> <p>There are different types of workspaces, and workspaces are arranged in a tree.  Each type of workspace may restrict the types of its children and may restrict the types it may be a child of; a parent-child relationship is allowed if and only if the parent allows the child and the child allows the parent.</p>"},{"location":"concepts/workspaces/#pages","title":"Pages","text":""},{"location":"concepts/workspaces/#workspace-types","title":"Workspace Types","text":"<p>What are workspaces and how to use them.</p>"},{"location":"concepts/workspaces/#virtual-workspaces","title":"Virtual Workspaces","text":"<p>What are virtual workspaces and how do they work?</p>"},{"location":"concepts/workspaces/#workspace-initialization","title":"Workspace Initialization","text":""},{"location":"concepts/workspaces/virtual-workspaces/","title":"Virtual Workspaces","text":"<p>Virtual workspaces are proxy-like apiservers under a custom URL that provide some computed view of real workspaces.</p>"},{"location":"concepts/workspaces/virtual-workspaces/#examples","title":"Examples","text":"<ol> <li>Controllers should not be able to directly access customer workspaces. They should only be able to access the objects that are connected to their provided APIs. In April 19's community call this virtual workspace was showcased, developed during v0.4 phase.</li> <li>If we keep the initializer model with <code>WorkspaceType</code>, there must be a virtual workspace for the \"workspace type owner\" that gives access to initializing workspaces.</li> </ol>"},{"location":"concepts/workspaces/virtual-workspaces/#faq","title":"FAQ","text":"<ul> <li>Can we use go clients to watch resources on a virtual workspace? Absolutely. From the point of view of the controllers it is just a normal (client) URL. So one can use client-go informers (or controller-runtime) to watch the objects in a virtual workspace.</li> </ul> <p>Note</p> <p>A normal service account lives in just ONE workspace and can only access its own workspace. So in order to use a service account for accessing cross-workspace data (and that's what is necessary in example 2 and 3 at least), we need a virtual workspace to add the necessary authZ.</p> <ul> <li>Are virtual workspaces read-only? No, they are not necessarily. Some are, some are not. The controller view virtual workspace will be writable, as well as the syncer virtual workspace.</li> <li>Do service teams have to write their own virtual workspace? Not for the standard cases as described above. There might be cases in the future where service teams provide their own virtual workspace for some very special purpose access patterns. But we are not there yet.</li> <li>Where does the developer get the URL from of the virtual workspace? The URLs will be \"published\" in some object status. E.g. APIExport.status will have a list of URLs that controllers have to connect to (example 1). We might do the same in WorkspaceType.status (example 2).</li> <li>Will there be multiple virtual workspace URLs my controller has to watch? Yes, as soon as we add sharding, it will become a list. So it might be that 1000 tenants are accessible under one URL, the next 1000 under another one, and so on. The controllers have to watch the mentioned URL lists in status of objects and start new instances (either with their own controller sharding eventually, or just in process with another go routine).</li> <li>Show me the code. The stock kcp virtual workspaces are in the package <code>pkg/virtual</code>.</li> <li>Who runs the virtual workspaces? The stock kcp virtual workspaces will be run through <code>kcp start</code> in-process. The personal workspace one (example 1) can also be run as its own process and the kcp apiserver will forward traffic to the external address. There might be reasons in the future like scalability that the later model is preferred. For the clients of virtual workspaces that has no impact. They are supposed to \"blindly\" use the URLs published in the API objects' status. Those URLs might point to in-process instances or external addresses depending on deployment topology.</li> </ul>"},{"location":"concepts/workspaces/workspace-initialization/","title":"Workspace Initialization","text":"<p>Workspace initialization in kcp involves setting up initial configurations and resources for a workspace when it is created. This process is managed through <code>initializers</code>, which are enabled via <code>WorkspaceType</code> objects. This concept is the opposite of Kubernetes finalizers. This document covers how to configure initializers, the necessary RBAC permissions, URL schemes, and the reasons for using initializers.</p>"},{"location":"concepts/workspaces/workspace-initialization/#initializers","title":"Initializers","text":"<p>Initializers are used to customize workspaces and bootstrap required resources upon creation. Initializers are defined in WorkspaceType objects. This way, a user can define a controller that will process the Workspace and remove the initializer, moving it from the Initializing phase to the Ready phase.</p>"},{"location":"concepts/workspaces/workspace-initialization/#defining-initializers-in-workspacetypes","title":"Defining Initializers in WorkspaceTypes","text":"<p>A <code>WorkspaceType</code> can specify an initializer using the <code>initializer</code> field. Here is an example of a <code>WorkspaceType</code> with an initializer.</p> <pre><code>apiVersion: tenancy.kcp.io/v1alpha1\nkind: WorkspaceType\nmetadata:\n  name: example\nspec:\n  initializer: true\n  defaultChildWorkspaceType:\n    name: universal\n    path: root\n</code></pre>"},{"location":"concepts/workspaces/workspace-initialization/#enforcing-permissions-for-initializers","title":"Enforcing Permissions for Initializers","text":"<p>The non-root user must have the <code>verb=initialize</code> on the <code>WorkspaceType</code> that the initializer is for. This ensures that only authorized users can perform initialization actions using virtual workspace endpoint. Here is an example of the <code>ClusterRole</code>.</p> <p><pre><code>apiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRole\nmetadata:\n  name: initialize-example-workspacetype\nrules:\n  - apiGroups: [\"tenancy.kcp.io\"]\n    resources: [\"workspacetypes\"]\n    resourceNames: [\"example\"]\n    verbs: [\"initialize\"]\n</code></pre> You can then bind this role to a user or a group.</p> <pre><code>apiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRoleBinding\nmetadata:\n  name: initialize-example-workspacetype-binding\nsubjects:\n  - kind: User\n    name: user1\n    apiGroup: rbac.authorization.k8s.io\nroleRef:\n  kind: ClusterRole\n  name: initialize-example-workspacetype\n  apiGroup: rbac.authorization.k8s.io\n</code></pre>"},{"location":"concepts/workspaces/workspace-initialization/#initializingworkspaces-virtual-workspace","title":"initializingworkspaces Virtual Workspace","text":"<p>As a service provider, you can use the <code>initializingworkspaces</code> virtual workspace to manage workspace resources in the initializing phase. This virtual workspace allows you to fetch <code>LogicalCluster</code> objects that are in the initializing phase and request initialization by a specific controller.</p> <p>This Virtual Workspace can fetch <code>LogicalCluster</code> either by specific its name or using wildcard.</p>"},{"location":"concepts/workspaces/workspace-initialization/#endpoint-url-path","title":"Endpoint URL path","text":"<p><code>initializingworkspaces</code> Virtual Workspace provide a virtual api-server to access workspaces that are initializing with the specific initializer. These URLs are published in the status of WorkspaceType object.</p> <pre><code>  virtualWorkspaces:\n  - url: https://&lt;front-proxy-ip&gt;:6443/services/initializingworkspaces/&lt;initializer&gt;\n</code></pre> <p>This is an example URL path for accessing logical cluster apis for a specific initializer in a <code>initializingworkspaces</code> virtual workspace.</p> <pre><code>/services/initializingworkspaces/&lt;initializer&gt;/clusters/*/apis/core.kcp.io/v1alpha1/logicalclusters\n</code></pre> <p>You can also use <code>LogicalCluster</code> name for the direct view, allowing to manage all resources within that logical cluster.</p> <pre><code>/services/initializingworkspaces/&lt;initializer&gt;/clusters/&lt;logical-cluster-name&gt;/apis/core.kcp.io/v1alpha1/logicalclusters\n</code></pre>"},{"location":"concepts/workspaces/workspace-initialization/#example-workflow","title":"Example workflow","text":"<ul> <li> <p>Add your custom WorkspaceType to the platform with an initializer.</p> </li> <li> <p>Create a workspace with the necessary warrants and scopes. The workspace will stay in the initializing state as the initializer is present.</p> </li> <li> <p>Use a controller to watch your initializing workspaces, you can interact with the workspace through the virtual workspace endpoint:</p> </li> </ul> <pre><code>/services/initializingworkspaces/foo/clusters/*/apis/core.kcp.io/v1alpha1/logicalclusters\n</code></pre> <ul> <li> <p>Once you get the object, you need to initialize the workspace with its related resources, using the same endpoint</p> </li> <li> <p>Once the initialization is complete, use the same endpoint to remove the initializer from the workspace.</p> </li> </ul>"},{"location":"concepts/workspaces/workspace-types/","title":"Workspace Types","text":"<p>Workspaces have a type. A type is defined by a <code>WorkspaceType</code>. A type defines initializers. They are set on new Workspace objects and block the workspace from leaving the initializing phase. Both system components and 3rd party components can use initializers to customize Workspaces on creation, e.g. to bootstrap resources inside the workspace, or to set up permission in its parent.</p> <p>kcp comes with a built-in set of workspace types, and the admin may create objects that define additional types.</p> <ul> <li>Root Workspace is a singleton.  It holds some data that applies   to all workspaces, such as the set of defined workspace types   (objects of type <code>WorkspaceType</code>).</li> <li>HomeRoot Workspace is normally a singleton, holding the branch   of workspaces that contains the user home workspaces as descendants.   Can only be a child of the root workspace, and can only have   HomeBucket children.</li> <li>HomeBucket Workspace are intermediate vertices in the hierarchy   between the HomeRoot and the user home workspaces.  Can be a child   of the root or another HomeBucket workspace.  Allowed children are   home and HomeBucket workspaces.</li> <li>Home Workspace is a user's home workspace.  These hold user   resources such as applications with services, secrets, configmaps,   deployments, etc.  Can only be a child of a HomeBucket workspace.</li> <li>Universal Workspace is a basic type of workspace with no   particular nature.  Has no restrictions on parent or child workspace   types.</li> </ul> <p>The following workspace types are created by kcp if the <code>workspace-types</code> battery is enabled:</p> <ul> <li>Organization Workspace are workspaces holding organizational   data, e.g. definitions of user workspaces, roles, policies,   accounting data. Can only be a child of root.</li> <li>Team Workspace can only be a child of an Organization workspace.</li> </ul> <p>A workspace of type <code>Universal</code> is a workspace without further initialization or special properties by default, and it can be used without a corresponding <code>WorkspaceType</code> object (though one can be added and its initializers will be applied).</p> <p>Note</p> <p>In order to create workspaces of a given type (including <code>Universal</code>) you must have <code>use</code> permissions against the <code>workspacetypes</code> resources with the lower-case name of the cluster workspace type (e.g. <code>universal</code>). All <code>system:authenticated</code> users inherit this permission automatically for type <code>Universal</code>.</p> <p>The different workspace types are discussed below.</p>"},{"location":"concepts/workspaces/workspace-types/#user-home-workspaces","title":"User Home Workspaces","text":"<p>User home workspaces are an optional feature of kcp. If enabled (through <code>--enable-home-workspaces</code>), there is a special virtual <code>Workspace</code> called <code>~</code> in the root workspace. It is used by <code>kubectl ws</code> to derive the full path to the user home workspace, similar to how Unix <code>cd ~</code> move the users to their home.</p> <p>The full path for a user's home workspace has a number of parts: <code>&lt;prefix&gt;(:&lt;bucket&gt;)+:&lt;user-name&gt;</code>. Buckets are used to ensure that at most ~1000 sub-buckets or users exist in any bucket, for scaling reasons. The bucket names are deterministically derived from the user name (via some hash). Example for user <code>adam</code> when using default configuration: <code>root:users:a8:f1:adam</code>.</p> <p>User home workspaces are created on-demand when they are first accessed, but this is not visible to the user, allowing the system to only incur the cost of these workspaces when they are needed. Only users of the configured home-creator-groups (default <code>system:authenticated</code>) will have a home workspace.</p>"},{"location":"concepts/workspaces/workspace-types/#bucket-configuration-options","title":"Bucket Configuration Options","text":"<p>The <code>kcp</code> administrator can configure:</p> <ul> <li><code>&lt;prefix&gt;</code>, which defaults to <code>root:users</code></li> <li>bucket depth, which defaults to 2</li> <li>bucket name length, in characters, which defaults to 2</li> </ul> <p>The following outlines valid configuration options. With the default setup, ~5 users or ~700 sub-buckets will be in any bucket.</p> <p>Warning</p> <p>DO NOT set the bucket size to be longer than 2, as this will adversely impact performance.</p> <p>User-names have <code>(26 * [(26 + 10 + 2) * 61] * 36 = 2169648)</code> permutations, and buckets are made up of lowercase-alpha chars.  Invalid configurations break the scale limit in sub-buckets or users. Valid configurations should target having not more than ~1000 sub-buckets per bucket and at least 5 users per bucket.</p>"},{"location":"concepts/workspaces/workspace-types/#valid-configurations","title":"Valid Configurations","text":"length depth sub-buckets users 1 3 26 * 1 = 26 2169648 / (26)^3 = 124 1 4 26 * 1 = 26 2169648 / (26)^4 = 5 2 2 26 * 26 = 676 2169648 / (26*26)^2 = 5"},{"location":"concepts/workspaces/workspace-types/#invalid-configurations","title":"Invalid Configurations","text":"<p>These are examples of invalid configurations and are for illustrative purposes only. In nearly all cases, the default values will be sufficient.</p> length depth sub-buckets users 1 1 26 * 1 = 26 2169648 / (26) = 83448 1 2 26 * 1 = 26 2169648 / (26)^2 = 3209 2 1 26 * 26 = 676 2169648 / (26*26) = 3209 2 3 26 * 26 = 676 2169648 / (26*26)^3 = .007 3 1 26 26 26 = 17576 2169648 / (262626) = 124 3 2 26 26 26 = 17576 2169648 / (262626)^2 = .007"},{"location":"concepts/workspaces/workspace-types/#organization-workspaces","title":"Organization Workspaces","text":"<p>Organization workspaces are workspaces of type <code>Organization</code>, defined in the root workspace. Organization workspaces are accessible at <code>/clusters/root:&lt;org-name&gt;</code>.</p> <p>Note</p> <p>The organization WorkspaceType can only be created in the root workspace verified through admission.</p> <p>Organization workspaces have standard resources (on-top of <code>Universal</code> workspaces) which include the <code>Workspace</code> API defined through an CRD deployed during organization workspace initialization.</p>"},{"location":"concepts/workspaces/workspace-types/#root-workspace","title":"Root Workspace","text":"<p>The default root workspace is a singleton in the system accessible under <code>/clusters/root</code>. It is not represented by a <code>Workspace</code> anywhere, but shares the same properties.</p> <p>Inside the root workspace at least the following resources are bootstrapped on kcp startup:</p> <ul> <li>Workspace CRD</li> <li>WorkspaceType CRD</li> <li>Shard CRD</li> <li>Partion CRD</li> <li>PartionSet CRD</li> </ul> <p>The root workspace is the only one that holds <code>Shard</code> objects. Shards are used to schedule a new Workspace to, i.e. to select in which etcd the workspace content is to be persisted.</p>"},{"location":"concepts/workspaces/workspace-types/#system-workspaces","title":"System Workspaces","text":"<p>System workspaces are local to a shard and are named in the pattern <code>system:&lt;system-workspace-name&gt;</code>.</p> <p>System workspace are only accessible to a shard-local admin user, and there is neither a definition via a Workspace, nor is there any validation of requests that the system workspace exists.</p> <p>As an example, the <code>system:admin</code> workspace exists for administrative objects that are scoped to the local shard (e.g. <code>lease</code> objects for kcp internal controllers if leader election is enabled). It is accessible via <code>/clusters/system:admin</code>.</p>"},{"location":"concepts/workspaces/workspace-types/#workspace-type-extensions-and-constraints","title":"Workspace Type Extensions and Constraints","text":"<p>kcp offers extensions and constraints that enable you inherit functionality from other  workspace types and create custom workspace hierarchies for your organizational structure.</p> <p>A <code>WorkspaceType</code> can extend one or more other <code>WorkspaceTypes</code> using the <code>spec.extend.with</code> field.</p> <p>Example <pre><code>apiVersion: tenancy.kcp.io/v1alpha1\nkind: WorkspaceType\nmetadata:\n  name: sample\nspec:\n  extend:\n    with:\n    - name: universal\n    - name: custom\n</code></pre> In this example, the <code>sample</code> workspace type: * inherits initializers from the extended types * is considered as an extended type during type constraint evaluation</p> <p>You can also extend <code>WorkspaceTypes</code> from other workspaces by specifying the path:</p> <pre><code>apiVersion: tenancy.kcp.io/v1alpha1\nkind: WorkspaceType\nmetadata:\n  name: custom\nspec:\n  extend:\n    with:\n    - name: standard\n      path: root:base\n</code></pre>"},{"location":"concepts/workspaces/workspace-types/#workspace-constraint-mechanisms","title":"Workspace Constraint Mechanisms","text":"<p>KCP provides two primary constraint mechanisms for workspace types: * <code>limitAllowedChildren</code>: Controls which workspace types can be created as children. * <code>limitAllowedParents</code>: Controls which workspace types can serve as parents.</p> <p><pre><code>...\nspec:\n  limitAllowedParents:\n    types:\n    - name: sample\n      path: root\n  limitAllowedChildren:\n    types:\n    - name: custom\n      path: root\n</code></pre> You can also block all types from being used as children: <pre><code>apiVersion: tenancy.kcp.io/v1alpha1\nkind: WorkspaceType\nmetadata:\n  name: leaf-workspace\nspec:\n  limitAllowedChildren:\n    none: true\n</code></pre> This ensures that no other workspace type can be created as a child of <code>leaf-workspace</code>.</p>"},{"location":"contributing/","title":"Contributing to kcp","text":"<p>kcp is Apache 2.0 licensed and we accept contributions via GitHub pull requests.</p> <p>Please read the following guide if you're interested in contributing to kcp.</p>"},{"location":"contributing/#certificate-of-origin","title":"Certificate of Origin","text":"<p>By contributing to this project you agree to the Developer Certificate of Origin (DCO). This document was created by the Linux Kernel community and is a simple statement that you, as a contributor, have the legal right to make the contribution. See the DCO file for details.</p>"},{"location":"contributing/#getting-started","title":"Getting Started","text":""},{"location":"contributing/#prerequisites","title":"Prerequisites","text":"<ol> <li>Clone this repository.</li> <li>Install Go (currently 1.23.7).</li> <li>Install kubectl.</li> </ol> <p>Please note that the go language version numbers in these files must exactly agree: go/go.mod file, kcp/.ci-operator.yaml, kcp/Dockerfile, and in all the kcp/.github/workflows yaml files that specify go-version. In kcp/.ci-operator.yaml the go version is indicated by the \"tag\" attribute. In kcp/Dockerfile it is indicated by the \"golang\" attribute. In go.mod it is indicated by the \"go\" directive.\" In the .github/workflows yaml files it is indicated by \"go-version\"</p>"},{"location":"contributing/#build-verify","title":"Build &amp; Verify","text":"<ol> <li> <p>In one terminal, build and start <code>kcp</code>: <pre><code>go run ./cmd/kcp start\n</code></pre></p> </li> <li> <p>In another terminal, tell <code>kubectl</code> where to find the kubeconfig:</p> </li> </ol> <pre><code>export KUBECONFIG=.kcp/admin.kubeconfig\n</code></pre> <ol> <li>Confirm you can connect to <code>kcp</code>:</li> </ol> <pre><code>kubectl api-resources\n</code></pre>"},{"location":"contributing/#finding-areas-to-contribute","title":"Finding Areas to Contribute","text":"<p>Starting to participate in a new project can sometimes be overwhelming, and you may not know where to begin. Fortunately, we are here to help! We track all of our tasks here in GitHub, and we label our issues to categorize them. Here are a couple of handy links to check out:</p> <ul> <li>Good first issue issues</li> <li>Help wanted issues</li> </ul> <p>You're certainly not limited to only these kinds of issues, though! If you're comfortable, please feel free to try working on anything that is open.</p> <p>We do use the assignee feature in GitHub for issues. If you find an unassigned issue, comment asking if you can be assigned, and ideally wait for a maintainer to respond. If you find an assigned issue and you want to work on it or help out, please reach out to the assignee first.</p> <p>Sometimes you might get an amazing idea and start working on a huge amount of code. We love and encourage excitement like this, but we do ask that before you embarking on a giant pull request, please reach out to the community first for an initial discussion. You could file an issue, send a discussion to our mailing list, and/or join one of our community meetings.</p> <p>Finally, we welcome and value all types of contributions, beyond \"just code\"! Other types include triaging bugs, tracking down and fixing flaky tests, improving our documentation, helping answer community questions, proposing and reviewing designs, etc.</p>"},{"location":"contributing/#priorities-milestones","title":"Priorities &amp; Milestones","text":"<p>We prioritize issues and features both synchronously (during community meetings) and asynchronously (Slack/GitHub conversations).</p> <p>We group issues together into milestones. Each milestone represents a set of new features and bug fixes that we want users to try out. We aim for each milestone to take about a month from start to finish.</p> <p>You can see the current list of milestones in GitHub.</p> <p>For a given issue or pull request, its milestone may be:</p> <ul> <li>unset/unassigned: we haven't looked at this yet, or if we have, we aren't sure if we want to do it and it needs more community discussion</li> <li>assigned to a named milestone</li> <li>assigned to <code>TBD</code> - we have looked at this, decided that it is important and we eventually would like to do it, but we aren't sure exactly when</li> </ul> <p>If you are confident about the target milestone for your issue or PR, please set it. If you don\u2019t have permissions, please ask &amp; we\u2019ll set it for you.</p>"},{"location":"contributing/#epics","title":"Epics","text":"<p>We use the epic label to track large features that typically involve multiple stories. When creating a new epic, please use the epic issue template.</p> <p>Please make sure that you fill in all the sections of the template (it's ok if some of this is done later, after creating the issue). If you need help with anything, please let us know.</p>"},{"location":"contributing/#story-tasks","title":"Story Tasks","text":"<p>Story tasks in an epic should generally represent an independent chunk of work that can be implemented. These don't necessarily need to be copied to standalone GitHub issues; it's ok if we just track the story in the epic as a task. On a case by case basis, if a story seems large enough that it warrants its own issue, we can discuss creating one.</p> <p>Please tag yourself using your GitHub handle next to a story task you plan to work on. If you don't have permission to do this, please let us know by either commenting on the issue, or reaching out in Slack, and we'll assist you.</p> <p>When you open a PR for a story task, please edit the epic description and add a link to the PR next to your task.</p> <p>When the PR has been merged, please make sure the task is checked off in the epic.</p>"},{"location":"contributing/#tracking-work","title":"Tracking Work","text":""},{"location":"contributing/#issue-status-and-project-board","title":"Issue Status and Project Board","text":"<p>We use the Github projects beta for project management, compare our project board. Please add issues and PRs into the kcp project and update the status (new, in-progress, ...) for those you are actively working on.</p>"},{"location":"contributing/#unplanneduntracked-work","title":"Unplanned/Untracked Work","text":"<p>If you find yourself working on something that is unplanned and/or untracked (i.e., not an open GitHub issue or story task in an epic), that's 100% ok, but we'd like to track this type of work too! Please file a new issue for it, and when you have a PR ready, mark the PR as fixing the issue.</p>"},{"location":"contributing/#coding-guidelines-conventions","title":"Coding Guidelines &amp; Conventions","text":"<ul> <li>Always be clear about what clients or client configs target. Never use an unqualified <code>client</code>. Instead, always qualify. For example:<ul> <li><code>rootClient</code></li> <li><code>orgClient</code></li> <li><code>pclusterClient</code></li> <li><code>rootKcpClient</code></li> <li><code>orgKubeClient</code></li> </ul> </li> <li>Configs intended for <code>NewForConfig</code> (i.e. today often called \"admin workspace config\") should uniformly be called <code>clusterConfig</code><ul> <li>Note: with org workspaces, <code>kcp</code> will no longer default clients to the \"root\" (\"admin\") logical cluster</li> <li>Note 2: sometimes we use clients for same purpose, but this can be harder to read</li> </ul> </li> <li>Cluster-aware clients should follow similar naming conventions:<ul> <li><code>crdClusterClient</code></li> <li><code>kcpClusterClient</code></li> <li><code>kubeClusterClient</code></li> </ul> </li> <li><code>clusterName</code> is a kcp term. It is NOT a name of a physical cluster. If we mean the latter, use <code>pclusterName</code> or similar.</li> <li>In the syncer: upstream = kcp, downstream = pcluster. Depending on direction, \"from\" and \"to\" can have different meanings. <code>source</code> and <code>sink</code> are synonyms for upstream and downstream.</li> <li>Qualify \"namespace\"s in code that handle up- and downstream, e.g. <code>upstreamNamespace</code>, <code>downstreamNamespace</code>, and also <code>upstreamObj</code>, <code>downstreamObj</code>.</li> <li>Logging:</li> <li>Use the <code>fmt.Sprintf(\"%s|%s/%s\", clusterName, namespace, name</code> syntax.</li> <li>Default log-level is 2.</li> <li>Controllers should generally log (a) one line (not more) non-error progress per item with <code>klog.V(2)</code> (b) actions like create/update/delete via <code>klog.V(3)</code> and (c) skipped actions, i.e. what was not done for reasons via <code>klog.V(4)</code>.</li> <li>When orgs land: <code>clusterName</code> or <code>fooClusterName</code> is always the fully qualified value that you can stick into obj.ObjectMeta.ClusterName. It's not necessarily the <code>(Cluster)Workspace.Name</code> from the object. For the latter, use <code>workspaceName</code> or <code>orgName</code>.</li> <li>Generally do <code>klog.Errorf</code> or <code>return err</code>, but not both together. If you need to make it clear where an error came from, you can wrap it.</li> <li>New features start under a feature-gate (<code>--feature-gate GateName=true</code>). (At some point in the future), new feature-gates are off by default at least until the APIs are promoted to beta (we are not there before we have reached MVP).</li> <li>Feature-gated code can be incomplete. Also their e2e coverage can be incomplete. We do not compromise on unit tests. Every feature-gated code needs full unit tests as every other code-path.</li> <li>Go Proverbs are good guidelines for style: https://go-proverbs.github.io/ \u2013 watch https://www.youtube.com/watch?v=PAAkCSZUG1c.</li> <li>We use Testify's require a   lot in tests, and avoid   assert.</li> </ul> <p>Note this subtle distinction of nested <code>require</code> statements:   <pre><code>require.Eventually(t, func() bool {\n  foos, err := client.List(...)\n  require.NoError(err) // fail fast, including failing require.Eventually immediately\n  return someCondition(foos)\n}, ...)\n</code></pre>   and   <pre><code>require.Eventually(t, func() bool {\n  foos, err := client.List(...)\n  if err != nil {\n     return false // keep trying\n  }\n  return someCondition(foos)\n}, ...)\n</code></pre>   The first fails fast on every client error. The second ignores client errors and keeps trying. Either   has its place, depending on whether the client error is to be expected (e.g. because of asynchronicity making the resource available),   or signals a real test problem.</p>"},{"location":"contributing/#using-kubebuilder-crd-validation-annotations","title":"Using Kubebuilder CRD Validation Annotations","text":"<p>All of the built-in types for <code>kcp</code> are <code>CustomResourceDefinitions</code>, and we generate YAML spec for them from our Go types using kubebuilder.</p> <p>When adding a field that requires validation, custom annotations are used to translate this logic into the generated OpenAPI spec. This doc gives an overview of possible validations. These annotations map directly to concepts in the OpenAPI Spec so, for instance, the <code>format</code> of strings is defined there, not in kubebuilder. Furthermore, Kubernetes has forked the OpenAPI project here and extends more formats in the extensions-apiserver here.</p>"},{"location":"contributing/#replicated-data-types","title":"Replicated Data Types","text":"<p>Some objects are replicated and cached amongst shards when <code>kcp</code> is run in a sharded configuration. When writing code to list or get these objects, be sure to reference both shard-local and cache informers. To make this more convenient, wrap the look up in a function pointer.</p> <p>For example:</p> <pre><code>func NewController(ctx,\n  localAPIExportInformer, cacheAPIExportInformer apisinformers.APIExportClusterInformer\n) (*controller, error) {\n  ...\n  return &amp;controller{\n  listAPIExports: func(clusterName logicalcluster.Name) ([]apisv1apha1.APIExport, error) {\n    exports, err := localAPIExportInformer.Cluster(clusterName).Lister().List(labels.Everything())\n    if err != nil {\n      return cacheAPIExportInformer.Cluster(clusterName).Lister().List(labels.Everything())\n    }\n    return exports, nil\n  ...\n  }\n}\n</code></pre> <p>A full list of replicated resources is currently outlined in the replication controller.</p>"},{"location":"contributing/#getting-your-pr-merged","title":"Getting your PR Merged","text":"<p>The <code>kcp</code> project uses <code>OWNERS</code> files to denote the collaborators who can assist you in getting your PR merged.  There are two roles: reviewer and approver.  Merging a PR requires sign off from both a reviewer and an approver.</p>"},{"location":"contributing/#continuous-integration","title":"Continuous Integration","text":"<p>kcp uses a combination of GitHub Actions and and prow to automate the build process.</p> <p>Here are the most important links:</p> <ul> <li>.github/workflows/ci.yaml defines the Github Actions based jobs.</li> <li>kcp-dev/kcp/.prow.yaml defines the prow based jobs.</li> </ul>"},{"location":"contributing/#debugging-flakes","title":"Debugging Flakes","text":"<p>Tests that sometimes pass and sometimes fail are known as flakes. Sometimes, there is only an issue with the test, while other times, there is an actual bug in the main code. Regardless of the root cause, it's important to try to eliminate as many flakes as possible.</p>"},{"location":"contributing/#unit-test-flakes","title":"Unit Test Flakes","text":"<p>If you're trying to debug a unit test flake, you can try to do something like this:</p> <pre><code>go test -race ./pkg/reconciler/apis/apibinding -run TestReconcileBinding -count 100 -failfast\n</code></pre> <p>This tests one specific package, running only a single test case by name, 100 times in a row. It fails as soon as it encounters any failure. If this passes, it may still be possible there is a flake somewhere, so you may need to run it a few times to be certain. If it fails, that's a great sign - you've been able to reproduce it locally. Now you need to dig into the test condition that is failing. Work backwards from the condition and try to determine if the condition is correct, and if it should be that way all the time. Look at the code under test and see if there are any reasons things might not be deterministic.</p>"},{"location":"contributing/#end-to-end-test-flakes","title":"End to End Test Flakes","text":"<p>Debugging an end-to-end (e2e) test that is flaky can be a bit trickier than a unit test. Our e2e tests run in one of two modes:</p> <ol> <li>Tests share a single kcp server</li> <li>Tests in a package share a single kcp server</li> </ol> <p>The <code>e2e-shared-server</code> CI job uses mode 1, and the <code>e2e</code> CI job uses mode 2.</p> <p>There are also a handful of tests that require a fully isolated kcp server, because they manipulate some configuration aspects that are system-wide and would break all the other tests. These tests run in both <code>e2e</code> and <code>e2e-shared-server</code>, separate from the other kcp instance(s).</p> <p>You can use the same <code>run</code>, <code>-count</code>, and <code>-failfast</code> settings from the unit test section above for trying to reproduce e2e flakes locally. Additionally, if you would like to operate in mode 1 (all tests share a single kcp server), you can start a kcp instance locally in a separate terminal or tab:</p> <pre><code>bin/test-server\n</code></pre> <p>Then, to have your test use that shared kcp server, you add <code>-args --use-default-kcp-server</code> to your <code>go test</code> run:</p> <pre><code>go test ./test/e2e/apibinding -count 20 -failfast -args --use-default-kcp-server\n</code></pre>"},{"location":"contributing/#community-roles","title":"Community Roles","text":""},{"location":"contributing/#reviewers","title":"Reviewers","text":"<p>Reviewers are responsible for reviewing code for correctness and adherence to standards. Oftentimes reviewers will be able to advise on code efficiency and style as it relates to golang or project conventions as well as other considerations that might not be obvious to the contributor.</p>"},{"location":"contributing/#approvers","title":"Approvers","text":"<p>Approvers are responsible for sign-off on the acceptance of the contribution. In essence, approval indicates that the change is desired and good for the project, aligns with code, api, and system conventions, and appears to follow all required process including adequate testing, documentation, follow ups, or notifications to other areas who might be interested or affected by the change.</p> <p>Approvers are also reviewers.</p>"},{"location":"contributing/#management-of-owners-files","title":"Management of <code>OWNERS</code> Files","text":"<p>If a reviewer or approver no longer wishes to be in their current role it is requested that a PR be opened to update the <code>OWNERS</code> file. <code>OWNERS</code> files may be periodically reviewed and updated based on project activity or feedback to ensure an acceptable contributor experience is maintained.</p>"},{"location":"contributing/inspecting-e2e-metrics/","title":"Inspecting Prometheus Metrics for e2e Runs","text":""},{"location":"contributing/inspecting-e2e-metrics/#metrics-gathering-during-e2e-runs","title":"Metrics Gathering During e2e Runs","text":"<p>kcp internally exposes the same metrics as kube api-server. These are being gathered during e2e runs if the <code>PROMETHEUS_URL</code> variable is set. Here, any e2e spawned kcp server will publish a scraping configuration to prometheus for the given Prometheus URL environment variable.</p> <p>For e2e tests, the above environment variable is respected by the following test servers:</p> <ul> <li>when a shared kcp server is spawned using <code>SharedKcpServer</code> or <code>PrivateKcpServer</code> in the <code>kcp.test.e2e.framework</code> package.</li> <li><code>cmd/sharded-test-server</code></li> <li><code>cmd/test-server/kcp</code></li> </ul>"},{"location":"contributing/inspecting-e2e-metrics/#inspecting-metrics-of-e2e-runs","title":"Inspecting Metrics of e2e Runs","text":""},{"location":"contributing/inspecting-e2e-metrics/#github","title":"GitHub","text":"<p>To inspect metrics from GitHub e2e test runs, download the metrics from the GitHub summary page:</p> <p></p> <p>And execute Prometheus locally to inspect metrics: <pre><code>$ mkdir -p metrics &amp;&amp; unzip -p e2e-sharded-metrics.zip | tar xvzf - -C metrics\n$ prometheus --storage.tsdb.path=metrics --config.file /dev/null\n</code></pre></p>"},{"location":"contributing/inspecting-e2e-metrics/#prow","title":"Prow","text":"<p>To inspect e2e metrics from prow runs, just copy-paste the prow link from the pull request:</p> <p></p> <p>And paste it into PromeCIeus:</p> <p></p> <p>Once the Prometheus pod is ready, the provisioned link can be called to inspect gathered metrics. Alternatively, download the prometheus tarball from the gcsweb frontend locally and start a Prometheus instance as described above.</p>"},{"location":"contributing/inspecting-e2e-metrics/#collecting-metrics-locally","title":"Collecting Metrics Locally","text":"<p>To collect metrics locally, a convenience script is available to download and execute a Prometheus instance. Calling <code>... ./hack/run-in-prometheus make ...</code> will download, start, collect metrics, and stop Prometheus during the test run.</p> <p>Example: <pre><code>$ WHAT=./test/e2e/virtual/apiexport TEST_ARGS=\"-v -run TestAPIExportAuthorizers\" ./hack/run-with-prometheus.sh make test-e2e\n</code></pre> A local Prometheus can then be started to inspect gathered metrics: <pre><code>$ ./hack/tools/prometheus --storage.tsdb.path=.prometheus_data --config.file /dev/null\n</code></pre></p>"},{"location":"contributing/publishing-a-new-kcp-release/","title":"Publishing a New kcp Release","text":"<p>Note</p> <p>You currently need write access to the kcp-dev/kcp repository to perform these tasks.</p>"},{"location":"contributing/publishing-a-new-kcp-release/#create-git-tags","title":"Create git Tags","text":""},{"location":"contributing/publishing-a-new-kcp-release/#prerequisite-make-sure-you-have-a-gpg-signing-key","title":"Prerequisite - Make Sure You Have a GPG Signing Key","text":"<ol> <li>https://docs.github.com/en/authentication/managing-commit-signature-verification/generating-a-new-gpg-key</li> <li>https://docs.github.com/en/authentication/managing-commit-signature-verification/adding-a-gpg-key-to-your-github-account</li> <li>https://docs.github.com/en/authentication/managing-commit-signature-verification/telling-git-about-your-signing-key</li> </ol>"},{"location":"contributing/publishing-a-new-kcp-release/#create-the-tags","title":"Create the Tags","text":"<p>kcp has 2 go modules, and a unique tag is needed for each module every time we create a new release.</p> <ol> <li><code>git fetch</code> from the main kcp repository (kcp-dev/kcp) to ensure you have the latest commits</li> <li>Tag the main module</li> <li>If your git remote for kcp-dev/kcp is named something other than <code>upstream</code>, change <code>REF</code> accordingly</li> <li> <p>If you are creating a release from a release branch, change <code>main</code> in <code>REF</code> accordingly, or you can       make <code>REF</code> a commit hash.</p> <pre><code>REF=upstream/main\nTAG=v1.2.3\ngit tag --sign --message \"$TAG\" \"$TAG\" \"$REF\"\n</code></pre> </li> <li> <p>Tag the <code>sdk</code> module, following the same logic as above for <code>REF</code> and <code>TAG</code></p> <pre><code>REF=upstream/main\nTAG=v1.2.3\ngit tag --sign --message \"sdk/$TAG\" \"sdk/$TAG\" \"$REF\"\n</code></pre> </li> <li> <p>Tag the <code>cli</code> module, following the same logic as above for <code>REF</code> and <code>TAG</code></p> <pre><code>REF=upstream/main\nTAG=v1.2.3\ngit tag --sign --message \"cli/$TAG\" \"cli/$TAG\" \"$REF\"\n</code></pre> </li> </ol>"},{"location":"contributing/publishing-a-new-kcp-release/#push-the-tags","title":"Push the Tags","text":"<pre><code>REMOTE=upstream\nTAG=v1.2.3\ngit push \"$REMOTE\" \"$TAG\" \"sdk/$TAG\" \"cli/$TAG\"\n</code></pre>"},{"location":"contributing/publishing-a-new-kcp-release/#if-its-a-new-minor-version","title":"If it's a New Minor Version","text":"<p>If this is the first release of a new minor version (e.g. the last release was v0.7.x, and you are releasing the first 0.8.x version), follow the following steps.</p> <p>Otherwise, you can skip to Generate release notes</p>"},{"location":"contributing/publishing-a-new-kcp-release/#create-a-release-branch","title":"Create a Release Branch","text":"<p>Set <code>REMOTE</code>, <code>REF</code>, and <code>VERSION</code> as appropriate.</p> <pre><code>REMOTE=upstream\nREF=\"$REMOTE/main\"\nVERSION=1.2\ngit checkout -b \"release-$VERSION\" \"$REF\"\ngit push \"$REMOTE\" \"release-$VERSION\"\n</code></pre>"},{"location":"contributing/publishing-a-new-kcp-release/#generate-release-notes","title":"Generate Release Notes","text":"<p>To generate release notes from the information in PR descriptions you should use Kubernetes' release-notes tool. This tool will use the <code>release-notes</code> blocks in PR descriptions and the <code>kind/</code> labels on those PRs to find user-facing changes and categorize them. You can run the command below to install the latest version of it:</p> <pre><code>go install k8s.io/release/cmd/release-notes@latest\n</code></pre> <p>To use <code>release-notes</code> you will need to generate a GitHub API token (Settings -&gt; Developers settings -&gt; Personal access tokens -&gt; Fine-grained tokens). A token with Public Repositories (read-only) repository access and no further permissions is sufficient. Store the token somewhere safe and export it as <code>GITHUB_TOKEN</code> environment variable.</p> <p>Then, run run the <code>release-notes</code> tool (set <code>PREV_VERSION</code> to the version released before the one you have just released).</p> <pre><code>TAG=v1.2.3\nPREV_TAG=v1.2.2\nrelease-notes \\\n  --required-author='' \\\n  --org kcp-dev \\\n  --repo kcp \\\n  --branch main \\\n  --start-rev $PREV_TAG \\\n  --end-rev $TAG \\\n  --output CHANGELOG.md \n</code></pre> <p>Don't commit the <code>CHANGELOG.md</code> to the repository, just keep it around to update the release on GitHub (next step).</p>"},{"location":"contributing/publishing-a-new-kcp-release/#revieweditpublish-the-release-in-github","title":"Review/Edit/Publish the Release in GitHub","text":"<p>The goreleaser workflow automatically creates a draft GitHub release for each tag.</p> <ol> <li>Navigate to the draft release for the tag you just pushed. You'll be able to find it under the releases page.</li> <li>If the release notes have been pre-populated, delete them.</li> <li>Copy release notes from the <code>CHANGELOG.md</code> file you generated in the previous step.</li> <li>Publish the release.</li> </ol>"},{"location":"contributing/publishing-a-new-kcp-release/#trigger-documentation-deployment","title":"Trigger Documentation Deployment","text":"<p>Documentation for the respective release branch needs to be triggered manually after the release branch has been pushed.</p> <ol> <li>Navigate to the Generate and push docs GitHub Action.</li> <li>Hit the \"Run forkflow\" button, run workflow against <code>release-$VERSION</code>.</li> <li>Make sure the triggered workflow ran and deployed a new version of the documentation to docs.kcp.io.</li> </ol>"},{"location":"contributing/publishing-a-new-kcp-release/#notify","title":"Notify","text":"<ol> <li>Create an email addressed to kcp-dev@googlegroups.com and kcp-users@googlegroups.com</li> <li>Subject: <code>[release] &lt;version&gt;</code> e.g. <code>[release] v0.8.0</code></li> <li>In the body, include noteworthy changes</li> <li>Provide a link to the release in GitHub for the full release notes</li> <li>Post a message in the #kcp-dev Slack channel</li> </ol>"},{"location":"contributing/rebasing-kubernetes/","title":"Rebasing Kubernetes","text":"<p>This describes the process of rebasing kcp onto a new Kubernetes version. For the examples below, we'll be rebasing onto v1.31.0</p>"},{"location":"contributing/rebasing-kubernetes/#1-update-kcp-devapimachinery","title":"1. Update kcp-dev/apimachinery","text":"<ol> <li>Create a new branch for the update, such as <code>1.31-prep</code>.</li> <li>Update go.mod:</li> <li>You may need to change the go version at the top of the file to match what's in go.mod in the root of the       Kubernetes repo. This is not required, but probably a good idea.</li> <li>Update the primary Kubernetes dependencies:       <pre><code>go get -u k8s.io/api@v0.31.0  k8s.io/apimachinery@v0.31.0 k8s.io/client-go@v0.31.0\n</code></pre></li> <li>Run <code>go mod tidy</code>.</li> <li>Manually review the code that is upstream from <code>third_party</code> and make the same/similar edits to anything that    changed upstream. For example, we maintain a slightly modified copy of the    shared informer code.</li> <li>Run <code>make lint test</code>. Fix any issues you encounter.</li> <li>Commit your changes.</li> <li>Push to your fork.</li> <li>Open a PR; get it reviewed and merged.</li> </ol>"},{"location":"contributing/rebasing-kubernetes/#2-update-kcp-devcode-generator","title":"2. Update kcp-dev/code-generator","text":"<ol> <li>Create a new branch for the update, such as <code>1.26-prep</code>.</li> <li>Update go.mod:</li> <li>You may need to change the go version at the top of the file to match what's in go.mod in the root of the       Kubernetes repo. This is not required, but probably a good idea.</li> <li>Update the primary Kubernetes dependencies:       <pre><code>go get -u k8s.io/apimachinery@v0.31.0 k8s.io/code-generator@v0.31.0\n</code></pre></li> <li>Run <code>go mod tidy</code>.</li> <li>Update Kubernetes tooling versions in the <code>Makefile</code>:</li> <li><code>KUBE_CLIENT_GEN_VER := v0.31.0</code></li> <li><code>KUBE_LISTER_GEN_VER := v0.31.0</code></li> <li><code>KUBE_INFORMER_GEN_VER := v0.31.0</code></li> <li><code>KUBE_APPLYCONFIGURATION_GEN_VER := v0.31.0</code></li> <li>Update generators, if needed:</li> <li>Manually review upstream to check for any changes to these generators in the <code>kubernetes</code> repository:<ol> <li>staging/src/k8s.io/code-generator/cmd/client-gen</li> <li>staging/src/k8s.io/code-generator/cmd/informer-gen</li> <li>staging/src/k8s.io/code-generator/cmd/lister-gen</li> </ol> </li> <li>If there were any substantive changes, make the corresponding edits to our generators.</li> <li>You'll probably want to commit your changes at this point.</li> <li>Run <code>make codegen</code>. You'll probably want to commit these changes as a standalone commit.</li> <li>Note: <code>examples</code> is separate go module, so you may need to run <code>go mod tidy</code> in that directory and update the       go.mod file accordingly if you make changes to the code-generator.</li> <li>Run <code>make lint test build</code> and fix any issues you encounter.</li> <li>Commit any remaining changes.</li> <li>Push to your fork.<ol> <li>Open a PR; get it reviewed and merged.</li> </ol> </li> <li>Push release:     <pre><code>REF=upstream/main\nREMOTE=upstream\nTAG=v2.3.0\ngit tag --sign --message \"$TAG\" \"$TAG\" \"$REF\"\ngit push \"$REMOTE\" \"$TAG\"\n</code></pre></li> </ol>"},{"location":"contributing/rebasing-kubernetes/#3-update-kcp-devclient-go","title":"3. Update kcp-dev/client-go","text":"<ol> <li>Create a new branch for the update, such as <code>1.26-prep</code>.</li> <li>Update go.mod:</li> <li>You may need to change the go version at the top of the file to match what's in go.mod in the root of the       Kubernetes repo. This is not required, but probably a good idea.</li> <li>Update the <code>kcp-dev/apimachinery</code> dependency:       <pre><code>go get -u github.com/kcp-dev/apimachinery@main\n</code></pre></li> <li>That should have updated the primary Kubernetes dependencies, but in case it didn't, you can do so manually:       <pre><code>go get -u k8s.io/api@v0.31.0 k8s.io/apimachinery@v0.31.0 k8s.io/client-go@v0.31.0 k8s.io/apiextensions-apiserver@v0.31.0\n</code></pre></li> <li>Run <code>go mod tidy</code>.</li> <li>Update the kcp code-generator tooling version in the <code>Makefile</code>:<ol> <li>Look up the latest commit from the main branch in https://github.com/kcp-dev/code-generator.</li> <li>Adjust <code>CODE_GENERATOR_VER</code> to match accordingly.</li> </ol> </li> <li>Manually review the code that is upstream from <code>third_party</code> and make the same/similar edits to anything that    changed upstream.</li> <li>Run <code>make codegen</code>. You'll probably want to commit these changes as a standalone commit.</li> <li>Run <code>make lint</code> and fix any issues you encounter.</li> <li>Commit any remaining changes.</li> <li>Push to your fork.</li> <li>Open a PR; get it reviewed and merged.</li> </ol>"},{"location":"contributing/rebasing-kubernetes/#4-update-kcp-devkubernetes","title":"4. Update kcp-dev/kubernetes","text":""},{"location":"contributing/rebasing-kubernetes/#terminology","title":"Terminology","text":"<p>Commits merged into <code>kcp-dev/kubernetes</code> follow this commit message format:</p> <ul> <li><code>UPSTREAM: &lt;UPSTREAM PR ID&gt;</code></li> <li>The number identifies a PR in upstream kubernetes     (i.e. <code>https://github.com/kubernetes/kubernetes/pull/&lt;pr id&gt;</code>)</li> <li>A commit with this message should only be picked into the subsequent rebase branch     if the commits of the referenced PR are not included in the upstream branch.</li> <li>To check if a given commit is included in the upstream branch, open the referenced     upstream PR and check any of its commits for the release tag (e.g. <code>v.1.26.3</code>)     targeted by the new rebase branch. For example:     </li> <li><code>UPSTREAM: &lt;carry&gt;:</code></li> <li>A persistent carry that should probably be picked for the subsequent rebase branch.</li> <li>In general, these commits are used to modify behavior for consistency or     compatibility with kcp.</li> <li><code>UPSTREAM: &lt;drop&gt;:</code></li> <li>A carry that should probably not be picked for the subsequent rebase branch.</li> <li>In general, these commits are used to maintain the codebase in ways that are     branch-specific, like the update of generated files or dependencies.</li> </ul>"},{"location":"contributing/rebasing-kubernetes/#spreadsheet-of-carry-commits-from-previous-release","title":"Spreadsheet of Carry Commits from Previous Release","text":"<p>If the old branch (generally also the default branch) is <code>upstream/kcp-feature-logical-cluster-1.24-v3</code> and the old version is <code>v1.24.3</code>, then a tsv file containing the set of carry commits that need to be considered for cherry-picking later can be generated using:</p> <pre><code># create column headers\necho 'Comment Sha\\tSummary\\tCommit link\\tPR link\\tAction' &gt; ~/Documents/v1.24.3.tsv\n\n# populate the sheet\ngit log $( git merge-base upstream/kcp-feature-logical-clusters-1.24-v3 v1.24.3 )..upstream/kcp-feature-logical-clusters-1.24-v3 --ancestry-path --reverse --no-merges --pretty='tformat:%h%x09%s%x09https://github.com/kcp-dev/kubernetes/commit/%h?w=1' | grep -E $'\\t''UPSTREAM: .'$'\\t' | sed -E 's~UPSTREAM: ([0-9]+)(:.)~UPSTREAM: \\1\\2\\thttps://github.com/kubernetes/kubernetes/pull/\\1~' &gt;&gt; ~/Documents/v1.24.3.tsv\n</code></pre> <p>The tsv file can be imported into a google sheets spreadsheet to track the progress of picking commits to the new rebase branch. The spreadsheet can also be a way of communicating with rebase reviewers. For an example, please see the spreadsheet used for the v1.27.3 rebase.</p>"},{"location":"contributing/rebasing-kubernetes/#rebase-process","title":"Rebase Process","text":"<ol> <li>First and foremost, take notes of what worked/didn't work well. Update this guide based on your experiences!</li> <li>Remember, if you mess up, <code>git rebase --abort</code> and <code>git reflog</code> are your very good friends!</li> <li>Terminology:</li> <li>\"Old\" version: the current version of Kubernetes that kcp is using</li> <li>\"New\" version: the new Kubernetes version on top of which you are rebasing</li> <li>The <code>upstream</code> in e.g. <code>upstream/kcp-feature-logical-cluster-1.24-v3</code> is the name of the git remote that points    to github.com/kcp-dev/kubernetes. Please adjust the commands below if your remote is named differently.</li> <li>Prepare the old branch for rebasing:</li> <li>In this example, the old version is 1.31.0, and the new version is 1.31.0</li> <li>Create a new temporary branch to clean up the accumulated commits on the old branch:       <pre><code>git checkout -b kcp-1.31-clean upstream/kcp-feature-logical-cluster-1.31-v3\n</code></pre></li> <li>Start by doing a soft git reset so your working directory contains all the changes from the branch in an       uncommitted state:       <pre><code>git reset v1.31.1 # this must be whatever upstream commit was the starting point for the old branch\n</code></pre></li> <li>Revert the following types of changes:<ul> <li>go.mod/go.sum</li> <li>Generated clients, listers, informers, applyconfigurations</li> <li>vendor/*   <pre><code>git checkout go.mod go.sum vendor\n\n# for any untracked files\ngit clean -n -d vendor/* # to check what files would be removed\ngit clean -f vendor/* # actually remove the new untracked files\n</code></pre></li> </ul> </li> <li>Create a single baseline commit:       <pre><code>git add .\ngit commit -m 'UPSTREAM: &lt;carry&gt;: baseline for kcp 1.31'\n</code></pre></li> <li>Create a baseline branch that starts from the new version. We'll use this to cherry-pick any commits from    upstream that we need to continue carrying (i.e., they have not yet merged into the new version).    <pre><code>git checkout -b kcp-1.31-baseline v1.31.1\n</code></pre></li> <li>Review the list of carry commits in the spreadsheet generated above. Look    for commit messages of the format    <code>UPSTREAM: 12345: ...</code>. The number indicates the upstream pull request. You'll need to inspect each pull request    to determine when it was merged. Anything that merged before the rebase version can be ignored and omitted    from the baseline branch. Anything that merged after the rebase version as well as anything not yet merged    must be cherry-picked to the baseline branch. For example, in 1.24, we have <code>UPSTREAM: 111898: Reflector: support    logging Unstructured type</code>. This was added in 1.31, and therefore needs to be cherry-picked to the    <code>kcp-1.31-baseline</code> branch.</li> <li>Create a branch where you'll attempt to do the rebase. This will start from the cleaned up old version branch:    <pre><code>git checkout -b kcp-1.31-rebase kcp-1.31-clean\n</code></pre></li> <li>Rebase onto the baseline branch you created above:    <pre><code>git rebase kcp-1.31-baseline\n</code></pre></li> </ol> <p>You will likely encounter numerous conflicts. This is 100% normal! Go through each file and resolve them as best    as you can. Make notes of any particular changes you have questions about, so you don't forget them. 10. Update kcp dependencies, for all kcp repositories that changed. For example:     <pre><code>hack/pin-dependency.sh github.com/kcp-dev/logicalcluster/v3 v3.0.4\nhack/pin-dependency.sh github.com/kcp-dev/apimachinery/v2 767ac05aebce82530dee46e9dab8c7bb47f1c823\nhack/pin-dependency.sh github.com/kcp-dev/client-go 654321d8cac56f9944e8cf801dc15e37b3a582f3\n</code></pre> 11. Review changes to the origins of pkg/genericcontrolplane to see if similar changes are needed in the new version:</p> Destination Source(s) pkg/genericcontrolplane cmd/kube-apiserver/app, pkg/kubeapiserver/admission pkg/genericcontrolplane/apis/apis.go pkg/controlplane/instance.go pkg/genericcontrolplane/options.go cmd/kube-apiserver/app/options/options.go pkg/api/genericcontrolplane/scheme.go pkg/api/legacyscheme/scheme.go pkg/apis/core/install/genericcontrolplane/install.go pkg/apis/core/install/install.go pkg/apis/core/register_generic_control_plane.go pkg/apis/core/register.go pkg/apis/core/v1/register_generic_control_plane.go pkg/apis/core/v1/register.go staging/src/k8s.io/api/core/v1/register_generic_control_plane.go staging/src/k8s.io/api/core/v1/register.go pkg/registry/core/rest/genericcontrolplane/storage_core.go pkg/registry/core/rest/storage_core.go pkg/kubeapiserver/legacy_storage_factory_builder.go pkg/kubeapiserver/default_storage_factory_builder.go <ol> <li> <p>Update generated clients, informers, listers, etc. because we generate logical cluster aware versions of these    for Kubernetes to use:    <pre><code>hack/update-codegen.sh\n</code></pre></p> </li> <li> <p>Check if any new controllers were added to Kubernetes. If so, determine if they are relevant to the control     plane, or if they're specific to workloads (anything related to pods/nodes/etc.). If they're for the control     plane and you think they should be enabled in kcp, you have 1 of 2 choices:</p> <ol> <li>Modify the controller in Kubernetes to be logical cluster aware</li> <li>Add code to kcp to spawn a new controller instance scoped to a single logical cluster</li> </ol> <p>For option 1, follow what we did in pkg/controller/namespace/namespace_controller.go.</p> <p>For option 2, follow what we did in kcp in either the garbage collector or quota controllers.</p> </li> <li> <p>Check if any new admission plugins were added to Kubernetes. Decide if we need them in kcp. If so, make a note     of them, and we'll add them to kcp below.</p> </li> <li> <p>Push your commits to your fork of Kubernetes in GitHub.</p> </li> <li> <p>Open a pull request for review against the baseline branch, e.g. kcp-1.26-baseline, but mark it <code>WIP</code> and maybe     even open it in draft mode - you don't want to merge anything just yet.</p> </li> </ol>"},{"location":"contributing/rebasing-kubernetes/#5-update-kcp-devkcp","title":"5. Update kcp-dev/kcp","text":"<ol> <li>At this point, you're ready to try to integrate the updates into kcp proper. There is still likely a good    amount of work to do, so don't get discouraged if you encounter dozens or hundreds of compilation issues at    this point. That's totally normal!</li> <li>Update go.mod:</li> <li>You'll need to adjust the versions of kcp-dev/apimachinery, client-go, and logicalcluster to match any changes       you made in previous steps. Make sure the versions that are in the Kubernetes go.mod match the versions used here.</li> <li>If any new staging repositories were added in between the old and new Kubernetes versions, you'll need to add       those in the <code>replace</code> directive at the bottom of go.mod. For example, in the 1.31 rebase, we had to add 4:<ul> <li>k8s.io/dynamic-resource-allocation</li> <li>k8s.io/kms</li> <li>k8s.io/sample-cli-plugin</li> <li>k8s.io/sample-controller</li> </ul> </li> <li>Go ahead and make a commit here, as the next change we'll be making is to point kcp at your local checkout of       Kubernetes.</li> <li>Point kcp at your local checkout of Kubernetes:    <pre><code># Change KUBE per your local setup\nKUBE=../../../go/src/k8s.io/kubernetes\ngsed -i \"s,k8s.io/\\(.*\\) =&gt; .*/kubernetes/.*,k8s.io/\\1 =&gt; $KUBE/vendor/k8s.io/\\1,;s,k8s.io/kubernetes =&gt; .*,k8s.io/kubernetes =&gt; $KUBE,\" go.mod\n</code></pre>    !!! warning       Don't commit your changes to go.mod/go.sum. They point to your local file system.</li> <li>Resolve any conflicts</li> <li>Run <code>make modules</code></li> <li>Run <code>make codegen</code></li> <li>Keep iterating to get all the code to compile</li> <li>Get the <code>lint</code> and <code>test</code> make targets to pass</li> <li>Get the <code>e2e-*</code> make targets to pass.</li> </ol>"},{"location":"contributing/rebasing-kubernetes/#6-test-ci","title":"6. Test CI","text":"<ol> <li>Undo your changes to go.mod and go.sum that point to your local checkout:    <pre><code>git checkout -- go.mod go.sum\n</code></pre></li> <li>Update go.mod to point to your rebase branch in your Kubernetes fork:    <pre><code>GITHUB_USER=&lt;your username here&gt; BRANCH=kcp-1.31-rebase hack/bump-k8s.sh\n</code></pre></li> <li>Commit this change with a message such as <code>UNDO: my Kubernetes</code>. We won't be using this commit when it's time to    merge, so we make it easy to find.</li> <li>Open a pull request in kcp. Wait for CI results. The <code>deps</code> job will always fail at this point because it's    pointing to your fork of Kubernetes. This is expected, so don't worry. Your job at this point is to get all the    other CI jobs to pass.</li> </ol>"},{"location":"contributing/rebasing-kubernetes/#7-get-it-merged","title":"7. Get it Merged!","text":"<ol> <li>Once CI is passing (except for the <code>deps</code> job, as expected), we're ready to merge!</li> <li>Coordinate with another project member - show them the test results, then get them to approve your rebase PR in    kcp-dev/kubernetes. Get the PR merged (this is something that currently must be done manually - ask someone with    merge rights to do it for you if you need help).</li> <li>Rename/create a new branch in kcp-dev/kubernetes based off what you just merged into kcp-1.31-baseline - this is    the actual rebase! The new branch could be named something like <code>kcp-1.31</code>.</li> <li>Back in your local checkout of kcp, update to the kcp-dev/kubernetes rebase branch:    <pre><code>BRANCH=kcp-1.31 hack/bump-k8s.sh\n</code></pre></li> <li>Commit this change. You can either squash this with your previous <code>UNDO</code> commit (reword it so it's not an <code>UNDO</code>    any more), or drop the <code>UNDO</code> commit and replace it with this one.</li> <li>Check on CI. Hopefully everything is green. If not, keep iterating on it.</li> </ol>"},{"location":"contributing/rebasing-kubernetes/#7-update-the-default-branch-in-kcp-devkubernetes","title":"7. Update the Default Branch in kcp-dev/kubernetes","text":"<ol> <li>Change it to your new rebase branch, e.g. <code>kcp-1.31</code></li> </ol>"},{"location":"contributing/replicate-new-resource/","title":"Replicating New Resources in the Cache Server","text":"<p>As of today adding a new resource for replication is a manual process that consists of the following steps:</p> <ol> <li> <p>You need to register a new CRD in the cache server.     Registration is required otherwise the cache server won\u2019t be able to serve the new resource.    It boils down to adding a new entry into an array.    If you don\u2019t have a CRD definition file for your type, you can use the crdpuller against any kube-apiserver to create the required manifest.</p> </li> <li> <p>Next, you need to register the new type in the replication controller.    For that you need to add a new entry into the following array</p> </li> </ol> <p>In general there are two types of resources.    The ones that are replicated automatically (i.e. APIExports)    and the ones that are subject to additional filtering, for example require some additional annotation (i.e. ClusterRoles).</p> <p>For optional resources we usually create a separate controller which simply annotates objects that need to be replicated.    Then during registration we provide a filtering function that checks for existence of the annotation (i.e. filtering function for CR)</p>"},{"location":"developers/","title":"Developers","text":"<p>This chapter covers developing controllers against kcp and hacking on kcp itself.</p>"},{"location":"developers/#storage-to-rest-patterns","title":"Storage to rest patterns","text":""},{"location":"developers/storage-to-rest-patterns/","title":"Storage to rest patterns","text":""},{"location":"developers/storage-to-rest-patterns/#workspaces","title":"workspaces","text":"<p>kcp promises to support 1 mln of workspaces.  A workspace is like a Kubernetes endpoint, i.e. an endpoint usual Kubernetes client tooling (client-go, controller-runtime and others)  and user interfaces (kubectl, helm, web console, ...) can talk to, just like to a Kubernetes cluster. Thus creating a workspace must be efficient, both in terms of storage and compute. It also must provide isolation, just like regular clusters.</p>"},{"location":"developers/storage-to-rest-patterns/#etcd","title":"etcd","text":"<p>etcd is the primary datastore used by kcp.  It stores data in a key-value store.  The store\u2019s logical view is a flat binary key space.  The key space has a lexically sorted index on byte string keys.</p> <p>In order to create a logical hierarchy, keys are usually mixed with <code>/</code> e.g.  <code>/company/branch/location</code></p> <p>To make it more concrete let\u2019s create a company acme with two branches, marketing, and sales in some locations.</p> <pre><code>etcdctl put /acme/marketing/london foo\netcdctl put /acme/marketing/gdansk foo2\netcdctl put /acme/sales/warszawa foo3\n</code></pre> <p>In order to get all branches of acme company, all we have to do is to ask the database to return all keys matching <code>/acme</code> prefix <pre><code>etcdctl get --prefix /acme\n/acme/marketing/gdansk\nfoo2\n/acme/marketing/london\nfoo\n/acme/sales/warszawa\nfoo3\n</code></pre></p> <p>In order to see all locations of marketing teams, we as for records matching /acme/marketing prefix <pre><code>etcdctl get --prefix /acme/marketing\n/acme/marketing/gdansk\nfoo2\n/acme/marketing/london\nfoo\n</code></pre></p> <p>Note that those queries are based on byte comparisons.  We didn't create an <code>/acme</code> key.  Everything matching the prefix will be returned when using the --prefix parameter.</p> <p>This is the key idea behind workspaces in kcp.  Creating a workspace on the storage layer is very efficient because it boils down to concatenating a string. It also provides isolation on the lowest possible level. Data is filtered by the database engine.</p>"},{"location":"developers/storage-to-rest-patterns/#the-generic-registry","title":"the generic registry","text":"<p>kcp is based on the generic apiserver library provided by Kubernetes.  The central type provided by the library that interacts with a storage layer is called the generic registry.  It connects an API endpoint (REST) with a database layer.  Almost all API types make use of it.</p> <p>The generic apiserver library keeps an in-memory representation of the store for each resource in an API group.  For example, <code>secrets</code> resources in the <code>core</code> API group gets their own storage.</p> <p>From the perspective of this document, we can assume that the most important feature of generic storage  is to compute the key that is passed to the database to find the data the user wants.</p> <p></p> <p>When the server starts it precomputes the <code>ResourcePrefix</code> with a group and a resource name.  Everything else, like a workspace name, is added dynamically.  For example, a request with a URL of <code>/clusters/acme/core/secrets</code> finds a storage responsible  for <code>core/secrets</code> resources and adds <code>acme</code> to the <code>ResourcePrefix</code>.  The new string becomes a key that is passed to etcd to find only resources for <code>acme</code> cluster.</p>"},{"location":"developers/controllers/writing-kcp-aware-controllers/","title":"Writing kcp-aware Controllers","text":""},{"location":"developers/controllers/writing-kcp-aware-controllers/#keys-for-objects-in-listersindexers","title":"Keys for Objects in Listers/Indexers","text":"<p>When you need to get an object from a kcp-aware lister or an indexer, you can't just pass the object's name to the <code>Get()</code> function, like you do with a typical controller targeting Kubernetes. Projects using kcp's copy of client-go are using a modified key function.</p> <p>Here are what keys look like for an object <code>foo</code> for both cluster-scoped and namespace-scoped varieties:</p> Organization Workspace Logical Cluster Namespace Key - - root - root - - root default default/root root my-org root:my-org - root:my-org root my-org root:my-org default default/root:my-org my-org my-workspace my-org:my-workspace - my-org:my-workspace my-org my-workspace my-org:my-workspace default default/my-org:my-workspace"},{"location":"developers/controllers/writing-kcp-aware-controllers/#encodingdecoding-keys","title":"Encoding/Decoding Keys","text":"<p>Use the <code>github.com/kcp-dev/apimachinery/pkg/cache</code> package to encode and decode keys.</p>"},{"location":"developers/internals/etcd-structure/","title":"etcd structure","text":"<p>kcp has made some changes to etcd storage paths to support logical clusters and APIExport identities. Please see  below for details.</p>"},{"location":"developers/internals/etcd-structure/#built-in-apis","title":"Built-in APIs","text":"<p>Built-in APIs, such as <code>namespaces</code>, <code>configmaps</code>, <code>clusterroles</code>, etc., use the following storage path structure:</p> <pre><code>/etcd prefix/API group name/API resource name/logical cluster name/[namespace, if applicable]/name\n</code></pre> <p>Let's break down the segments in the etcd path for this example ConfigMap instance:</p> <pre><code>/registry/core/configmaps/root/default/kube-root-ca.crt\n</code></pre> Segment name Description registry etcd prefix core API group name configmaps API resource name root logical cluster name default namespace kube-root-ca.crt ConfigMap name"},{"location":"developers/internals/etcd-structure/#standard-custom-resource-instances","title":"Standard custom resource instances","text":"<p>Custom resource instances for a CustomResourceDefinition in a workspace use the following storage path structure:</p> <pre><code>/etcd prefix/API group name/API resource name/\"customresources\"/logical cluster name/[namespace, if applicable]/name\n</code></pre> <p>Let's break down the segments in the etcd path for this example APIBinding instance:</p> <pre><code>/registry/apis.kcp.io/apibindings/customresources/2cynbfy2m0wtjqcs/scheduling.kcp.io-1mc2w\n</code></pre> Segment name Description registry etcd prefix apis.kcp.io API group name apibindings API resource name customresources hard-coded (as opposed to an APIExport identity hash) 2cynbfy2m0wtjqcs logical cluster name scheduling.kcp.io-1mc2w APIBinding name"},{"location":"developers/internals/etcd-structure/#bound-custom-resource-instances","title":"\"Bound\" custom resource instances","text":"<p>Custom resource instances for an API provided by an APIExport, bound by an APIBinding, use the following storage  path structure:</p> <pre><code>/etcd prefix/API group name/API resource name/APIExport identity hash/logical cluster name/[namespace, if applicable]\n/name\n</code></pre> <p>Let's break down the segments in the etcd path for this example Workspace instance:</p> <pre><code>/registry/tenancy.kcp.io/workspaces/cbf732f90c9b7eac67e3d8f8e4c22cf8de0e36acefab6bbab738a85535c0d4cf/root/compute\n</code></pre> Segment name Description registry etcd prefix tenancy.kcp.io API group name workspaces API resource name cbf732f90c9b7eac67e3d8f8e4c22cf8de0e36acefab6bbab738a85535c0d4cf APIExport identity hash root logical cluster name compute Workspace name"},{"location":"developers/internals/using-kcp-as-a-library/","title":"Using kcp as a library","text":"<p>Instead of running the kcp as a binary using <code>go run</code>, you can include the kcp api-server in your own projects. To create and start the api-server with the default options (including an embedded etcd server):</p> <pre><code>options, err := serveroptions.NewOptions().Complete()\nif err != nil {\n    panic(err)\n}\n\ncfg, err := server.NewConfig(options)\nif err != nil {\n    panic(err)\n}\n\ncompletedCfg, err := cfg.Complete()\nif err != nil {\n    panic(err)\n}\n\nsrv, err := server.NewServer(completedCfg)\nif err != nil {\n    panic(err)\n}\n\n// Run() will block until the apiserver stops or an error occurs.\nif err := srv.Run(ctx); err != nil {\n    panic(err)\n}\n</code></pre> <p>You may also configure post-start hooks which are useful if you need to start a some process that depends on a connection to the newly created api-server such as a controller manager.</p> <pre><code>// Create a new api-server with default options\noptions, err := serveroptions.NewOptions().Complete()\nif err != nil {\n    panic(err)\n}\n\ncfg, err := server.NewConfig(options)\nif err != nil {\n    panic(err)\n}\n\ncompletedCfg, err := cfg.Complete()\nif err != nil {\n    panic(err)\n}\n\nsrv, err := server.NewServer(completedCfg)\nif err != nil {\n    panic(err)\n}\n\n// Register a post-start hook that connects to the api-server\nsrv.AddPostStartHook(\"connect-to-api\", func(ctx genericapiserver.PostStartHookContext) error {\n    // Create a new client using the client config from our newly created api-server\n    client := clientset.NewForConfigOrDie(ctx.LoopbackClientConfig)\n    _, err := client.Discovery().ServerGroups()\n\n    return err\n})\n\n// Start the api-server\nif err := srv.Run(ctx); err != nil {\n    panic(err)\n}\n</code></pre>"},{"location":"developers/investigations/","title":"Investigations","text":""},{"location":"developers/investigations/#logical-clusters","title":"Logical clusters","text":"<p>Investigation on logical clusters</p>"},{"location":"developers/investigations/#minimal-api-server","title":"Minimal API Server","text":"<p>Investigation on minimal API Server</p>"},{"location":"developers/investigations/#self-service-policy","title":"Self-service policy","text":"<p>Improve consistency and reusability of self-service and policy enforcement across multiple Kubernetes clusters.</p>"},{"location":"developers/investigations/#transparent-multi-cluster","title":"Transparent multi-cluster","text":"<p>Investigation of transparent multi-cluster</p>"},{"location":"developers/investigations/logical-clusters/","title":"Logical clusters","text":"<p>Kubernetes evolved from and was influenced by earlier systems that had weaker internal tenancy than a general-purpose compute platform requires. The namespace, quota, admission, and RBAC concepts were all envisioned quite early in the project, but evolved with Kubernetes and not all impacts to future evolution were completely anticipated. Tenancy within clusters is handled at the resource and namespace level, and within a namespace there are a limited number of boundaries. Most organizations use either namespace or cluster separation as their primary unit of self service, with variations leveraging a rich ecosystem of tools.</p> <p>The one concrete component that cannot be tenanted are API resources - from a Kubernetes perspective we have too much history and ecosystem to desire a change, and so intra-cluster tenancy will always be at its weakest when users desire to add diverse extensions and tools. In addition, controllers are practically limited to a cluster scope or a namespace scope by the design of our APIs and authorization models and so intra-cluster tenancy for extensions is particularly weak (you can't prevent an ingress controller from viewing \"all secrets\").</p> <p>Our admission chain has historically been very powerful and allowed deep policy to be enforced for tenancy, but the lack of a dynamic plugin model in golang has limited us to what can be accomplished to external RPC webhooks which have a number of significant performance and reliability impacts (especially when coupled to the cluster the webhook is acting on). If we want to have larger numbers of tenants or stronger subdivision, we need to consider improving the scalability of our policy chain in a number of dimensions.</p> <p>Ideally, as a community we could improve both namespace and cluster tenancy at the same time in a way that provides enhanced tools for teams and organizations, addresses extensions holistically, and improves the reliability and performance of policy control from our control planes.</p>"},{"location":"developers/investigations/logical-clusters/#goal-getting-a-new-cluster-that-allows-a-team-to-add-extensions-efficiently-should-be-effectively-zero-cost","title":"Goal: Getting a new cluster that allows a team to add extensions efficiently should be effectively zero cost","text":"<p>If a cluster is a desirable unit of tenancy, clusters should be amortized \"free\" and easy to operationalize as self-service. We have explored in the community a number of approaches that make new clusters cheaper (specifically virtual clusters in SIG-multi-tenancy, as well as the natural cloud vendor \"as-a-service\" options where they amortize the cost of many small clusters), but there are certain fundamental fixed costs that inflate the cost of those clusters. If we could make one more cluster the same cost as a namespace, we could dramatically improve isolation of teams as well as offering an advantage for more alignment on tenancy across the ecosystem.</p>"},{"location":"developers/investigations/logical-clusters/#constraint-a-naive-client-should-see-no-difference-between-a-physical-or-logical-cluster","title":"Constraint: A naive client should see no difference between a physical or logical cluster","text":"<p>A logical cluster that behaves differently from a physical cluster is not valuable for existing tools. We would need to lean heavily on our existing abstractions to ensure clients see no difference, and to focus on implementation options that avoid reduplicating a large amount of the Kube API surface area.</p>"},{"location":"developers/investigations/logical-clusters/#constraint-the-implementation-must-improve-isolation-within-a-single-process","title":"Constraint: The implementation must improve isolation within a single process","text":"<p>As clusters grow larger or are used in more complex fashion, the failure modes of a single process API server have received significant attention within the last few years. To offer cheaper clusters, we'd have to also improve isolation between simultaneous clients and manage etcd usage, traffic, and both cpu and memory use at the control plane. These stronger controls would be beneficial to physical clusters and organizations running more complex clusters as well.</p>"},{"location":"developers/investigations/logical-clusters/#constraint-we-should-improve-in-process-options-for-policy","title":"Constraint: We should improve in-process options for policy","text":"<p>Early in Kubernetes we discussed our options for extension of the core capability - admission control and controllers are the two primary levers, with aggregated APIs being an escape hatch for more complex API behavior (including the ability to wrap existing APIs or CRDs). We should consider options that could reduce the cost of complex policy such as making using the Kube API more library-like (to enable forking) as well as in-process options for policy that could deliver order of magnitude higher reliability and performance than webhooks.</p>"},{"location":"developers/investigations/logical-clusters/#areas-of-investigation","title":"Areas of investigation","text":"<ol> <li>Define high level user use cases</li> <li>Prototype modelling the simplest option to enable <code>kcp</code> demo functionality and team subdivision</li> <li>Explore client changes required to make multi-cluster controllers efficient</li> <li>Support surfacing into a logical cluster API resources from another logical cluster</li> <li>Layering RBAC so that changes in one logical cluster are additive to a source policy that is out of the logical cluster's control</li> <li>Explore quota of requests, cpu, memory, and persistent to complement P&amp;F per logical cluster with hard and soft limits</li> <li>Explore making <code>kcp</code> usable as a library so that an extender could write Golang admission / hierarchal policy for logical clusters that reduces the need for external extension</li> <li>Work through how a set of etcd objects could be moved to another etcd for sharding operations and keep clients unaware (similar to the \"restore a cluster from backup\" problem)</li> <li>Explore providing a read only resource underlay from another logical cluster so that immutable default objects can be provided</li> <li>Investigate use cases that would benefit even a single cluster (justify having this be a feature in kube-apiserver on by default)</li> </ol>"},{"location":"developers/investigations/logical-clusters/#use-cases","title":"Use cases","text":"<p>The use cases of logical cluster can be seen to overlap heavily with transparent multi-cluster use cases, and are captured at the highest level in GOALS.md.  The use cases below attempt to focus on logical clusters independent of the broader goals.</p>"},{"location":"developers/investigations/logical-clusters/#as-a-developer-of-crds-controllers-extensions","title":"As a developer of CRDs / controllers / extensions","text":"<ul> <li>I can launch a local Kube control plane and test out multiple different versions of the same CRD in parallel quickly</li> <li>I can create a control plane for my organization's cloud resources (CRDs) that is centralized but doesn't require me to provision nodes.</li> </ul>"},{"location":"developers/investigations/logical-clusters/#as-an-infrastructure-admin","title":"As an infrastructure admin","text":"<ul> <li>I can have strong tenant separation between different application teams</li> <li>Allow tenant teams to run their own custom resources (CRDs) and controllers without impacting others</li> <li>Subdivide access to the underlying clusters, keep those clusters simpler and with fewer extensions, and reduce the impact of cluster failure</li> </ul>"},{"location":"developers/investigations/logical-clusters/#as-a-user-on-an-existing-kubernetes-cluster","title":"As a user on an existing Kubernetes cluster","text":"<ul> <li>I can get a temporary space to test an extension before installing it</li> <li>I can create clusters that have my own namespaces</li> </ul>"},{"location":"developers/investigations/logical-clusters/#progress","title":"Progress","text":""},{"location":"developers/investigations/logical-clusters/#logical-clusters-represented-as-a-prefix-to-etcd","title":"Logical clusters represented as a prefix to etcd","text":"<p>In the early prototype stage <code>kcp</code> has a series of patches that allow a header or API prefix path to alter the prefix used to retrieve resources from etcd. The set of available resources is stripped down to a minimal set of hardcoded APIs including namespaces, rbac, and crds by patching those out of kube-apiserver type registration.</p> <p>The header <code>X-Kubernetes-Cluster</code> supports either a named logical cluster or the value <code>*</code>, or the prefix <code>/cluster/&lt;name&gt;</code> may be used at the root. This alters the behavior of a number of components, primarily retrieval and storage of API objects in etcd by adding a new segment to the etcd key (instead of <code>/&lt;resource&gt;/&lt;namespace&gt;/&lt;name&gt;</code>, <code>/&lt;resource&gt;/&lt;cluster&gt;/&lt;namespace&gt;/&lt;name&gt;</code>). Providing <code>*</code> is currently acting on watch to support watching resources across all clusters, which also has the side effect of populating the object <code>metadata.clusterName</code> field. If no logical cluster name is provided, the value <code>admin</code> is used (which behaves as a normal kube-apiserver would).</p> <p>This means new logical clusters start off empty (no RBAC or CRD resources), which the <code>kcp</code> prototype mitigates by calculating the set of API resources available by merging from the default <code>admin</code> CRDs + the hardcoded APIs. That demonstrates one avenue of efficiency - a new logical cluster has an amortized cost near zero for both RBAC (no duplication of several hundred RBAC roles into the logical cluster) and API OpenAPI documents (built on demand as the union of another logical cluster and any CRDs added to the new cluster).</p> <p>To LIST or WATCH these resources, the user specifies <code>*</code> as their cluster name which adjusts the key prefix to fetch all resources across all logical clusters. It's likely some intermediate step between client and server would be necessary to support \"watch subset\" efficiently, which would require work to better enable clients to recognize the need to relist as well as the need to make the prototype support some level of logical cluster subset retrieval besides just an etcd key prefix scan.</p>"},{"location":"developers/investigations/logical-clusters/#next-steps","title":"Next steps","text":"<ul> <li>Continuing to explore how clients might query multiple resources across multiple logical clusters</li> <li>What changes to resource version are necessary to allow</li> </ul>"},{"location":"developers/investigations/logical-clusters/#zero-configuration-on-startup-in-local-dev","title":"Zero configuration on startup in local dev","text":"<p>The <code>kcp</code> binary embeds <code>etcd</code> in a single node config and manages it for local iterative development. This is in keeping with optimize for local workflow, but can be replaced by connecting to an existing etcd instance (not currently implemented).  Ideally, a <code>kcp</code> like process would have minimal external dependencies and be capable of running in a shardable configuration efficiently (each shard handling 100k objects), with other components handling logical cluster sharding.</p>"},{"location":"developers/investigations/logical-clusters/#crd-virtualization-inheritance-and-normalization","title":"CRD virtualization, inheritance, and normalization","text":"<p>A simple implementation of CRD virtualization (different logical clusters having different api resources), CRD inheritance (a logical cluster inheriting CRDs from a parent logical cluster), and CRD normalization (between multiple physical clusters) to find the lowest-common-denominator resource has been prototyped.</p> <p>The CRD structure in the kube-apiserver is currently \"up front\" (as soon as a CRD is created it shows up in apiresources), but with the goal of reducing the up front cost of a logical cluster we may wish to suggest refactors upstream that would make the model more amenable to \"on demand\" construction and merging at runtime. OpenAPI merging is a very expensive part of the kube-apiserver historically (rapid CRD changes can have a massive memory and CPU impact) and this may be a logical area to invest to allow scaling within regular clusters.</p> <p>Inheritance allows an admin to control which resources a client might use - this would be particularly useful in more opinionated platform flows for organizations that wish to offer only a subset of APIs. The simplest approach here is that all logical clusters inherit the admin virtual cluster (the default), but more complicated flows with policy and chaining should be possible.</p> <p>Normalization involves reading OpenAPI docs from one or more child clusters, converting those to CRDs, finding the lowest compatible version of those CRDs (the version that shares all fields), and materializing those objects as CRDs in a logical cluster. This allows the minimum viable hook for turning a generic control plane into a spot where real Kube objects can run, and would be a key part of transparent multi-cluster.</p>"},{"location":"developers/investigations/minimal-api-server/","title":"Minimal API Server","text":"<p>The Kubernetes API machinery provides a pattern for declarative config-driven API with a number of conventions that simplify building configuration loops and consolidating sources of truth. There have been many efforts to make that tooling more reusable and less dependent on the rest of the Kube concepts but without a strong use case driving separation and a design the tooling is still fairly coupled to Kube.</p>"},{"location":"developers/investigations/minimal-api-server/#goal","title":"Goal","text":"<p>Building and sustaining an API server that:</p> <ol> <li>reuses much of the Kubernetes API server codebase to support Kube-like CRUD operations</li> <li>adds, removes, or excludes some / any / all the built-in Kubernetes types</li> <li>excludes some default assumptions of Kubernetes specific to the \"Kube as a cluster\" like \"create the kubernetes default svc\"</li> <li>replaces / modifies some implementations like custom resources, backend storage (etcd vs others), RBAC, admission control, and other primitives</li> </ol> <p>As a secondary goal, identifying where exceptions or undocumented assumptions exist in the libraries that would make clients behave differently generically (where an abstraction is not complete) should help ensure future clients can more concretely work across different API servers consistently.</p>"},{"location":"developers/investigations/minimal-api-server/#constraint-the-abstraction-for-a-minimal-api-server-should-not-hinder-kubernetes-development","title":"Constraint: The abstraction for a minimal API server should not hinder Kubernetes development","text":"<p>The primary consumer of the Kube API is Kubernetes - any abstraction that makes a standalone API server possible must not regress Kubernetes performance or overly complicate Kubernetes evolution. The abstraction should be an opportunity to improve interfaces within Kubernetes to decouple components and improve comprehension.</p>"},{"location":"developers/investigations/minimal-api-server/#constraint-reusing-the-existing-code-base","title":"Constraint: Reusing the existing code base","text":"<p>While it is certainly possible to rebuild all of Kube from scratch, a large amount of client tooling and benefit exists within patterns like declarative apply. This investigation is scoped to working within the context of improving the existing code and making the minimal changes within the bounds of API compatibility to broaden utility.</p> <p>It should be possible to add an arbitrary set of the existing Kube resources to the minimal API server, up to and including what an existing kube-apiserver exposes. Several use cases desire RBAC, Namespaces, Secrets, or other parts of the workload, while ensuring a \"pick and choose\" mindset keeps the interface supporting the needs of the full Kubernetes server.</p> <p>In the short term, it would not be a goal of this investigation to replace the underlying storage implementation <code>etcd</code>, but it should be possible to more easily inject the appropriate initialization code so that someone can easily start an API server that uses a different storage mechanism.</p>"},{"location":"developers/investigations/minimal-api-server/#areas-of-investigation","title":"Areas of investigation","text":"<ol> <li>Define high level user use cases</li> <li>Document existing efforts inside and outside of the SIG process</li> <li>Identify near-term SIG API-Machinery work that would benefit from additional decoupling (also wg-code-organization)</li> <li>Find consensus points on near term changes and draft a KEP</li> </ol>"},{"location":"developers/investigations/minimal-api-server/#use-cases","title":"Use cases","text":""},{"location":"developers/investigations/minimal-api-server/#as-a-developer-of-crds-controllers-extensions","title":"As a developer of CRDs / controllers / extensions","text":"<ul> <li>I can launch a local Kube API and test out multiple different versions of the same CRD in parallel quickly (shared with logical-clusters)</li> <li>I can create a control plane for my organization's cloud resources (CRDs) that is centralized but doesn't require me to provision nodes (shared with logical-clusters)</li> <li>... benefits for unit testing CRDs in controller projects?</li> </ul>"},{"location":"developers/investigations/minimal-api-server/#as-a-kubernetes-core-developer","title":"As a Kubernetes core developer","text":"<ul> <li>The core API server libraries are better separated and more strongly reviewed</li> <li>Additional contributors are incentivized to maintain the core libraries of Kube because of a broader set of use cases</li> <li>Kube client tools have fewer edge cases because they are tested against multiple sets of resources</li> <li>...</li> </ul>"},{"location":"developers/investigations/minimal-api-server/#as-an-aggregated-api-server-developer","title":"As an aggregated API server developer","text":"<ul> <li>It is easy to reuse the k8s.io/apiserver code base to provide:</li> <li>A virtual read-only resource that proxies to another type<ul> <li>e.g. metrics-server</li> </ul> </li> <li>An end user facing resource backed by a CRD (editable only by admins) that has additional validation and transformation<ul> <li>e.g. service catalog</li> </ul> </li> <li>A subresource implementation for a core type (pod/logs) that is not embedded in the Kube apiserver code</li> </ul>"},{"location":"developers/investigations/minimal-api-server/#as-a-devops-team","title":"As a devops team","text":"<ul> <li>I want to be able to create declarative APIs using the controller pattern ...</li> <li>So that I can have declarative infrastructure without a full Kube cluster (https://github.com/thetirefire/badidea and https://docs.google.com/presentation/d/1TfCrsBEgvyOQ1MGC7jBKTvyaelAYCZzl3udRjPlVmWg/edit#slide=id.g401c104a3c_0_0)</li> <li>So that I can have controllers that list/watch/sync/react to user focused changes</li> <li>So that I can have a kubectl apply loop for my intent (spec) and see the current state (status)</li> <li>So that I can move cloud infrastructure integrations like AWS Controllers for k8s out of individual clusters into a centrally secured spot</li> <li>I want to be offer a \"cluster-like\" user experience to a Kube application author without exposing the cluster directly (transparent multi-cluster)</li> <li>So that I can keep app authors from directly knowing about where the app runs for security / infrastructure abstraction</li> <li>So that I can control where applications run across multiple clusters centrally</li> <li>So that I can offer self-service provisioning at a higher level than namespace or cluster</li> <li>I want to consolidate all of my infrastructure and use gitops to talk to them the same way I do for clusters</li> <li>...</li> </ul> <p>More detailed requests</p> <ul> <li>With some moderate boilerplate (50-100 lines of code) I can start a Kube compliant API server with (some / any of):</li> <li>Only custom built-in types (code -&gt; generic registry -&gt; etcd)</li> <li>CRDs (CustomResourceDefinition)</li> <li>Aggregated API support (APIService)</li> <li>Quotas and rate control and priority and fairness</li> <li>A custom admission chain that does not depend on webhooks but is inline code</li> <li>A different backend for storage other than etcd (projects like kine)</li> <li>Add / wrap some HTTP handlers with middleware</li> </ul>"},{"location":"developers/investigations/minimal-api-server/#progress","title":"Progress","text":"<p>Initial work in k/k fork involved stripping out elements of kube-apiserver start that required \"the full stack\" or internal controllers such as the kubernetes.default.svc maintainer (roughly <code>pkg/master</code>). It also looked at how to pull a subset of Kube resources (namespaces, rbac, but not pods) from the core resource group. The <code>kcp</code> binary uses fairly normal <code>k8s.io/apiserver</code> methods to init the apiserver process.</p> <p>Next steps include identifying example use cases and the interfaces they wish to customize in the control plane (see above) and then looking at how those could be composed in an approachable way. That also involves exploring what refactors and organization makes sense within the k/k project in concert with 1-2 sig-apimachinery members.</p>"},{"location":"developers/investigations/self-service-policy/","title":"Self-service policy","text":""},{"location":"developers/investigations/self-service-policy/#goal","title":"Goal","text":"<p>Improve consistency and reusability of self-service and policy enforcement across multiple Kubernetes clusters.</p> <p>Just like Kubernetes standardized deploying containerized software onto a small set of machines, we want to standardize self-service of application focused integration across multiple teams with organizational control.</p> <p>Or possibly</p> <p>Kubernetes standardized deploying applications into chunks of capacity.  We want to standardize isolating and integrating application teams across organizations, and to do that in a way that makes applications everywhere more secure.</p>"},{"location":"developers/investigations/self-service-policy/#problem","title":"Problem","text":"<p>A key component of large Kubernetes clusters is shared use, where the usage pattern might vary from externally controlled (via gitops / existing operational tools) to a permissive self-service model.  The most common partitioning model in Kubernetes is namespace, and the second most common model is cluster.</p> <p>Self-service is currently limited by the set of resources that are namespace scoped for the former, and by the need to parameterize and configure multiple clusters consistently for the latter. Cluster partitioning can uniquely offer distinct sets of APIs to consumers. Namespace partitioning is cheap up until the scale limits of the cluster (~10k namespaces), while cluster partitioning usually has a fixed cost per cluster in operational and resource usage, as well as lower total utilization.</p> <p>Once a deployment reaches the scale limit of a single cluster, operators often need to redefine their policies and tools to work in a multi-cluster environment. Many large deployers create their own systems for managing self-service policy above their clusters and leverage individual subsystems within Kubernetes to accomplish those goals.</p>"},{"location":"developers/investigations/self-service-policy/#approach","title":"Approach","text":"<p>The logical cluster concept offers an opportunity to allow self-service at a cluster scope, with the effective cost of the namespace partitioning scheme. In addition, the separation of workload at control plane (kcp) and data plane (physical cluster) via transparent multi-cluster or similar schemes allows strong policy control of what configuration is allowed (reject early), restriction of the supported API surface area for workload APIs (limit / control certain fields like pod security), and limits the access of individual users to the underlying infra (much like clusters limit access to nodes).</p> <p>It should be possible to accomplish current self-service namespace and cluster partitioning via the logical cluster mechanism + policy enforcement, and to incentivize a wider range of \"external policy control\" users to adopt self-service via stronger control points and desirable use cases (multi-cluster resiliency for apps).</p> <p>We want to enable concrete points of injection of policy that are difficult today in Kubernetes tenancy:</p> <ol> <li>The acquisition of a new logical cluster with capabilities and constraints</li> <li>How the APIs in a logical cluster are transformed to an underlying cluster</li> <li>How to manage the evolution of APIs available to a logical cluster over time</li> <li>New hierarchal policy options are more practical since different logical clusters can have different APIs</li> </ol>"},{"location":"developers/investigations/self-service-policy/#areas-of-investigation","title":"Areas of investigation","text":"<ul> <li>Using logical clusters as a mechanism for tenancy, but having a backing implementation that can change</li> <li>I.e. materialize logical clusters as an API resource in a separate logical cluster</li> <li>Or implementing logical clusters outside the system and having the kcp server implementation be a shim</li> <li>Formal \"policy module\" implementations that can be plugged into a minimal API server while using logical cluster impl</li> <li>Catalog the set of tenancy constructs in use in Kube</li> <li>Draw heavily on sig-multitenancy explorations - work done by cluster api nested, virtual clusters, namespace tenancy, and hierarchal namespace designs</li> <li>Look at reference materials created by large organizational adopters of Kube</li> <li>Consider making \"cost\" a first class control concept alongside quota and RBAC (i.e. a service load balancer \"costs\" $1, whereas a regular service costs $0.001)</li> <li>Could this more effectively limit user action</li> <li>Explore hierarchy of policy - if logical clusters are selectable by label, could you have composability of policy using controllers</li> <li>Explore using implicit resources</li> <li>i.e. within a logical cluster have all resources of type RoleBinding be fetched from two sources - within the cluster, and in a separate logical cluster - and merged, so that you could change the global source and watches would still fire</li> <li>Impliict resources have risk though - no way to \"lock\" them so the consequences of an implicit change can be expensive</li> </ul>"},{"location":"developers/investigations/self-service-policy/#progress","title":"Progress","text":""},{"location":"developers/investigations/self-service-policy/#simple-example-of-a-policy-implementation","title":"Simple example of a policy implementation","text":"<p>Building out an example flow that goes from creating a logical cluster resource that results in a logical cluster being accessible to client, with potential hook points for deeper integration.</p>"},{"location":"developers/investigations/self-service-policy/#describe-a-complicated-policy-implementation","title":"Describe a complicated policy implementation","text":"<p>An example hosted multi-tenant service with billing, organizational policy, and tenancy isolation.</p>"},{"location":"developers/investigations/transparent-multi-cluster/","title":"Transparent multi-cluster","text":""},{"location":"developers/investigations/transparent-multi-cluster/#note-this-was-a-prototype-that-was-not-continued-the-ideas-here-are-still-valid-and-could-be-picked-up-by-a-future-project","title":"NOTE: This was a prototype that was not continued. The ideas here are still valid and could be picked up by a future project.","text":"<p>A key tenet of Kubernetes is that workload placement is node-agnostic until the user needs it to be - Kube offers a homogeneous compute surface that admins or app devs can \"break-glass\" and set constraints all the way down to writing software that deeply integrates with nodes. But for the majority of workloads a cluster is no more important than a node - it's a detail determined by some human or automated process.</p> <p>A key area of investigation for <code>kcp</code> is exploring transparency of workloads to clusters. Aspirationally we want Kube workloads to be resilient to the operational characteristics of the underlying infrastructure and clusters orthogonally to the workload, by isolating the user from knowing of the details of the infrastructure. If workload APIs are more consistently \"node-less\" and \"cluster-agnostic\" that opens up ways to drive workload consistency across a large swathe of the compute landscape.</p>"},{"location":"developers/investigations/transparent-multi-cluster/#goal-the-majority-of-applications-and-teams-should-have-workflows-where-cluster-is-a-detail","title":"Goal: The majority of applications and teams should have workflows where cluster is a detail","text":"<p>A number of projects have explored this since the beginning of Kubernetes - this prototype should explore in detail  how we can make a normal Kubernetes flow for most users be cluster-independent but still \"break-glass\" and describe placement in detail. Since this is a broad topic, and we want to benefit the majority of users, we need to also add constraints that maximize the chance of these approaches being adopted.</p>"},{"location":"developers/investigations/transparent-multi-cluster/#constraint-the-workflows-and-practices-teams-use-today-should-be-minimally-disrupted","title":"Constraint: The workflows and practices teams use today should be minimally disrupted","text":"<p>Users typically only change their workflows when an improvement offers a significant multiplier. To be effective we must reduce friction (which reduces multipliers) and offer significant advantages to that workflow.</p> <p>Tools, practices, user experiences, and automation should \"just work\" when applied to cluster-agnostic or cluster-aware workloads. This includes gitops, rich web interfaces, <code>kubectl</code>, etc. That implies that a \"cluster\" and a \"Kube API\" is our key target, and that we must preserve a majority of semantic meaning of existing APIs.</p>"},{"location":"developers/investigations/transparent-multi-cluster/#constraint-95-of-workloads-should-just-work-when-kubectl-applyd-to-kcp","title":"Constraint: 95% of workloads should \"just work\" when <code>kubectl apply</code>d to <code>kcp</code>","text":"<p>It continues to be possible to build different abstractions on top of Kube, but existing workloads are what really benefit users. They have chosen the Kube abstractions deliberately because they are general purpose - rather than describe a completely new system we believe it is more effective to uplevel these existing apps.  That means that existing primitives like Service, Deployment, PersistentVolumeClaim, StatefulSet must all require no changes to move from single-cluster to multi-cluster</p> <p>By choosing this constraint, we also accept that we will have to be opinionated on making the underlying clusters consistent, and we will have to limit / constrain certain behaviors. Ideally, we focus on preserving the user's view of the changes on a logical cluster, while making the workloads on a physical cluster look more consistent for infrastructure admins. This implies we need to explore both what these workloads might look like (a review of applications) and describe the points of control / abstraction between levels.</p>"},{"location":"developers/investigations/transparent-multi-cluster/#constraint-90-of-application-infrastructure-controllers-should-be-useful-against-kcp","title":"Constraint: 90% of application infrastructure controllers should be useful against <code>kcp</code>","text":"<p>A controller that performs app infra related functions should be useful without change against <code>kcp</code>. For instance, the etcd operator takes an <code>Etcd</code> cluster CRD and creates pods. It should be possible for that controller to target a <code>kcp</code> logical cluster with the CRD and create pods on the logical cluster that are transparently placed onto a cluster.</p>"},{"location":"developers/investigations/transparent-multi-cluster/#areas-of-investigation","title":"Areas of investigation","text":"<ol> <li>Define high level user use cases</li> <li>Study approaches from the ecosystem that do not require workloads to change significantly to spread</li> <li>Explore characteristics of most common Kube workload objects that could allow them to be transparently placed</li> <li>Identify the control points and data flow between workload and physical cluster that would be generally useful across a wide range of approaches - such as:<ol> <li>How placement is assigned, altered, and removed (\"scheduling\" or \"placement\")</li> <li>How workloads are transformed from high level to low level and then summarized back</li> <li>Categorize approaches in the ecosystem and gaps where collaboration could improve velocity 4.</li> </ol> </li> <li>Identify key infrastructure characteristics for multi-cluster<ol> <li>Networking between components and transparency of location to movement</li> <li>Data movement, placement, and replication</li> <li>Abstraction/interception of off-cluster dependencies (external to the system)</li> <li>Consistency of infrastructure (where does Kube not sufficiently drive operational consistency)</li> </ol> </li> <li>Seek consensus in user communities on whether the abstractions are practical</li> <li>Invest in key technologies in the appropriate projects</li> <li>Formalize parts of the prototype into project(s) drawing on the elements above if successful!</li> </ol>"},{"location":"developers/investigations/transparent-multi-cluster/#use-cases","title":"Use cases","text":"<p>Representing feedback from a number of multi-cluster users with a diverse set of technologies in play:</p>"},{"location":"developers/investigations/transparent-multi-cluster/#as-a-user","title":"As a user","text":"<ol> <li>I can <code>kubectl apply</code> a workload that is agnostic to node placement to <code>kcp</code> and see the workload assigned to real resources and start running and the status summarized back to me.</li> <li>I can move an application (defined in 1) between two physical clusters by changing a single high level attribute</li> <li>As a user when I move an application (as defined in 2) no disruption of internal or external traffic is visible to my consumers</li> <li>As a user I can debug my application in a familiar manner regardless of cluster</li> <li>As a user with a stateful application by persistent volumes can move / replicate / be shared across clusters in a manner consistent with my storage type (read-write-one / read-write-many).</li> </ol>"},{"location":"developers/investigations/transparent-multi-cluster/#as-an-infrastructure-admin","title":"As an infrastructure admin","text":"<ol> <li>I can decommission a physical cluster and see workloads moved without disruption</li> <li>I can set capacity bounds that control admission to a particular cluster and react to workload growth organically</li> </ol>"},{"location":"developers/investigations/transparent-multi-cluster/#progress","title":"Progress","text":"<p>In the early prototype stage <code>kcp</code> uses the <code>syncer</code> and the <code>deployment-splitter</code> as stand-ins for more complex scheduling and transformation. This section should see more updates in the near term as we move beyond areas 1-2 (use cases and ecosystem research)</p>"},{"location":"developers/investigations/transparent-multi-cluster/#possible-design-simplifications","title":"Possible design simplifications","text":"<ol> <li>Focus on every object having an annotation saying which clusters it is targeted at</li> <li>We can control the annotation via admission eventually, works for all objects</li> <li>Tracking declarative and atomic state change (NONE -&gt; A, A-&gt;(A,B), A-&gt;NONE) on      objects</li> <li>RBAC stays at the higher level and applies to the logical clusters, is not synced</li> <li>Implication is that controllers won't be syncable today, BUT that's ok because it's likely giving workloads control over the underlying cluster is a non-goal to start and would have to be explicit opt-in by admin</li> <li>Controllers already need to separate input from output - most controllers assume they're the same (but things like service load balancer)</li> <li>Think of scheduling as a policy at global, per logical cluster, and optionally the namespace (policy of object type -&gt; 0..N clusters)</li> <li>Simplification over doing a bunch of per object work, since we want to be transparent (per object is a future optimization with some limits)</li> </ol>"},{"location":"reference/","title":"Reference","text":"<p>This chapter provides automatically generated references for the kcp <code>kubectl</code> plugin and APIs included in kcp.</p>"},{"location":"reference/cli/create_workspace/","title":"create workspace","text":"<pre><code>create workspace [flags]\n</code></pre>"},{"location":"reference/cli/create_workspace/#examples","title":"Examples","text":"<pre><code>kubectl create workspace &lt;workspace name&gt; [--type=&lt;type&gt;] [--enter [--ignore-not-ready]] --ignore-existing\n</code></pre>"},{"location":"reference/cli/create_workspace/#options","title":"Options","text":"<pre><code>      --add_dir_header                   If true, adds the file directory to the header of the log messages\n      --alsologtostderr                  log to standard error as well as files (no effect when -logtostderr=true)\n      --as-uid string                    UID to impersonate for the operation\n      --certificate-authority string     Path to a cert file for the certificate authority\n      --context string                   The name of the kubeconfig context to use\n      --disable-compression              If true, opt-out of response compression for all requests to the server\n      --enter                            Immediately enter the created workspace\n  -h, --help                             help for workspace\n      --ignore-existing                  Ignore if the workspace already exists. Requires none or absolute type path.\n      --insecure-skip-tls-verify         If true, the server's certificate will not be checked for validity. This will make your HTTPS connections insecure\n      --kubeconfig string                path to the kubeconfig file\n      --location-selector string         A label selector to select the scheduling location of the created workspace.\n      --log_backtrace_at traceLocation   when logging hits line file:N, emit a stack trace (default :0)\n      --log_dir string                   If non-empty, write log files in this directory (no effect when -logtostderr=true)\n      --log_file string                  If non-empty, use this log file (no effect when -logtostderr=true)\n      --log_file_max_size uint           Defines the maximum size a log file can grow to (no effect when -logtostderr=true). Unit is megabytes. If the value is 0, the maximum file size is unlimited. (default 1800)\n      --logtostderr                      log to standard error instead of files (default true)\n  -n, --namespace string                 If present, the namespace scope for this CLI request\n      --one_output                       If true, only write logs to their native severity level (vs also writing to each lower severity level; no effect when -logtostderr=true)\n      --password string                  Password for basic authentication to the API server\n      --proxy-url string                 If provided, this URL will be used to connect via proxy\n      --server string                    The address and port of the Kubernetes API server\n      --skip_headers                     If true, avoid header prefixes in the log messages\n      --skip_log_headers                 If true, avoid headers when opening log files (no effect when -logtostderr=true)\n      --stderrthreshold severity         logs at or above this threshold go to stderr when writing to files and stderr (no effect when -logtostderr=true or -alsologtostderr=true) (default 2)\n      --tls-server-name string           If provided, this name will be used to validate server certificate. If this is not provided, hostname used to contact the server is used.\n      --token string                     Bearer token for authentication to the API server\n      --type string                      A workspace type. The default type depends on where this child workspace is created.\n      --user string                      The name of the kubeconfig user to use\n      --username string                  Username for basic authentication to the API server\n  -v, --v Level                          number for the log level verbosity\n      --vmodule moduleSpec               comma-separated list of pattern=N settings for file-filtered logging\n</code></pre>"},{"location":"reference/cli/create_workspace/#see-also","title":"SEE ALSO","text":"<ul> <li>create  - </li> </ul>"},{"location":"reference/cli/kcp/","title":"kcp","text":""},{"location":"reference/cli/kcp/#synopsis","title":"Synopsis","text":"<p>KCP is the easiest way to manage Kubernetes applications against one or more clusters, by giving you a personal control plane that schedules your workloads onto one or many clusters, and making it simple to pick up and move. Advanced use cases including spreading your apps across clusters for resiliency, scheduling batch workloads onto clusters with free capacity, and enabling collaboration for individual teams without having access to the underlying clusters.</p> <p>This command provides KCP specific sub-command for kubectl.</p>"},{"location":"reference/cli/kcp/#options","title":"Options","text":"<pre><code>      --add_dir_header                   If true, adds the file directory to the header of the log messages\n      --alsologtostderr                  log to standard error as well as files (no effect when -logtostderr=true)\n  -h, --help                             help for kcp\n      --log_backtrace_at traceLocation   when logging hits line file:N, emit a stack trace (default :0)\n      --log_dir string                   If non-empty, write log files in this directory (no effect when -logtostderr=true)\n      --log_file string                  If non-empty, use this log file (no effect when -logtostderr=true)\n      --log_file_max_size uint           Defines the maximum size a log file can grow to (no effect when -logtostderr=true). Unit is megabytes. If the value is 0, the maximum file size is unlimited. (default 1800)\n      --logtostderr                      log to standard error instead of files (default true)\n      --one_output                       If true, only write logs to their native severity level (vs also writing to each lower severity level; no effect when -logtostderr=true)\n      --skip_headers                     If true, avoid header prefixes in the log messages\n      --skip_log_headers                 If true, avoid headers when opening log files (no effect when -logtostderr=true)\n      --stderrthreshold severity         logs at or above this threshold go to stderr when writing to files and stderr (no effect when -logtostderr=true or -alsologtostderr=true) (default 2)\n  -v, --v Level                          number for the log level verbosity\n      --vmodule moduleSpec               comma-separated list of pattern=N settings for file-filtered logging\n</code></pre>"},{"location":"reference/cli/kcp/#see-also","title":"SEE ALSO","text":"<ul> <li>kcp bind  - Bind different types into current workspace.</li> <li>kcp claims  - Operations related to viewing or updating permission claims</li> <li>kcp crd    - CRD related operations</li> <li>kcp workspace    - Manages KCP workspaces</li> </ul>"},{"location":"reference/cli/kcp_bind/","title":"kcp bind","text":"<pre><code>kcp bind [flags]\n</code></pre>"},{"location":"reference/cli/kcp_bind/#options","title":"Options","text":"<pre><code>  -h, --help   help for bind\n</code></pre>"},{"location":"reference/cli/kcp_bind/#options-inherited-from-parent-commands","title":"Options inherited from parent commands","text":"<pre><code>      --add_dir_header                   If true, adds the file directory to the header of the log messages\n      --alsologtostderr                  log to standard error as well as files (no effect when -logtostderr=true)\n      --log_backtrace_at traceLocation   when logging hits line file:N, emit a stack trace (default :0)\n      --log_dir string                   If non-empty, write log files in this directory (no effect when -logtostderr=true)\n      --log_file string                  If non-empty, use this log file (no effect when -logtostderr=true)\n      --log_file_max_size uint           Defines the maximum size a log file can grow to (no effect when -logtostderr=true). Unit is megabytes. If the value is 0, the maximum file size is unlimited. (default 1800)\n      --logtostderr                      log to standard error instead of files (default true)\n      --one_output                       If true, only write logs to their native severity level (vs also writing to each lower severity level; no effect when -logtostderr=true)\n      --skip_headers                     If true, avoid header prefixes in the log messages\n      --skip_log_headers                 If true, avoid headers when opening log files (no effect when -logtostderr=true)\n      --stderrthreshold severity         logs at or above this threshold go to stderr when writing to files and stderr (no effect when -logtostderr=true or -alsologtostderr=true) (default 2)\n  -v, --v Level                          number for the log level verbosity\n      --vmodule moduleSpec               comma-separated list of pattern=N settings for file-filtered logging\n</code></pre>"},{"location":"reference/cli/kcp_bind/#see-also","title":"SEE ALSO","text":"<ul> <li>kcp    - kubectl plugin for KCP</li> <li>kcp bind apiexport  - Bind to an APIExport</li> </ul>"},{"location":"reference/cli/kcp_bind_apiexport/","title":"kcp bind apiexport","text":"<pre><code>kcp bind apiexport &lt;workspace_path:apiexport-name&gt; [flags]\n</code></pre>"},{"location":"reference/cli/kcp_bind_apiexport/#examples","title":"Examples","text":"<pre><code>    # Create an APIBinding named \"my-binding\" that binds to the APIExport \"my-export\" in the \"root:my-service\" workspace.\n    kubectl kcp bind apiexport root:my-service:my-export --name my-binding\n</code></pre>"},{"location":"reference/cli/kcp_bind_apiexport/#options","title":"Options","text":"<pre><code>      --as-uid string                  UID to impersonate for the operation\n      --certificate-authority string   Path to a cert file for the certificate authority\n      --context string                 The name of the kubeconfig context to use\n      --disable-compression            If true, opt-out of response compression for all requests to the server\n  -h, --help                           help for apiexport\n      --insecure-skip-tls-verify       If true, the server's certificate will not be checked for validity. This will make your HTTPS connections insecure\n      --kubeconfig string              path to the kubeconfig file\n      --name string                    Name of the APIBinding to create.\n  -n, --namespace string               If present, the namespace scope for this CLI request\n      --password string                Password for basic authentication to the API server\n      --proxy-url string               If provided, this URL will be used to connect via proxy\n      --server string                  The address and port of the Kubernetes API server\n      --timeout duration               Duration to wait for APIBinding to be created successfully. (default 30s)\n      --tls-server-name string         If provided, this name will be used to validate server certificate. If this is not provided, hostname used to contact the server is used.\n      --token string                   Bearer token for authentication to the API server\n      --user string                    The name of the kubeconfig user to use\n      --username string                Username for basic authentication to the API server\n</code></pre>"},{"location":"reference/cli/kcp_bind_apiexport/#options-inherited-from-parent-commands","title":"Options inherited from parent commands","text":"<pre><code>      --add_dir_header                   If true, adds the file directory to the header of the log messages\n      --alsologtostderr                  log to standard error as well as files (no effect when -logtostderr=true)\n      --log_backtrace_at traceLocation   when logging hits line file:N, emit a stack trace (default :0)\n      --log_dir string                   If non-empty, write log files in this directory (no effect when -logtostderr=true)\n      --log_file string                  If non-empty, use this log file (no effect when -logtostderr=true)\n      --log_file_max_size uint           Defines the maximum size a log file can grow to (no effect when -logtostderr=true). Unit is megabytes. If the value is 0, the maximum file size is unlimited. (default 1800)\n      --logtostderr                      log to standard error instead of files (default true)\n      --one_output                       If true, only write logs to their native severity level (vs also writing to each lower severity level; no effect when -logtostderr=true)\n      --skip_headers                     If true, avoid header prefixes in the log messages\n      --skip_log_headers                 If true, avoid headers when opening log files (no effect when -logtostderr=true)\n      --stderrthreshold severity         logs at or above this threshold go to stderr when writing to files and stderr (no effect when -logtostderr=true or -alsologtostderr=true) (default 2)\n  -v, --v Level                          number for the log level verbosity\n      --vmodule moduleSpec               comma-separated list of pattern=N settings for file-filtered logging\n</code></pre>"},{"location":"reference/cli/kcp_bind_apiexport/#see-also","title":"SEE ALSO","text":"<ul> <li>kcp bind  - Bind different types into current workspace.</li> </ul>"},{"location":"reference/cli/kcp_claims/","title":"kcp claims","text":"<pre><code>kcp claims [flags]\n</code></pre>"},{"location":"reference/cli/kcp_claims/#examples","title":"Examples","text":"<pre><code>    # Lists the permission claims and their respective status related to a specific APIBinding.\n    kubectl claims get apibinding cert-manager\n\n    # List permission claims and their respective status for all APIBindings in current workspace.\n    kubectl claims get apibinding\n</code></pre>"},{"location":"reference/cli/kcp_claims/#options","title":"Options","text":"<pre><code>  -h, --help   help for claims\n</code></pre>"},{"location":"reference/cli/kcp_claims/#options-inherited-from-parent-commands","title":"Options inherited from parent commands","text":"<pre><code>      --add_dir_header                   If true, adds the file directory to the header of the log messages\n      --alsologtostderr                  log to standard error as well as files (no effect when -logtostderr=true)\n      --log_backtrace_at traceLocation   when logging hits line file:N, emit a stack trace (default :0)\n      --log_dir string                   If non-empty, write log files in this directory (no effect when -logtostderr=true)\n      --log_file string                  If non-empty, use this log file (no effect when -logtostderr=true)\n      --log_file_max_size uint           Defines the maximum size a log file can grow to (no effect when -logtostderr=true). Unit is megabytes. If the value is 0, the maximum file size is unlimited. (default 1800)\n      --logtostderr                      log to standard error instead of files (default true)\n      --one_output                       If true, only write logs to their native severity level (vs also writing to each lower severity level; no effect when -logtostderr=true)\n      --skip_headers                     If true, avoid header prefixes in the log messages\n      --skip_log_headers                 If true, avoid headers when opening log files (no effect when -logtostderr=true)\n      --stderrthreshold severity         logs at or above this threshold go to stderr when writing to files and stderr (no effect when -logtostderr=true or -alsologtostderr=true) (default 2)\n  -v, --v Level                          number for the log level verbosity\n      --vmodule moduleSpec               comma-separated list of pattern=N settings for file-filtered logging\n</code></pre>"},{"location":"reference/cli/kcp_claims/#see-also","title":"SEE ALSO","text":"<ul> <li>kcp    - kubectl plugin for KCP</li> <li>kcp claims get  - Operations related to fetching APIs with respect to permission claims</li> </ul>"},{"location":"reference/cli/kcp_claims_get/","title":"kcp claims get","text":"<pre><code>kcp claims get [flags]\n</code></pre>"},{"location":"reference/cli/kcp_claims_get/#options","title":"Options","text":"<pre><code>  -h, --help   help for get\n</code></pre>"},{"location":"reference/cli/kcp_claims_get/#options-inherited-from-parent-commands","title":"Options inherited from parent commands","text":"<pre><code>      --add_dir_header                   If true, adds the file directory to the header of the log messages\n      --alsologtostderr                  log to standard error as well as files (no effect when -logtostderr=true)\n      --log_backtrace_at traceLocation   when logging hits line file:N, emit a stack trace (default :0)\n      --log_dir string                   If non-empty, write log files in this directory (no effect when -logtostderr=true)\n      --log_file string                  If non-empty, use this log file (no effect when -logtostderr=true)\n      --log_file_max_size uint           Defines the maximum size a log file can grow to (no effect when -logtostderr=true). Unit is megabytes. If the value is 0, the maximum file size is unlimited. (default 1800)\n      --logtostderr                      log to standard error instead of files (default true)\n      --one_output                       If true, only write logs to their native severity level (vs also writing to each lower severity level; no effect when -logtostderr=true)\n      --skip_headers                     If true, avoid header prefixes in the log messages\n      --skip_log_headers                 If true, avoid headers when opening log files (no effect when -logtostderr=true)\n      --stderrthreshold severity         logs at or above this threshold go to stderr when writing to files and stderr (no effect when -logtostderr=true or -alsologtostderr=true) (default 2)\n  -v, --v Level                          number for the log level verbosity\n      --vmodule moduleSpec               comma-separated list of pattern=N settings for file-filtered logging\n</code></pre>"},{"location":"reference/cli/kcp_claims_get/#see-also","title":"SEE ALSO","text":"<ul> <li>kcp claims  - Operations related to viewing or updating permission claims</li> <li>kcp claims get apibinding    - Get claims related to apibinding</li> </ul>"},{"location":"reference/cli/kcp_claims_get_apibinding/","title":"kcp claims get apibinding","text":"<pre><code>kcp claims get apibinding &lt;apibinding_name&gt; [flags]\n</code></pre>"},{"location":"reference/cli/kcp_claims_get_apibinding/#options","title":"Options","text":"<pre><code>      --as-uid string                  UID to impersonate for the operation\n      --certificate-authority string   Path to a cert file for the certificate authority\n      --context string                 The name of the kubeconfig context to use\n      --disable-compression            If true, opt-out of response compression for all requests to the server\n  -h, --help                           help for apibinding\n      --insecure-skip-tls-verify       If true, the server's certificate will not be checked for validity. This will make your HTTPS connections insecure\n      --kubeconfig string              path to the kubeconfig file\n  -n, --namespace string               If present, the namespace scope for this CLI request\n      --password string                Password for basic authentication to the API server\n      --proxy-url string               If provided, this URL will be used to connect via proxy\n      --server string                  The address and port of the Kubernetes API server\n      --tls-server-name string         If provided, this name will be used to validate server certificate. If this is not provided, hostname used to contact the server is used.\n      --token string                   Bearer token for authentication to the API server\n      --user string                    The name of the kubeconfig user to use\n      --username string                Username for basic authentication to the API server\n</code></pre>"},{"location":"reference/cli/kcp_claims_get_apibinding/#options-inherited-from-parent-commands","title":"Options inherited from parent commands","text":"<pre><code>      --add_dir_header                   If true, adds the file directory to the header of the log messages\n      --alsologtostderr                  log to standard error as well as files (no effect when -logtostderr=true)\n      --log_backtrace_at traceLocation   when logging hits line file:N, emit a stack trace (default :0)\n      --log_dir string                   If non-empty, write log files in this directory (no effect when -logtostderr=true)\n      --log_file string                  If non-empty, use this log file (no effect when -logtostderr=true)\n      --log_file_max_size uint           Defines the maximum size a log file can grow to (no effect when -logtostderr=true). Unit is megabytes. If the value is 0, the maximum file size is unlimited. (default 1800)\n      --logtostderr                      log to standard error instead of files (default true)\n      --one_output                       If true, only write logs to their native severity level (vs also writing to each lower severity level; no effect when -logtostderr=true)\n      --skip_headers                     If true, avoid header prefixes in the log messages\n      --skip_log_headers                 If true, avoid headers when opening log files (no effect when -logtostderr=true)\n      --stderrthreshold severity         logs at or above this threshold go to stderr when writing to files and stderr (no effect when -logtostderr=true or -alsologtostderr=true) (default 2)\n  -v, --v Level                          number for the log level verbosity\n      --vmodule moduleSpec               comma-separated list of pattern=N settings for file-filtered logging\n</code></pre>"},{"location":"reference/cli/kcp_claims_get_apibinding/#see-also","title":"SEE ALSO","text":"<ul> <li>kcp claims get  - Operations related to fetching APIs with respect to permission claims</li> </ul>"},{"location":"reference/cli/kcp_crd/","title":"kcp crd","text":"<pre><code>kcp crd [flags]\n</code></pre>"},{"location":"reference/cli/kcp_crd/#options","title":"Options","text":"<pre><code>  -h, --help   help for crd\n</code></pre>"},{"location":"reference/cli/kcp_crd/#options-inherited-from-parent-commands","title":"Options inherited from parent commands","text":"<pre><code>      --add_dir_header                   If true, adds the file directory to the header of the log messages\n      --alsologtostderr                  log to standard error as well as files (no effect when -logtostderr=true)\n      --log_backtrace_at traceLocation   when logging hits line file:N, emit a stack trace (default :0)\n      --log_dir string                   If non-empty, write log files in this directory (no effect when -logtostderr=true)\n      --log_file string                  If non-empty, use this log file (no effect when -logtostderr=true)\n      --log_file_max_size uint           Defines the maximum size a log file can grow to (no effect when -logtostderr=true). Unit is megabytes. If the value is 0, the maximum file size is unlimited. (default 1800)\n      --logtostderr                      log to standard error instead of files (default true)\n      --one_output                       If true, only write logs to their native severity level (vs also writing to each lower severity level; no effect when -logtostderr=true)\n      --skip_headers                     If true, avoid header prefixes in the log messages\n      --skip_log_headers                 If true, avoid headers when opening log files (no effect when -logtostderr=true)\n      --stderrthreshold severity         logs at or above this threshold go to stderr when writing to files and stderr (no effect when -logtostderr=true or -alsologtostderr=true) (default 2)\n  -v, --v Level                          number for the log level verbosity\n      --vmodule moduleSpec               comma-separated list of pattern=N settings for file-filtered logging\n</code></pre>"},{"location":"reference/cli/kcp_crd/#see-also","title":"SEE ALSO","text":"<ul> <li>kcp    - kubectl plugin for KCP</li> <li>kcp crd snapshot  - Snapshot a CRD and convert it to an APIResourceSchema</li> </ul>"},{"location":"reference/cli/kcp_crd_snapshot/","title":"kcp crd snapshot","text":"<pre><code>kcp crd snapshot -f FILE --prefix PREFIX [flags]\n</code></pre>"},{"location":"reference/cli/kcp_crd_snapshot/#examples","title":"Examples","text":"<pre><code>    # Convert a CRD in a yaml file to an APIResourceSchema. For a CRD named widgets.example.io, and a prefix value of\n    # 'today', the new APIResourceSchema's name will be today.widgets.example.io.\n    kubectl kcp crd snapshot -f crd.yaml --prefix 2022-05-07 &gt; api-resource-schema.yaml\n\n    # Convert a CRD from STDIN\n    kubectl get crd foo -o yaml | kubectl kcp crd snapshot -f - --prefix today &gt; output.yaml\n</code></pre>"},{"location":"reference/cli/kcp_crd_snapshot/#options","title":"Options","text":"<pre><code>  -f, --filename string   Path to a file containing the CRD to convert to an APIResourceSchema, or - for stdin\n  -h, --help              help for snapshot\n  -o, --output string     Output format. Valid values are 'json' and 'yaml' (default \"yaml\")\n      --prefix string     Prefix to use for the APIResourceSchema's name, before &lt;resource&gt;.&lt;group&gt;\n</code></pre>"},{"location":"reference/cli/kcp_crd_snapshot/#options-inherited-from-parent-commands","title":"Options inherited from parent commands","text":"<pre><code>      --add_dir_header                   If true, adds the file directory to the header of the log messages\n      --alsologtostderr                  log to standard error as well as files (no effect when -logtostderr=true)\n      --log_backtrace_at traceLocation   when logging hits line file:N, emit a stack trace (default :0)\n      --log_dir string                   If non-empty, write log files in this directory (no effect when -logtostderr=true)\n      --log_file string                  If non-empty, use this log file (no effect when -logtostderr=true)\n      --log_file_max_size uint           Defines the maximum size a log file can grow to (no effect when -logtostderr=true). Unit is megabytes. If the value is 0, the maximum file size is unlimited. (default 1800)\n      --logtostderr                      log to standard error instead of files (default true)\n      --one_output                       If true, only write logs to their native severity level (vs also writing to each lower severity level; no effect when -logtostderr=true)\n      --skip_headers                     If true, avoid header prefixes in the log messages\n      --skip_log_headers                 If true, avoid headers when opening log files (no effect when -logtostderr=true)\n      --stderrthreshold severity         logs at or above this threshold go to stderr when writing to files and stderr (no effect when -logtostderr=true or -alsologtostderr=true) (default 2)\n  -v, --v Level                          number for the log level verbosity\n      --vmodule moduleSpec               comma-separated list of pattern=N settings for file-filtered logging\n</code></pre>"},{"location":"reference/cli/kcp_crd_snapshot/#see-also","title":"SEE ALSO","text":"<ul> <li>kcp crd    - CRD related operations</li> </ul>"},{"location":"reference/cli/kcp_workspace/","title":"kcp workspace","text":"<pre><code>kcp workspace [create|create-context|use|current|&lt;workspace&gt;|..|.|-|~|&lt;root:absolute:workspace&gt;] [flags]\n</code></pre>"},{"location":"reference/cli/kcp_workspace/#examples","title":"Examples","text":"<pre><code>    # shows the workspace you are currently using\n    kubectl workspace .\n\n    # enter a given workspace (this will change the current-context of your current KUBECONFIG)\n    kubectl workspace use my-workspace\n\n    # short-hand for the use syntax\n    kubectl workspace my-workspace\n\n    # enter a given absolute workspace\n    kubectl workspace :root:default:my-workspace\n\n    # short-hand for the current root workspace\n    kubectl workspace :\n\n    # enter a given relative workspace\n    kubectl workspace some:nested:workspace\n\n    # enter the parent workspace\n    kubectl workspace ..\n\n    # enter the grand parent workspace\n    kubectl workspace ..:..\n\n    # enter the previous workspace\n    kubectl workspace -\n\n    # go to your home workspace\n    kubectl workspace\n\n    # create a workspace and immediately enter it\n    kubectl workspace create my-workspace --enter\n\n    # create a context with the current workspace, e.g. root:default:my-workspace\n    kubectl workspace create-context\n\n    # create a context with the current workspace, named context-name\n    kubectl workspace create-context context-name\n</code></pre>"},{"location":"reference/cli/kcp_workspace/#options","title":"Options","text":"<pre><code>      --as-uid string                  UID to impersonate for the operation\n      --certificate-authority string   Path to a cert file for the certificate authority\n      --context string                 The name of the kubeconfig context to use\n      --disable-compression            If true, opt-out of response compression for all requests to the server\n  -h, --help                           help for workspace\n      --insecure-skip-tls-verify       If true, the server's certificate will not be checked for validity. This will make your HTTPS connections insecure\n      --kubeconfig string              path to the kubeconfig file\n  -n, --namespace string               If present, the namespace scope for this CLI request\n      --password string                Password for basic authentication to the API server\n      --proxy-url string               If provided, this URL will be used to connect via proxy\n      --server string                  The address and port of the Kubernetes API server\n      --short                          Print only the name of the workspace, e.g. for integration into the shell prompt\n      --tls-server-name string         If provided, this name will be used to validate server certificate. If this is not provided, hostname used to contact the server is used.\n      --token string                   Bearer token for authentication to the API server\n      --user string                    The name of the kubeconfig user to use\n      --username string                Username for basic authentication to the API server\n</code></pre>"},{"location":"reference/cli/kcp_workspace/#options-inherited-from-parent-commands","title":"Options inherited from parent commands","text":"<pre><code>      --add_dir_header                   If true, adds the file directory to the header of the log messages\n      --alsologtostderr                  log to standard error as well as files (no effect when -logtostderr=true)\n      --log_backtrace_at traceLocation   when logging hits line file:N, emit a stack trace (default :0)\n      --log_dir string                   If non-empty, write log files in this directory (no effect when -logtostderr=true)\n      --log_file string                  If non-empty, use this log file (no effect when -logtostderr=true)\n      --log_file_max_size uint           Defines the maximum size a log file can grow to (no effect when -logtostderr=true). Unit is megabytes. If the value is 0, the maximum file size is unlimited. (default 1800)\n      --logtostderr                      log to standard error instead of files (default true)\n      --one_output                       If true, only write logs to their native severity level (vs also writing to each lower severity level; no effect when -logtostderr=true)\n      --skip_headers                     If true, avoid header prefixes in the log messages\n      --skip_log_headers                 If true, avoid headers when opening log files (no effect when -logtostderr=true)\n      --stderrthreshold severity         logs at or above this threshold go to stderr when writing to files and stderr (no effect when -logtostderr=true or -alsologtostderr=true) (default 2)\n  -v, --v Level                          number for the log level verbosity\n      --vmodule moduleSpec               comma-separated list of pattern=N settings for file-filtered logging\n</code></pre>"},{"location":"reference/cli/kcp_workspace/#see-also","title":"SEE ALSO","text":"<ul> <li>kcp    - kubectl plugin for KCP</li> <li>kcp workspace create-context  - Create a kubeconfig context for the current workspace</li> <li>kcp workspace current    - Print the current workspace. Same as 'kubectl ws .'.</li> <li>kcp workspace tree  - Print the current workspace tree.</li> <li>kcp workspace use    - Uses the given workspace as the current workspace. Using - means previous workspace, .. means parent workspace, . mean current, ~ means home workspace</li> </ul>"},{"location":"reference/cli/kcp_workspace_create-context/","title":"kcp workspace create-context","text":"<pre><code>kcp workspace create-context [&lt;context-name&gt;] [--overwrite] [flags]\n</code></pre>"},{"location":"reference/cli/kcp_workspace_create-context/#examples","title":"Examples","text":"<pre><code>kcp workspace create-context\n</code></pre>"},{"location":"reference/cli/kcp_workspace_create-context/#options","title":"Options","text":"<pre><code>      --as-uid string                  UID to impersonate for the operation\n      --certificate-authority string   Path to a cert file for the certificate authority\n      --context string                 The name of the kubeconfig context to use\n      --disable-compression            If true, opt-out of response compression for all requests to the server\n  -h, --help                           help for create-context\n      --insecure-skip-tls-verify       If true, the server's certificate will not be checked for validity. This will make your HTTPS connections insecure\n      --kubeconfig string              path to the kubeconfig file\n  -n, --namespace string               If present, the namespace scope for this CLI request\n      --overwrite                      Overwrite the context if it already exists\n      --password string                Password for basic authentication to the API server\n      --proxy-url string               If provided, this URL will be used to connect via proxy\n      --server string                  The address and port of the Kubernetes API server\n      --tls-server-name string         If provided, this name will be used to validate server certificate. If this is not provided, hostname used to contact the server is used.\n      --token string                   Bearer token for authentication to the API server\n      --user string                    The name of the kubeconfig user to use\n      --username string                Username for basic authentication to the API server\n</code></pre>"},{"location":"reference/cli/kcp_workspace_create-context/#options-inherited-from-parent-commands","title":"Options inherited from parent commands","text":"<pre><code>      --add_dir_header                   If true, adds the file directory to the header of the log messages\n      --alsologtostderr                  log to standard error as well as files (no effect when -logtostderr=true)\n      --log_backtrace_at traceLocation   when logging hits line file:N, emit a stack trace (default :0)\n      --log_dir string                   If non-empty, write log files in this directory (no effect when -logtostderr=true)\n      --log_file string                  If non-empty, use this log file (no effect when -logtostderr=true)\n      --log_file_max_size uint           Defines the maximum size a log file can grow to (no effect when -logtostderr=true). Unit is megabytes. If the value is 0, the maximum file size is unlimited. (default 1800)\n      --logtostderr                      log to standard error instead of files (default true)\n      --one_output                       If true, only write logs to their native severity level (vs also writing to each lower severity level; no effect when -logtostderr=true)\n      --skip_headers                     If true, avoid header prefixes in the log messages\n      --skip_log_headers                 If true, avoid headers when opening log files (no effect when -logtostderr=true)\n      --stderrthreshold severity         logs at or above this threshold go to stderr when writing to files and stderr (no effect when -logtostderr=true or -alsologtostderr=true) (default 2)\n  -v, --v Level                          number for the log level verbosity\n      --vmodule moduleSpec               comma-separated list of pattern=N settings for file-filtered logging\n</code></pre>"},{"location":"reference/cli/kcp_workspace_create-context/#see-also","title":"SEE ALSO","text":"<ul> <li>kcp workspace    - Manages KCP workspaces</li> </ul>"},{"location":"reference/cli/kcp_workspace_current/","title":"kcp workspace current","text":"<pre><code>kcp workspace current [--short] [flags]\n</code></pre>"},{"location":"reference/cli/kcp_workspace_current/#examples","title":"Examples","text":"<pre><code>kcp workspace current\n</code></pre>"},{"location":"reference/cli/kcp_workspace_current/#options","title":"Options","text":"<pre><code>      --as-uid string                  UID to impersonate for the operation\n      --certificate-authority string   Path to a cert file for the certificate authority\n      --context string                 The name of the kubeconfig context to use\n      --disable-compression            If true, opt-out of response compression for all requests to the server\n  -h, --help                           help for current\n      --insecure-skip-tls-verify       If true, the server's certificate will not be checked for validity. This will make your HTTPS connections insecure\n      --kubeconfig string              path to the kubeconfig file\n  -n, --namespace string               If present, the namespace scope for this CLI request\n      --password string                Password for basic authentication to the API server\n      --proxy-url string               If provided, this URL will be used to connect via proxy\n      --server string                  The address and port of the Kubernetes API server\n      --short                          Print only the name of the workspace, e.g. for integration into the shell prompt\n      --tls-server-name string         If provided, this name will be used to validate server certificate. If this is not provided, hostname used to contact the server is used.\n      --token string                   Bearer token for authentication to the API server\n      --user string                    The name of the kubeconfig user to use\n      --username string                Username for basic authentication to the API server\n</code></pre>"},{"location":"reference/cli/kcp_workspace_current/#options-inherited-from-parent-commands","title":"Options inherited from parent commands","text":"<pre><code>      --add_dir_header                   If true, adds the file directory to the header of the log messages\n      --alsologtostderr                  log to standard error as well as files (no effect when -logtostderr=true)\n      --log_backtrace_at traceLocation   when logging hits line file:N, emit a stack trace (default :0)\n      --log_dir string                   If non-empty, write log files in this directory (no effect when -logtostderr=true)\n      --log_file string                  If non-empty, use this log file (no effect when -logtostderr=true)\n      --log_file_max_size uint           Defines the maximum size a log file can grow to (no effect when -logtostderr=true). Unit is megabytes. If the value is 0, the maximum file size is unlimited. (default 1800)\n      --logtostderr                      log to standard error instead of files (default true)\n      --one_output                       If true, only write logs to their native severity level (vs also writing to each lower severity level; no effect when -logtostderr=true)\n      --skip_headers                     If true, avoid header prefixes in the log messages\n      --skip_log_headers                 If true, avoid headers when opening log files (no effect when -logtostderr=true)\n      --stderrthreshold severity         logs at or above this threshold go to stderr when writing to files and stderr (no effect when -logtostderr=true or -alsologtostderr=true) (default 2)\n  -v, --v Level                          number for the log level verbosity\n      --vmodule moduleSpec               comma-separated list of pattern=N settings for file-filtered logging\n</code></pre>"},{"location":"reference/cli/kcp_workspace_current/#see-also","title":"SEE ALSO","text":"<ul> <li>kcp workspace    - Manages KCP workspaces</li> </ul>"},{"location":"reference/cli/kcp_workspace_tree/","title":"kcp workspace tree","text":"<pre><code>kcp workspace tree [flags]\n</code></pre>"},{"location":"reference/cli/kcp_workspace_tree/#examples","title":"Examples","text":"<pre><code>kcp workspace tree\n</code></pre>"},{"location":"reference/cli/kcp_workspace_tree/#options","title":"Options","text":"<pre><code>      --as-uid string                  UID to impersonate for the operation\n      --certificate-authority string   Path to a cert file for the certificate authority\n      --context string                 The name of the kubeconfig context to use\n      --disable-compression            If true, opt-out of response compression for all requests to the server\n  -f, --full                           Show full workspace names\n  -h, --help                           help for tree\n      --insecure-skip-tls-verify       If true, the server's certificate will not be checked for validity. This will make your HTTPS connections insecure\n      --kubeconfig string              path to the kubeconfig file\n  -n, --namespace string               If present, the namespace scope for this CLI request\n      --password string                Password for basic authentication to the API server\n      --proxy-url string               If provided, this URL will be used to connect via proxy\n      --server string                  The address and port of the Kubernetes API server\n      --tls-server-name string         If provided, this name will be used to validate server certificate. If this is not provided, hostname used to contact the server is used.\n      --token string                   Bearer token for authentication to the API server\n      --user string                    The name of the kubeconfig user to use\n      --username string                Username for basic authentication to the API server\n</code></pre>"},{"location":"reference/cli/kcp_workspace_tree/#options-inherited-from-parent-commands","title":"Options inherited from parent commands","text":"<pre><code>      --add_dir_header                   If true, adds the file directory to the header of the log messages\n      --alsologtostderr                  log to standard error as well as files (no effect when -logtostderr=true)\n      --log_backtrace_at traceLocation   when logging hits line file:N, emit a stack trace (default :0)\n      --log_dir string                   If non-empty, write log files in this directory (no effect when -logtostderr=true)\n      --log_file string                  If non-empty, use this log file (no effect when -logtostderr=true)\n      --log_file_max_size uint           Defines the maximum size a log file can grow to (no effect when -logtostderr=true). Unit is megabytes. If the value is 0, the maximum file size is unlimited. (default 1800)\n      --logtostderr                      log to standard error instead of files (default true)\n      --one_output                       If true, only write logs to their native severity level (vs also writing to each lower severity level; no effect when -logtostderr=true)\n      --skip_headers                     If true, avoid header prefixes in the log messages\n      --skip_log_headers                 If true, avoid headers when opening log files (no effect when -logtostderr=true)\n      --stderrthreshold severity         logs at or above this threshold go to stderr when writing to files and stderr (no effect when -logtostderr=true or -alsologtostderr=true) (default 2)\n  -v, --v Level                          number for the log level verbosity\n      --vmodule moduleSpec               comma-separated list of pattern=N settings for file-filtered logging\n</code></pre>"},{"location":"reference/cli/kcp_workspace_tree/#see-also","title":"SEE ALSO","text":"<ul> <li>kcp workspace    - Manages KCP workspaces</li> </ul>"},{"location":"reference/cli/kcp_workspace_use/","title":"kcp workspace use","text":"<pre><code>kcp workspace use &lt;workspace&gt;|..|.|-|~|&lt;:root:absolute:workspace&gt;|&lt;relative:workspace&gt; [flags]\n</code></pre>"},{"location":"reference/cli/kcp_workspace_use/#options","title":"Options","text":"<pre><code>      --as-uid string                  UID to impersonate for the operation\n      --certificate-authority string   Path to a cert file for the certificate authority\n      --context string                 The name of the kubeconfig context to use\n      --disable-compression            If true, opt-out of response compression for all requests to the server\n  -h, --help                           help for use\n      --insecure-skip-tls-verify       If true, the server's certificate will not be checked for validity. This will make your HTTPS connections insecure\n      --kubeconfig string              path to the kubeconfig file\n  -n, --namespace string               If present, the namespace scope for this CLI request\n      --password string                Password for basic authentication to the API server\n      --proxy-url string               If provided, this URL will be used to connect via proxy\n      --server string                  The address and port of the Kubernetes API server\n      --short                          Print only the name of the workspace, e.g. for integration into the shell prompt\n      --tls-server-name string         If provided, this name will be used to validate server certificate. If this is not provided, hostname used to contact the server is used.\n      --token string                   Bearer token for authentication to the API server\n      --user string                    The name of the kubeconfig user to use\n      --username string                Username for basic authentication to the API server\n</code></pre>"},{"location":"reference/cli/kcp_workspace_use/#options-inherited-from-parent-commands","title":"Options inherited from parent commands","text":"<pre><code>      --add_dir_header                   If true, adds the file directory to the header of the log messages\n      --alsologtostderr                  log to standard error as well as files (no effect when -logtostderr=true)\n      --log_backtrace_at traceLocation   when logging hits line file:N, emit a stack trace (default :0)\n      --log_dir string                   If non-empty, write log files in this directory (no effect when -logtostderr=true)\n      --log_file string                  If non-empty, use this log file (no effect when -logtostderr=true)\n      --log_file_max_size uint           Defines the maximum size a log file can grow to (no effect when -logtostderr=true). Unit is megabytes. If the value is 0, the maximum file size is unlimited. (default 1800)\n      --logtostderr                      log to standard error instead of files (default true)\n      --one_output                       If true, only write logs to their native severity level (vs also writing to each lower severity level; no effect when -logtostderr=true)\n      --skip_headers                     If true, avoid header prefixes in the log messages\n      --skip_log_headers                 If true, avoid headers when opening log files (no effect when -logtostderr=true)\n      --stderrthreshold severity         logs at or above this threshold go to stderr when writing to files and stderr (no effect when -logtostderr=true or -alsologtostderr=true) (default 2)\n  -v, --v Level                          number for the log level verbosity\n      --vmodule moduleSpec               comma-separated list of pattern=N settings for file-filtered logging\n</code></pre>"},{"location":"reference/cli/kcp_workspace_use/#see-also","title":"SEE ALSO","text":"<ul> <li>kcp workspace    - Manages KCP workspaces</li> </ul>"},{"location":"reference/cli/workspace/","title":"workspace","text":"<pre><code>workspace [create|create-context|use|current|&lt;workspace&gt;|..|.|-|~|&lt;root:absolute:workspace&gt;] [flags]\n</code></pre>"},{"location":"reference/cli/workspace/#examples","title":"Examples","text":"<pre><code>    # shows the workspace you are currently using\n    kubectl workspace .\n\n    # enter a given workspace (this will change the current-context of your current KUBECONFIG)\n    kubectl workspace use my-workspace\n\n    # short-hand for the use syntax\n    kubectl workspace my-workspace\n\n    # enter a given absolute workspace\n    kubectl workspace :root:default:my-workspace\n\n    # short-hand for the current root workspace\n    kubectl workspace :\n\n    # enter a given relative workspace\n    kubectl workspace some:nested:workspace\n\n    # enter the parent workspace\n    kubectl workspace ..\n\n    # enter the grand parent workspace\n    kubectl workspace ..:..\n\n    # enter the previous workspace\n    kubectl workspace -\n\n    # go to your home workspace\n    kubectl workspace\n\n    # create a workspace and immediately enter it\n    kubectl workspace create my-workspace --enter\n\n    # create a context with the current workspace, e.g. root:default:my-workspace\n    kubectl workspace create-context\n\n    # create a context with the current workspace, named context-name\n    kubectl workspace create-context context-name\n</code></pre>"},{"location":"reference/cli/workspace/#options","title":"Options","text":"<pre><code>      --add_dir_header                   If true, adds the file directory to the header of the log messages\n      --alsologtostderr                  log to standard error as well as files (no effect when -logtostderr=true)\n      --as-uid string                    UID to impersonate for the operation\n      --certificate-authority string     Path to a cert file for the certificate authority\n      --context string                   The name of the kubeconfig context to use\n      --disable-compression              If true, opt-out of response compression for all requests to the server\n  -h, --help                             help for workspace\n      --insecure-skip-tls-verify         If true, the server's certificate will not be checked for validity. This will make your HTTPS connections insecure\n      --kubeconfig string                path to the kubeconfig file\n      --log_backtrace_at traceLocation   when logging hits line file:N, emit a stack trace (default :0)\n      --log_dir string                   If non-empty, write log files in this directory (no effect when -logtostderr=true)\n      --log_file string                  If non-empty, use this log file (no effect when -logtostderr=true)\n      --log_file_max_size uint           Defines the maximum size a log file can grow to (no effect when -logtostderr=true). Unit is megabytes. If the value is 0, the maximum file size is unlimited. (default 1800)\n      --logtostderr                      log to standard error instead of files (default true)\n  -n, --namespace string                 If present, the namespace scope for this CLI request\n      --one_output                       If true, only write logs to their native severity level (vs also writing to each lower severity level; no effect when -logtostderr=true)\n      --password string                  Password for basic authentication to the API server\n      --proxy-url string                 If provided, this URL will be used to connect via proxy\n      --server string                    The address and port of the Kubernetes API server\n      --short                            Print only the name of the workspace, e.g. for integration into the shell prompt\n      --skip_headers                     If true, avoid header prefixes in the log messages\n      --skip_log_headers                 If true, avoid headers when opening log files (no effect when -logtostderr=true)\n      --stderrthreshold severity         logs at or above this threshold go to stderr when writing to files and stderr (no effect when -logtostderr=true or -alsologtostderr=true) (default 2)\n      --tls-server-name string           If provided, this name will be used to validate server certificate. If this is not provided, hostname used to contact the server is used.\n      --token string                     Bearer token for authentication to the API server\n      --user string                      The name of the kubeconfig user to use\n      --username string                  Username for basic authentication to the API server\n  -v, --v Level                          number for the log level verbosity\n      --vmodule moduleSpec               comma-separated list of pattern=N settings for file-filtered logging\n</code></pre>"},{"location":"reference/cli/workspace/#see-also","title":"SEE ALSO","text":"<ul> <li>workspace create-context  - Create a kubeconfig context for the current workspace</li> <li>workspace current    - Print the current workspace. Same as 'kubectl ws .'.</li> <li>workspace tree  - Print the current workspace tree.</li> <li>workspace use    - Uses the given workspace as the current workspace. Using - means previous workspace, .. means parent workspace, . mean current, ~ means home workspace</li> </ul>"},{"location":"reference/cli/workspace_create-context/","title":"workspace create-context","text":"<pre><code>workspace create-context [&lt;context-name&gt;] [--overwrite] [flags]\n</code></pre>"},{"location":"reference/cli/workspace_create-context/#examples","title":"Examples","text":"<pre><code>kcp workspace create-context\n</code></pre>"},{"location":"reference/cli/workspace_create-context/#options","title":"Options","text":"<pre><code>      --as-uid string                  UID to impersonate for the operation\n      --certificate-authority string   Path to a cert file for the certificate authority\n      --context string                 The name of the kubeconfig context to use\n      --disable-compression            If true, opt-out of response compression for all requests to the server\n  -h, --help                           help for create-context\n      --insecure-skip-tls-verify       If true, the server's certificate will not be checked for validity. This will make your HTTPS connections insecure\n      --kubeconfig string              path to the kubeconfig file\n  -n, --namespace string               If present, the namespace scope for this CLI request\n      --overwrite                      Overwrite the context if it already exists\n      --password string                Password for basic authentication to the API server\n      --proxy-url string               If provided, this URL will be used to connect via proxy\n      --server string                  The address and port of the Kubernetes API server\n      --tls-server-name string         If provided, this name will be used to validate server certificate. If this is not provided, hostname used to contact the server is used.\n      --token string                   Bearer token for authentication to the API server\n      --user string                    The name of the kubeconfig user to use\n      --username string                Username for basic authentication to the API server\n</code></pre>"},{"location":"reference/cli/workspace_create-context/#options-inherited-from-parent-commands","title":"Options inherited from parent commands","text":"<pre><code>      --add_dir_header                   If true, adds the file directory to the header of the log messages\n      --alsologtostderr                  log to standard error as well as files (no effect when -logtostderr=true)\n      --log_backtrace_at traceLocation   when logging hits line file:N, emit a stack trace (default :0)\n      --log_dir string                   If non-empty, write log files in this directory (no effect when -logtostderr=true)\n      --log_file string                  If non-empty, use this log file (no effect when -logtostderr=true)\n      --log_file_max_size uint           Defines the maximum size a log file can grow to (no effect when -logtostderr=true). Unit is megabytes. If the value is 0, the maximum file size is unlimited. (default 1800)\n      --logtostderr                      log to standard error instead of files (default true)\n      --one_output                       If true, only write logs to their native severity level (vs also writing to each lower severity level; no effect when -logtostderr=true)\n      --skip_headers                     If true, avoid header prefixes in the log messages\n      --skip_log_headers                 If true, avoid headers when opening log files (no effect when -logtostderr=true)\n      --stderrthreshold severity         logs at or above this threshold go to stderr when writing to files and stderr (no effect when -logtostderr=true or -alsologtostderr=true) (default 2)\n  -v, --v Level                          number for the log level verbosity\n      --vmodule moduleSpec               comma-separated list of pattern=N settings for file-filtered logging\n</code></pre>"},{"location":"reference/cli/workspace_create-context/#see-also","title":"SEE ALSO","text":"<ul> <li>workspace    - Manages KCP workspaces</li> </ul>"},{"location":"reference/cli/workspace_current/","title":"workspace current","text":"<pre><code>workspace current [--short] [flags]\n</code></pre>"},{"location":"reference/cli/workspace_current/#examples","title":"Examples","text":"<pre><code>kcp workspace current\n</code></pre>"},{"location":"reference/cli/workspace_current/#options","title":"Options","text":"<pre><code>      --as-uid string                  UID to impersonate for the operation\n      --certificate-authority string   Path to a cert file for the certificate authority\n      --context string                 The name of the kubeconfig context to use\n      --disable-compression            If true, opt-out of response compression for all requests to the server\n  -h, --help                           help for current\n      --insecure-skip-tls-verify       If true, the server's certificate will not be checked for validity. This will make your HTTPS connections insecure\n      --kubeconfig string              path to the kubeconfig file\n  -n, --namespace string               If present, the namespace scope for this CLI request\n      --password string                Password for basic authentication to the API server\n      --proxy-url string               If provided, this URL will be used to connect via proxy\n      --server string                  The address and port of the Kubernetes API server\n      --short                          Print only the name of the workspace, e.g. for integration into the shell prompt\n      --tls-server-name string         If provided, this name will be used to validate server certificate. If this is not provided, hostname used to contact the server is used.\n      --token string                   Bearer token for authentication to the API server\n      --user string                    The name of the kubeconfig user to use\n      --username string                Username for basic authentication to the API server\n</code></pre>"},{"location":"reference/cli/workspace_current/#options-inherited-from-parent-commands","title":"Options inherited from parent commands","text":"<pre><code>      --add_dir_header                   If true, adds the file directory to the header of the log messages\n      --alsologtostderr                  log to standard error as well as files (no effect when -logtostderr=true)\n      --log_backtrace_at traceLocation   when logging hits line file:N, emit a stack trace (default :0)\n      --log_dir string                   If non-empty, write log files in this directory (no effect when -logtostderr=true)\n      --log_file string                  If non-empty, use this log file (no effect when -logtostderr=true)\n      --log_file_max_size uint           Defines the maximum size a log file can grow to (no effect when -logtostderr=true). Unit is megabytes. If the value is 0, the maximum file size is unlimited. (default 1800)\n      --logtostderr                      log to standard error instead of files (default true)\n      --one_output                       If true, only write logs to their native severity level (vs also writing to each lower severity level; no effect when -logtostderr=true)\n      --skip_headers                     If true, avoid header prefixes in the log messages\n      --skip_log_headers                 If true, avoid headers when opening log files (no effect when -logtostderr=true)\n      --stderrthreshold severity         logs at or above this threshold go to stderr when writing to files and stderr (no effect when -logtostderr=true or -alsologtostderr=true) (default 2)\n  -v, --v Level                          number for the log level verbosity\n      --vmodule moduleSpec               comma-separated list of pattern=N settings for file-filtered logging\n</code></pre>"},{"location":"reference/cli/workspace_current/#see-also","title":"SEE ALSO","text":"<ul> <li>workspace    - Manages KCP workspaces</li> </ul>"},{"location":"reference/cli/workspace_tree/","title":"workspace tree","text":"<pre><code>workspace tree [flags]\n</code></pre>"},{"location":"reference/cli/workspace_tree/#examples","title":"Examples","text":"<pre><code>kcp workspace tree\n</code></pre>"},{"location":"reference/cli/workspace_tree/#options","title":"Options","text":"<pre><code>      --as-uid string                  UID to impersonate for the operation\n      --certificate-authority string   Path to a cert file for the certificate authority\n      --context string                 The name of the kubeconfig context to use\n      --disable-compression            If true, opt-out of response compression for all requests to the server\n  -f, --full                           Show full workspace names\n  -h, --help                           help for tree\n      --insecure-skip-tls-verify       If true, the server's certificate will not be checked for validity. This will make your HTTPS connections insecure\n      --kubeconfig string              path to the kubeconfig file\n  -n, --namespace string               If present, the namespace scope for this CLI request\n      --password string                Password for basic authentication to the API server\n      --proxy-url string               If provided, this URL will be used to connect via proxy\n      --server string                  The address and port of the Kubernetes API server\n      --tls-server-name string         If provided, this name will be used to validate server certificate. If this is not provided, hostname used to contact the server is used.\n      --token string                   Bearer token for authentication to the API server\n      --user string                    The name of the kubeconfig user to use\n      --username string                Username for basic authentication to the API server\n</code></pre>"},{"location":"reference/cli/workspace_tree/#options-inherited-from-parent-commands","title":"Options inherited from parent commands","text":"<pre><code>      --add_dir_header                   If true, adds the file directory to the header of the log messages\n      --alsologtostderr                  log to standard error as well as files (no effect when -logtostderr=true)\n      --log_backtrace_at traceLocation   when logging hits line file:N, emit a stack trace (default :0)\n      --log_dir string                   If non-empty, write log files in this directory (no effect when -logtostderr=true)\n      --log_file string                  If non-empty, use this log file (no effect when -logtostderr=true)\n      --log_file_max_size uint           Defines the maximum size a log file can grow to (no effect when -logtostderr=true). Unit is megabytes. If the value is 0, the maximum file size is unlimited. (default 1800)\n      --logtostderr                      log to standard error instead of files (default true)\n      --one_output                       If true, only write logs to their native severity level (vs also writing to each lower severity level; no effect when -logtostderr=true)\n      --skip_headers                     If true, avoid header prefixes in the log messages\n      --skip_log_headers                 If true, avoid headers when opening log files (no effect when -logtostderr=true)\n      --stderrthreshold severity         logs at or above this threshold go to stderr when writing to files and stderr (no effect when -logtostderr=true or -alsologtostderr=true) (default 2)\n  -v, --v Level                          number for the log level verbosity\n      --vmodule moduleSpec               comma-separated list of pattern=N settings for file-filtered logging\n</code></pre>"},{"location":"reference/cli/workspace_tree/#see-also","title":"SEE ALSO","text":"<ul> <li>workspace    - Manages KCP workspaces</li> </ul>"},{"location":"reference/cli/workspace_use/","title":"workspace use","text":"<pre><code>workspace use &lt;workspace&gt;|..|.|-|~|&lt;:root:absolute:workspace&gt;|&lt;relative:workspace&gt; [flags]\n</code></pre>"},{"location":"reference/cli/workspace_use/#options","title":"Options","text":"<pre><code>      --as-uid string                  UID to impersonate for the operation\n      --certificate-authority string   Path to a cert file for the certificate authority\n      --context string                 The name of the kubeconfig context to use\n      --disable-compression            If true, opt-out of response compression for all requests to the server\n  -h, --help                           help for use\n      --insecure-skip-tls-verify       If true, the server's certificate will not be checked for validity. This will make your HTTPS connections insecure\n      --kubeconfig string              path to the kubeconfig file\n  -n, --namespace string               If present, the namespace scope for this CLI request\n      --password string                Password for basic authentication to the API server\n      --proxy-url string               If provided, this URL will be used to connect via proxy\n      --server string                  The address and port of the Kubernetes API server\n      --short                          Print only the name of the workspace, e.g. for integration into the shell prompt\n      --tls-server-name string         If provided, this name will be used to validate server certificate. If this is not provided, hostname used to contact the server is used.\n      --token string                   Bearer token for authentication to the API server\n      --user string                    The name of the kubeconfig user to use\n      --username string                Username for basic authentication to the API server\n</code></pre>"},{"location":"reference/cli/workspace_use/#options-inherited-from-parent-commands","title":"Options inherited from parent commands","text":"<pre><code>      --add_dir_header                   If true, adds the file directory to the header of the log messages\n      --alsologtostderr                  log to standard error as well as files (no effect when -logtostderr=true)\n      --log_backtrace_at traceLocation   when logging hits line file:N, emit a stack trace (default :0)\n      --log_dir string                   If non-empty, write log files in this directory (no effect when -logtostderr=true)\n      --log_file string                  If non-empty, use this log file (no effect when -logtostderr=true)\n      --log_file_max_size uint           Defines the maximum size a log file can grow to (no effect when -logtostderr=true). Unit is megabytes. If the value is 0, the maximum file size is unlimited. (default 1800)\n      --logtostderr                      log to standard error instead of files (default true)\n      --one_output                       If true, only write logs to their native severity level (vs also writing to each lower severity level; no effect when -logtostderr=true)\n      --skip_headers                     If true, avoid header prefixes in the log messages\n      --skip_log_headers                 If true, avoid headers when opening log files (no effect when -logtostderr=true)\n      --stderrthreshold severity         logs at or above this threshold go to stderr when writing to files and stderr (no effect when -logtostderr=true or -alsologtostderr=true) (default 2)\n  -v, --v Level                          number for the log level verbosity\n      --vmodule moduleSpec               comma-separated list of pattern=N settings for file-filtered logging\n</code></pre>"},{"location":"reference/cli/workspace_use/#see-also","title":"SEE ALSO","text":"<ul> <li>workspace    - Manages KCP workspaces</li> </ul>"},{"location":"reference/crd/apis.kcp.io/apibindings/","title":"APIBinding","text":""},{"location":"reference/crd/apis.kcp.io/apibindings/#apibinding-crd-schema-reference-group-apiskcpio","title":"APIBinding CRD schema reference (group apis.kcp.io)","text":"APIBinding enables a set of resources and their behaviour through an external service provider in this workspace.  The service provider uses an APIExport to expose the API.  Full name: apibindings.apis.kcp.io Group: apis.kcp.io Singular name: apibinding Plural name: apibindings Scope: Cluster Versions: v1alpha1"},{"location":"reference/crd/apis.kcp.io/apibindings/#v1alpha1","title":"Version v1alpha1","text":""},{"location":"reference/crd/apis.kcp.io/apibindings/#property-details-v1alpha1","title":"Properties","text":"string <p>APIVersion defines the versioned schema of this representation of an object. Servers should convert recognized schemas to the latest internal value, and may reject unrecognized values. More info: https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources</p> string <p>Kind is a string value representing the REST resource this object represents. Servers may infer this from the endpoint the client submits requests to. Cannot be updated. In CamelCase. More info: https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds</p> object object Required <p>Spec holds the desired state.</p> array <p>permissionClaims records decisions about permission claims requested by the API service provider. Individual claims can be accepted or rejected. If accepted, the API service provider gets the requested access to the specified resources in this workspace. Access is granted per GroupResource, identity, and other properties.</p> object <p>AcceptablePermissionClaim is a PermissionClaim that records if the user accepts or rejects it.</p> boolean <p>all claims all resources for the given group/resource. This is mutually exclusive with resourceSelector.</p> string <p>group is the name of an API group. For core groups this is the empty string \u2018\u201c\u201d\u2019.</p> string <p>This is the identity for a given APIExport that the APIResourceSchema belongs to. The hash can be found on APIExport and APIResourceSchema\u2019s status. It will be empty for core types. Note that one must look this up for a particular KCP instance.</p> string Required <p>resource is the name of the resource. Note: it is worth noting that you can not ask for permissions for resource provided by a CRD not provided by an api export.</p> array <p>resourceSelector is a list of claimed resource selectors.</p> object string <p>name of an object within a claimed group/resource. It matches the metadata.name field of the underlying object. If namespace is unset, all objects matching that name will be claimed.</p> string <p>namespace containing the named object. Matches metadata.namespace field. If \u201cname\u201d is unset, all objects from the namespace are being claimed.</p> string Required object Required <p>reference uniquely identifies an API to bind to.</p> object <p>export is a reference to an APIExport by cluster name and export name. The creator of the APIBinding needs to have access to the APIExport with the verb <code>bind</code> in order to bind to it.</p> string Required <p>name is the name of the APIExport that describes the API.</p> string <p>path is a logical cluster path where the APIExport is defined. If the path is unset, the logical cluster of the APIBinding is used.</p> object <p>Status communicates the observed state.</p> string <p>APIExportClusterName records the name (not path) of the logical cluster that contains the APIExport.</p> array <p>appliedPermissionClaims is a list of the permission claims the system has seen and applied, according to the requests of the API service provider in the APIExport and the acceptance state in spec.permissionClaims.</p> object <p>PermissionClaim identifies an object by GR and identity hash. Its purpose is to determine the added permissions that a service provider may request and that a consumer may accept and allow the service provider access to.</p> boolean <p>all claims all resources for the given group/resource. This is mutually exclusive with resourceSelector.</p> string <p>group is the name of an API group. For core groups this is the empty string \u2018\u201c\u201d\u2019.</p> string <p>This is the identity for a given APIExport that the APIResourceSchema belongs to. The hash can be found on APIExport and APIResourceSchema\u2019s status. It will be empty for core types. Note that one must look this up for a particular KCP instance.</p> string Required <p>resource is the name of the resource. Note: it is worth noting that you can not ask for permissions for resource provided by a CRD not provided by an api export.</p> array <p>resourceSelector is a list of claimed resource selectors.</p> object string <p>name of an object within a claimed group/resource. It matches the metadata.name field of the underlying object. If namespace is unset, all objects matching that name will be claimed.</p> string <p>namespace containing the named object. Matches metadata.namespace field. If \u201cname\u201d is unset, all objects from the namespace are being claimed.</p> array <p>boundResources records the state of bound APIs.</p> object <p>BoundAPIResource describes a bound GroupVersionResource through an APIResourceSchema of an APIExport..</p> string Required <p>group is the group of the bound API. Empty string for the core API group.</p> string Required <p>resource is the resource of the bound API.</p> <p>kubebuilder:validation:MinLength=1</p> object Required <p>Schema references the APIResourceSchema that is bound to this API.</p> string Required <p>UID is the UID of the APIResourceSchema that is bound to this API.</p> string Required <p>identityHash is the hash of the API identity that this schema is bound to. The API identity determines the etcd prefix used to persist the object. Different identity means that the objects are effectively served and stored under a distinct resource. A CRD of the same GroupVersionResource uses a different identity and hence a separate etcd prefix.</p> string Required <p>name is the bound APIResourceSchema name.</p> array <p>storageVersions lists all versions of a resource that were ever persisted. Tracking these versions allows a migration path for stored versions in etcd. The field is mutable so a migration controller can finish a migration to another version (ensuring no old objects are left in storage), and then remove the rest of the versions from this list.</p> <p>Versions may not be removed while they exist in this list.</p> string array <p>conditions is a list of conditions that apply to the APIBinding.</p> object <p>Condition defines an observation of a object operational state.</p> string Required <p>Last time the condition transitioned from one status to another. This should be when the underlying condition changed. If that is not known, then using the time when the API field changed is acceptable.</p> string <p>A human readable message indicating details about the transition. This field may be empty.</p> string <p>The reason for the condition\u2019s last transition in CamelCase. The specific API may choose whether or not this field is considered a guaranteed API. This field may not be empty.</p> string <p>Severity provides an explicit classification of Reason code, so the users or machines can immediately understand the current situation and act accordingly. The Severity field MUST be set only when Status=False.</p> string Required <p>Status of the condition, one of True, False, Unknown.</p> string Required <p>Type of condition in CamelCase or in foo.example.com/CamelCase. Many .condition.type values are consistent across resources like Available, but because arbitrary conditions can be useful (see .node.status.conditions), the ability to deconflict is important.</p> array <p>exportPermissionClaims records the permissions that the export provider is asking for the binding to grant.</p> object <p>PermissionClaim identifies an object by GR and identity hash. Its purpose is to determine the added permissions that a service provider may request and that a consumer may accept and allow the service provider access to.</p> boolean <p>all claims all resources for the given group/resource. This is mutually exclusive with resourceSelector.</p> string <p>group is the name of an API group. For core groups this is the empty string \u2018\u201c\u201d\u2019.</p> string <p>This is the identity for a given APIExport that the APIResourceSchema belongs to. The hash can be found on APIExport and APIResourceSchema\u2019s status. It will be empty for core types. Note that one must look this up for a particular KCP instance.</p> string Required <p>resource is the name of the resource. Note: it is worth noting that you can not ask for permissions for resource provided by a CRD not provided by an api export.</p> array <p>resourceSelector is a list of claimed resource selectors.</p> object string <p>name of an object within a claimed group/resource. It matches the metadata.name field of the underlying object. If namespace is unset, all objects matching that name will be claimed.</p> string <p>namespace containing the named object. Matches metadata.namespace field. If \u201cname\u201d is unset, all objects from the namespace are being claimed.</p> string <p>phase is the current phase of the APIBinding: - \u201c\u201d: the APIBinding has just been created, waiting to be bound. - Binding: the APIBinding is being bound. - Bound: the APIBinding is bound and the referenced APIs are available in the workspace.</p>"},{"location":"reference/crd/apis.kcp.io/apibindings/#v1alpha1-.apiVersion","title":".apiVersion","text":""},{"location":"reference/crd/apis.kcp.io/apibindings/#v1alpha1-.kind","title":".kind","text":""},{"location":"reference/crd/apis.kcp.io/apibindings/#v1alpha1-.metadata","title":".metadata","text":""},{"location":"reference/crd/apis.kcp.io/apibindings/#v1alpha1-.spec","title":".spec","text":""},{"location":"reference/crd/apis.kcp.io/apibindings/#v1alpha1-.spec.permissionClaims","title":".spec.permissionClaims","text":""},{"location":"reference/crd/apis.kcp.io/apibindings/#v1alpha1-.spec.permissionClaims[*]","title":".spec.permissionClaims[*]","text":""},{"location":"reference/crd/apis.kcp.io/apibindings/#v1alpha1-.spec.permissionClaims[*].all","title":".spec.permissionClaims[*].all","text":""},{"location":"reference/crd/apis.kcp.io/apibindings/#v1alpha1-.spec.permissionClaims[*].group","title":".spec.permissionClaims[*].group","text":""},{"location":"reference/crd/apis.kcp.io/apibindings/#v1alpha1-.spec.permissionClaims[*].identityHash","title":".spec.permissionClaims[*].identityHash","text":""},{"location":"reference/crd/apis.kcp.io/apibindings/#v1alpha1-.spec.permissionClaims[*].resource","title":".spec.permissionClaims[*].resource","text":""},{"location":"reference/crd/apis.kcp.io/apibindings/#v1alpha1-.spec.permissionClaims[*].resourceSelector","title":".spec.permissionClaims[*].resourceSelector","text":""},{"location":"reference/crd/apis.kcp.io/apibindings/#v1alpha1-.spec.permissionClaims[*].resourceSelector[*]","title":".spec.permissionClaims[*].resourceSelector[*]","text":""},{"location":"reference/crd/apis.kcp.io/apibindings/#v1alpha1-.spec.permissionClaims[*].resourceSelector[*].name","title":".spec.permissionClaims[*].resourceSelector[*].name","text":""},{"location":"reference/crd/apis.kcp.io/apibindings/#v1alpha1-.spec.permissionClaims[*].resourceSelector[*].namespace","title":".spec.permissionClaims[*].resourceSelector[*].namespace","text":""},{"location":"reference/crd/apis.kcp.io/apibindings/#v1alpha1-.spec.permissionClaims[*].state","title":".spec.permissionClaims[*].state","text":""},{"location":"reference/crd/apis.kcp.io/apibindings/#v1alpha1-.spec.reference","title":".spec.reference","text":""},{"location":"reference/crd/apis.kcp.io/apibindings/#v1alpha1-.spec.reference.export","title":".spec.reference.export","text":""},{"location":"reference/crd/apis.kcp.io/apibindings/#v1alpha1-.spec.reference.export.name","title":".spec.reference.export.name","text":""},{"location":"reference/crd/apis.kcp.io/apibindings/#v1alpha1-.spec.reference.export.path","title":".spec.reference.export.path","text":""},{"location":"reference/crd/apis.kcp.io/apibindings/#v1alpha1-.status","title":".status","text":""},{"location":"reference/crd/apis.kcp.io/apibindings/#v1alpha1-.status.apiExportClusterName","title":".status.apiExportClusterName","text":""},{"location":"reference/crd/apis.kcp.io/apibindings/#v1alpha1-.status.appliedPermissionClaims","title":".status.appliedPermissionClaims","text":""},{"location":"reference/crd/apis.kcp.io/apibindings/#v1alpha1-.status.appliedPermissionClaims[*]","title":".status.appliedPermissionClaims[*]","text":""},{"location":"reference/crd/apis.kcp.io/apibindings/#v1alpha1-.status.appliedPermissionClaims[*].all","title":".status.appliedPermissionClaims[*].all","text":""},{"location":"reference/crd/apis.kcp.io/apibindings/#v1alpha1-.status.appliedPermissionClaims[*].group","title":".status.appliedPermissionClaims[*].group","text":""},{"location":"reference/crd/apis.kcp.io/apibindings/#v1alpha1-.status.appliedPermissionClaims[*].identityHash","title":".status.appliedPermissionClaims[*].identityHash","text":""},{"location":"reference/crd/apis.kcp.io/apibindings/#v1alpha1-.status.appliedPermissionClaims[*].resource","title":".status.appliedPermissionClaims[*].resource","text":""},{"location":"reference/crd/apis.kcp.io/apibindings/#v1alpha1-.status.appliedPermissionClaims[*].resourceSelector","title":".status.appliedPermissionClaims[*].resourceSelector","text":""},{"location":"reference/crd/apis.kcp.io/apibindings/#v1alpha1-.status.appliedPermissionClaims[*].resourceSelector[*]","title":".status.appliedPermissionClaims[*].resourceSelector[*]","text":""},{"location":"reference/crd/apis.kcp.io/apibindings/#v1alpha1-.status.appliedPermissionClaims[*].resourceSelector[*].name","title":".status.appliedPermissionClaims[*].resourceSelector[*].name","text":""},{"location":"reference/crd/apis.kcp.io/apibindings/#v1alpha1-.status.appliedPermissionClaims[*].resourceSelector[*].namespace","title":".status.appliedPermissionClaims[*].resourceSelector[*].namespace","text":""},{"location":"reference/crd/apis.kcp.io/apibindings/#v1alpha1-.status.boundResources","title":".status.boundResources","text":""},{"location":"reference/crd/apis.kcp.io/apibindings/#v1alpha1-.status.boundResources[*]","title":".status.boundResources[*]","text":""},{"location":"reference/crd/apis.kcp.io/apibindings/#v1alpha1-.status.boundResources[*].group","title":".status.boundResources[*].group","text":""},{"location":"reference/crd/apis.kcp.io/apibindings/#v1alpha1-.status.boundResources[*].resource","title":".status.boundResources[*].resource","text":""},{"location":"reference/crd/apis.kcp.io/apibindings/#v1alpha1-.status.boundResources[*].schema","title":".status.boundResources[*].schema","text":""},{"location":"reference/crd/apis.kcp.io/apibindings/#v1alpha1-.status.boundResources[*].schema.UID","title":".status.boundResources[*].schema.UID","text":""},{"location":"reference/crd/apis.kcp.io/apibindings/#v1alpha1-.status.boundResources[*].schema.identityHash","title":".status.boundResources[*].schema.identityHash","text":""},{"location":"reference/crd/apis.kcp.io/apibindings/#v1alpha1-.status.boundResources[*].schema.name","title":".status.boundResources[*].schema.name","text":""},{"location":"reference/crd/apis.kcp.io/apibindings/#v1alpha1-.status.boundResources[*].storageVersions","title":".status.boundResources[*].storageVersions","text":""},{"location":"reference/crd/apis.kcp.io/apibindings/#v1alpha1-.status.boundResources[*].storageVersions[*]","title":".status.boundResources[*].storageVersions[*]","text":""},{"location":"reference/crd/apis.kcp.io/apibindings/#v1alpha1-.status.conditions","title":".status.conditions","text":""},{"location":"reference/crd/apis.kcp.io/apibindings/#v1alpha1-.status.conditions[*]","title":".status.conditions[*]","text":""},{"location":"reference/crd/apis.kcp.io/apibindings/#v1alpha1-.status.conditions[*].lastTransitionTime","title":".status.conditions[*].lastTransitionTime","text":""},{"location":"reference/crd/apis.kcp.io/apibindings/#v1alpha1-.status.conditions[*].message","title":".status.conditions[*].message","text":""},{"location":"reference/crd/apis.kcp.io/apibindings/#v1alpha1-.status.conditions[*].reason","title":".status.conditions[*].reason","text":""},{"location":"reference/crd/apis.kcp.io/apibindings/#v1alpha1-.status.conditions[*].severity","title":".status.conditions[*].severity","text":""},{"location":"reference/crd/apis.kcp.io/apibindings/#v1alpha1-.status.conditions[*].status","title":".status.conditions[*].status","text":""},{"location":"reference/crd/apis.kcp.io/apibindings/#v1alpha1-.status.conditions[*].type","title":".status.conditions[*].type","text":""},{"location":"reference/crd/apis.kcp.io/apibindings/#v1alpha1-.status.exportPermissionClaims","title":".status.exportPermissionClaims","text":""},{"location":"reference/crd/apis.kcp.io/apibindings/#v1alpha1-.status.exportPermissionClaims[*]","title":".status.exportPermissionClaims[*]","text":""},{"location":"reference/crd/apis.kcp.io/apibindings/#v1alpha1-.status.exportPermissionClaims[*].all","title":".status.exportPermissionClaims[*].all","text":""},{"location":"reference/crd/apis.kcp.io/apibindings/#v1alpha1-.status.exportPermissionClaims[*].group","title":".status.exportPermissionClaims[*].group","text":""},{"location":"reference/crd/apis.kcp.io/apibindings/#v1alpha1-.status.exportPermissionClaims[*].identityHash","title":".status.exportPermissionClaims[*].identityHash","text":""},{"location":"reference/crd/apis.kcp.io/apibindings/#v1alpha1-.status.exportPermissionClaims[*].resource","title":".status.exportPermissionClaims[*].resource","text":""},{"location":"reference/crd/apis.kcp.io/apibindings/#v1alpha1-.status.exportPermissionClaims[*].resourceSelector","title":".status.exportPermissionClaims[*].resourceSelector","text":""},{"location":"reference/crd/apis.kcp.io/apibindings/#v1alpha1-.status.exportPermissionClaims[*].resourceSelector[*]","title":".status.exportPermissionClaims[*].resourceSelector[*]","text":""},{"location":"reference/crd/apis.kcp.io/apibindings/#v1alpha1-.status.exportPermissionClaims[*].resourceSelector[*].name","title":".status.exportPermissionClaims[*].resourceSelector[*].name","text":""},{"location":"reference/crd/apis.kcp.io/apibindings/#v1alpha1-.status.exportPermissionClaims[*].resourceSelector[*].namespace","title":".status.exportPermissionClaims[*].resourceSelector[*].namespace","text":""},{"location":"reference/crd/apis.kcp.io/apibindings/#v1alpha1-.status.phase","title":".status.phase","text":""},{"location":"reference/crd/apis.kcp.io/apiconversions/","title":"APIConversion","text":""},{"location":"reference/crd/apis.kcp.io/apiconversions/#apiconversion-crd-schema-reference-group-apiskcpio","title":"APIConversion CRD schema reference (group apis.kcp.io)","text":"APIConversion contains rules to convert between different API versions in an APIResourceSchema. The name must match the name of the APIResourceSchema for the conversions to take effect.  Full name: apiconversions.apis.kcp.io Group: apis.kcp.io Singular name: apiconversion Plural name: apiconversions Scope: Cluster Versions: v1alpha1"},{"location":"reference/crd/apis.kcp.io/apiconversions/#v1alpha1","title":"Version v1alpha1","text":""},{"location":"reference/crd/apis.kcp.io/apiconversions/#property-details-v1alpha1","title":"Properties","text":"string <p>APIVersion defines the versioned schema of this representation of an object. Servers should convert recognized schemas to the latest internal value, and may reject unrecognized values. More info: https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources</p> string <p>Kind is a string value representing the REST resource this object represents. Servers may infer this from the endpoint the client submits requests to. Cannot be updated. In CamelCase. More info: https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds</p> object Required object Required <p>Spec holds the desired state.</p> array Required <p>conversions specify rules to convert between different API versions in an APIResourceSchema.</p> object <p>APIVersionConversion contains rules to convert between two specific API versions in an APIResourceSchema. Additionally, to avoid data loss when round-tripping from a version that contains a new field to one that doesn\u2019t and back again, you can specify a list of fields to preserve (these are stored in annotations).</p> string Required <p>from is the source version.</p> array <p>preserve contains a list of JSONPath expressions to fields to preserve in the originating version of the object, relative to its root, such as \u2018.spec.name.first\u2019.</p> string array Required <p>rules contains field-specific conversion expressions.</p> object <p>APIConversionRule specifies how to convert a single field.</p> string Required <p>destination is a JSONPath expression to the field in the target version of the object, relative to its root, such as \u2018.spec.name.first\u2019.</p> string Required <p>field is a JSONPath expression to the field in the originating version of the object, relative to its root, such as \u2018.spec.name.first\u2019.</p> string <p>transformation is an optional CEL expression used to execute user-specified rules to transform the originating field \u2013 identified by \u2018self\u2019 \u2013 to the destination field.</p> string Required <p>to is the target version.</p>"},{"location":"reference/crd/apis.kcp.io/apiconversions/#v1alpha1-.apiVersion","title":".apiVersion","text":""},{"location":"reference/crd/apis.kcp.io/apiconversions/#v1alpha1-.kind","title":".kind","text":""},{"location":"reference/crd/apis.kcp.io/apiconversions/#v1alpha1-.metadata","title":".metadata","text":""},{"location":"reference/crd/apis.kcp.io/apiconversions/#v1alpha1-.spec","title":".spec","text":""},{"location":"reference/crd/apis.kcp.io/apiconversions/#v1alpha1-.spec.conversions","title":".spec.conversions","text":""},{"location":"reference/crd/apis.kcp.io/apiconversions/#v1alpha1-.spec.conversions[*]","title":".spec.conversions[*]","text":""},{"location":"reference/crd/apis.kcp.io/apiconversions/#v1alpha1-.spec.conversions[*].from","title":".spec.conversions[*].from","text":""},{"location":"reference/crd/apis.kcp.io/apiconversions/#v1alpha1-.spec.conversions[*].preserve","title":".spec.conversions[*].preserve","text":""},{"location":"reference/crd/apis.kcp.io/apiconversions/#v1alpha1-.spec.conversions[*].preserve[*]","title":".spec.conversions[*].preserve[*]","text":""},{"location":"reference/crd/apis.kcp.io/apiconversions/#v1alpha1-.spec.conversions[*].rules","title":".spec.conversions[*].rules","text":""},{"location":"reference/crd/apis.kcp.io/apiconversions/#v1alpha1-.spec.conversions[*].rules[*]","title":".spec.conversions[*].rules[*]","text":""},{"location":"reference/crd/apis.kcp.io/apiconversions/#v1alpha1-.spec.conversions[*].rules[*].destination","title":".spec.conversions[*].rules[*].destination","text":""},{"location":"reference/crd/apis.kcp.io/apiconversions/#v1alpha1-.spec.conversions[*].rules[*].field","title":".spec.conversions[*].rules[*].field","text":""},{"location":"reference/crd/apis.kcp.io/apiconversions/#v1alpha1-.spec.conversions[*].rules[*].transformation","title":".spec.conversions[*].rules[*].transformation","text":""},{"location":"reference/crd/apis.kcp.io/apiconversions/#v1alpha1-.spec.conversions[*].to","title":".spec.conversions[*].to","text":""},{"location":"reference/crd/apis.kcp.io/apiexportendpointslices/","title":"APIExportEndpointSlice","text":""},{"location":"reference/crd/apis.kcp.io/apiexportendpointslices/#apiexportendpointslice-crd-schema-reference-group-apiskcpio","title":"APIExportEndpointSlice CRD schema reference (group apis.kcp.io)","text":"APIExportEndpointSlice is a sink for the endpoints of an APIExport. These endpoints can be filtered by a Partition. They get consumed by the managers to start controllers and informers for the respective APIExport services.  Full name: apiexportendpointslices.apis.kcp.io Group: apis.kcp.io Singular name: apiexportendpointslice Plural name: apiexportendpointslices Scope: Cluster Versions: v1alpha1"},{"location":"reference/crd/apis.kcp.io/apiexportendpointslices/#v1alpha1","title":"Version v1alpha1","text":""},{"location":"reference/crd/apis.kcp.io/apiexportendpointslices/#property-details-v1alpha1","title":"Properties","text":"string <p>APIVersion defines the versioned schema of this representation of an object. Servers should convert recognized schemas to the latest internal value, and may reject unrecognized values. More info: https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources</p> string <p>Kind is a string value representing the REST resource this object represents. Servers may infer this from the endpoint the client submits requests to. Cannot be updated. In CamelCase. More info: https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds</p> object object <p>spec holds the desired state: - the targeted APIExport - an optional partition for filtering</p> object Required <p>export points to the API export.</p> string Required <p>name is the name of the APIExport that describes the API.</p> string <p>path is a logical cluster path where the APIExport is defined. If the path is unset, the logical cluster of the APIBinding is used.</p> string <p>partition (optional) points to a partition that is used for filtering the endpoints of the APIExport part of the slice.</p> object <p>status communicates the observed state: the filtered list of endpoints for the APIExport service.</p> array <p>conditions is a list of conditions that apply to the APIExportEndpointSlice.</p> object <p>Condition defines an observation of a object operational state.</p> string Required <p>Last time the condition transitioned from one status to another. This should be when the underlying condition changed. If that is not known, then using the time when the API field changed is acceptable.</p> string <p>A human readable message indicating details about the transition. This field may be empty.</p> string <p>The reason for the condition\u2019s last transition in CamelCase. The specific API may choose whether or not this field is considered a guaranteed API. This field may not be empty.</p> string <p>Severity provides an explicit classification of Reason code, so the users or machines can immediately understand the current situation and act accordingly. The Severity field MUST be set only when Status=False.</p> string Required <p>Status of the condition, one of True, False, Unknown.</p> string Required <p>Type of condition in CamelCase or in foo.example.com/CamelCase. Many .condition.type values are consistent across resources like Available, but because arbitrary conditions can be useful (see .node.status.conditions), the ability to deconflict is important.</p> array <p>endpoints contains all the URLs of the APIExport service.</p> object <p>APIExportEndpoint contains the endpoint information of an APIExport service for a specific shard.</p> string Required <p>url is an APIExport virtual workspace URL.</p> string <p>shardSelector is the selector used to filter the shards. It is used to filter the shards when determining partition scope when deriving the endpoints. This is set by owning shard, and is used by follower shards to determine if its inscope or not.</p>"},{"location":"reference/crd/apis.kcp.io/apiexportendpointslices/#v1alpha1-.apiVersion","title":".apiVersion","text":""},{"location":"reference/crd/apis.kcp.io/apiexportendpointslices/#v1alpha1-.kind","title":".kind","text":""},{"location":"reference/crd/apis.kcp.io/apiexportendpointslices/#v1alpha1-.metadata","title":".metadata","text":""},{"location":"reference/crd/apis.kcp.io/apiexportendpointslices/#v1alpha1-.spec","title":".spec","text":""},{"location":"reference/crd/apis.kcp.io/apiexportendpointslices/#v1alpha1-.spec.export","title":".spec.export","text":""},{"location":"reference/crd/apis.kcp.io/apiexportendpointslices/#v1alpha1-.spec.export.name","title":".spec.export.name","text":""},{"location":"reference/crd/apis.kcp.io/apiexportendpointslices/#v1alpha1-.spec.export.path","title":".spec.export.path","text":""},{"location":"reference/crd/apis.kcp.io/apiexportendpointslices/#v1alpha1-.spec.partition","title":".spec.partition","text":""},{"location":"reference/crd/apis.kcp.io/apiexportendpointslices/#v1alpha1-.status","title":".status","text":""},{"location":"reference/crd/apis.kcp.io/apiexportendpointslices/#v1alpha1-.status.conditions","title":".status.conditions","text":""},{"location":"reference/crd/apis.kcp.io/apiexportendpointslices/#v1alpha1-.status.conditions[*]","title":".status.conditions[*]","text":""},{"location":"reference/crd/apis.kcp.io/apiexportendpointslices/#v1alpha1-.status.conditions[*].lastTransitionTime","title":".status.conditions[*].lastTransitionTime","text":""},{"location":"reference/crd/apis.kcp.io/apiexportendpointslices/#v1alpha1-.status.conditions[*].message","title":".status.conditions[*].message","text":""},{"location":"reference/crd/apis.kcp.io/apiexportendpointslices/#v1alpha1-.status.conditions[*].reason","title":".status.conditions[*].reason","text":""},{"location":"reference/crd/apis.kcp.io/apiexportendpointslices/#v1alpha1-.status.conditions[*].severity","title":".status.conditions[*].severity","text":""},{"location":"reference/crd/apis.kcp.io/apiexportendpointslices/#v1alpha1-.status.conditions[*].status","title":".status.conditions[*].status","text":""},{"location":"reference/crd/apis.kcp.io/apiexportendpointslices/#v1alpha1-.status.conditions[*].type","title":".status.conditions[*].type","text":""},{"location":"reference/crd/apis.kcp.io/apiexportendpointslices/#v1alpha1-.status.endpoints","title":".status.endpoints","text":""},{"location":"reference/crd/apis.kcp.io/apiexportendpointslices/#v1alpha1-.status.endpoints[*]","title":".status.endpoints[*]","text":""},{"location":"reference/crd/apis.kcp.io/apiexportendpointslices/#v1alpha1-.status.endpoints[*].url","title":".status.endpoints[*].url","text":""},{"location":"reference/crd/apis.kcp.io/apiexportendpointslices/#v1alpha1-.status.shardSelector","title":".status.shardSelector","text":""},{"location":"reference/crd/apis.kcp.io/apiexports/","title":"APIExport","text":""},{"location":"reference/crd/apis.kcp.io/apiexports/#apiexport-crd-schema-reference-group-apiskcpio","title":"APIExport CRD schema reference (group apis.kcp.io)","text":"APIExport registers an API and implementation to allow consumption by others through APIBindings.  Full name: apiexports.apis.kcp.io Group: apis.kcp.io Singular name: apiexport Plural name: apiexports Scope: Cluster Versions: v1alpha1"},{"location":"reference/crd/apis.kcp.io/apiexports/#v1alpha1","title":"Version v1alpha1","text":""},{"location":"reference/crd/apis.kcp.io/apiexports/#property-details-v1alpha1","title":"Properties","text":"string <p>APIVersion defines the versioned schema of this representation of an object. Servers should convert recognized schemas to the latest internal value, and may reject unrecognized values. More info: https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources</p> string <p>Kind is a string value representing the REST resource this object represents. Servers may infer this from the endpoint the client submits requests to. Cannot be updated. In CamelCase. More info: https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds</p> object object <p>Spec holds the desired state.</p> object <p>identity points to a secret that contains the API identity in the \u2018key\u2019 file. The API identity determines an unique etcd prefix for objects stored via this APIExport.</p> <p>Different APIExport in a workspace can share a common identity, or have different ones. The identity (the secret) can also be transferred to another workspace when the APIExport is moved.</p> <p>The identity is a secret of the API provider. The APIBindings referencing this APIExport will store a derived, non-sensitive value of this identity.</p> <p>The identity of an APIExport cannot be changed. A derived, non-sensitive value of the identity key is stored in the APIExport status and this value is immutable.</p> <p>The identity is defaulted. A secret with the name of the APIExport is automatically created.</p> object <p>secretRef is a reference to a secret that contains the API identity in the \u2018key\u2019 file.</p> string <p>name is unique within a namespace to reference a secret resource.</p> string <p>namespace defines the space within which the secret name must be unique.</p> array <p>latestResourceSchemas records the latest APIResourceSchemas that are exposed with this APIExport.</p> <p>The schemas can be changed in the life-cycle of the APIExport. These changes have no effect on existing APIBindings, but only on newly bound ones.</p> <p>For updating existing APIBindings, use an APIDeployment keeping bound workspaces up-to-date.</p> string object <p>maximalPermissionPolicy will allow for a service provider to set an upper bound on what is allowed for a consumer of this API. If the policy is not set, no upper bound is applied, i.e the consuming users can do whatever the user workspace allows the user to do.</p> <p>The policy consists of RBAC (Cluster)Roles and (Cluster)Bindings. A request of a user in a workspace that binds to this APIExport via an APIBinding is additionally checked against these rules, with the user name and the groups prefixed with <code>apis.kcp.io:binding:</code>.</p> <p>For example: assume a user <code>adam</code> with groups <code>system:authenticated</code> and <code>a-team</code> binds to this APIExport in another workspace root:org:ws. Then a request in that workspace against a resource of this APIExport is authorized as every other request in that workspace, but in addition the RBAC policy here in the APIExport workspace has to grant access to the user <code>apis.kcp.io:binding:adam</code> with the groups <code>apis.kcp.io:binding:system:authenticated</code> and <code>apis.kcp.io:binding:a-team</code>.</p> object <p>local is the policy that is defined in same workspace as the API Export.</p> array <p>permissionClaims make resources available in APIExport\u2019s virtual workspace that are not part of the actual APIExport resources.</p> <p>PermissionClaims are optional and should be the least access necessary to complete the functions that the service provider needs. Access is asked for on a GroupResource + identity basis.</p> <p>PermissionClaims must be accepted by the user\u2019s explicit acknowledgement. Hence, when claims change, the respecting objects are not visible immediately.</p> <p>PermissionClaims overlapping with the APIExport resources are ignored.</p> object <p>PermissionClaim identifies an object by GR and identity hash. Its purpose is to determine the added permissions that a service provider may request and that a consumer may accept and allow the service provider access to.</p> boolean <p>all claims all resources for the given group/resource. This is mutually exclusive with resourceSelector.</p> string <p>group is the name of an API group. For core groups this is the empty string \u2018\u201c\u201d\u2019.</p> string <p>This is the identity for a given APIExport that the APIResourceSchema belongs to. The hash can be found on APIExport and APIResourceSchema\u2019s status. It will be empty for core types. Note that one must look this up for a particular KCP instance.</p> string Required <p>resource is the name of the resource. Note: it is worth noting that you can not ask for permissions for resource provided by a CRD not provided by an api export.</p> array <p>resourceSelector is a list of claimed resource selectors.</p> object string <p>name of an object within a claimed group/resource. It matches the metadata.name field of the underlying object. If namespace is unset, all objects matching that name will be claimed.</p> string <p>namespace containing the named object. Matches metadata.namespace field. If \u201cname\u201d is unset, all objects from the namespace are being claimed.</p> object <p>Status communicates the observed state.</p> array <p>conditions is a list of conditions that apply to the APIExport.</p> object <p>Condition defines an observation of a object operational state.</p> string Required <p>Last time the condition transitioned from one status to another. This should be when the underlying condition changed. If that is not known, then using the time when the API field changed is acceptable.</p> string <p>A human readable message indicating details about the transition. This field may be empty.</p> string <p>The reason for the condition\u2019s last transition in CamelCase. The specific API may choose whether or not this field is considered a guaranteed API. This field may not be empty.</p> string <p>Severity provides an explicit classification of Reason code, so the users or machines can immediately understand the current situation and act accordingly. The Severity field MUST be set only when Status=False.</p> string Required <p>Status of the condition, one of True, False, Unknown.</p> string Required <p>Type of condition in CamelCase or in foo.example.com/CamelCase. Many .condition.type values are consistent across resources like Available, but because arbitrary conditions can be useful (see .node.status.conditions), the ability to deconflict is important.</p> string <p>identityHash is the hash of the API identity key of this APIExport. This value is immutable as soon as it is set.</p> array <p>virtualWorkspaces contains all APIExport virtual workspace URLs.</p> <p>Deprecated: use APIExportEndpointSlice.status.endpoints instead</p> object string Required <p>url is an APIExport virtual workspace URL.</p>"},{"location":"reference/crd/apis.kcp.io/apiexports/#v1alpha1-.apiVersion","title":".apiVersion","text":""},{"location":"reference/crd/apis.kcp.io/apiexports/#v1alpha1-.kind","title":".kind","text":""},{"location":"reference/crd/apis.kcp.io/apiexports/#v1alpha1-.metadata","title":".metadata","text":""},{"location":"reference/crd/apis.kcp.io/apiexports/#v1alpha1-.spec","title":".spec","text":""},{"location":"reference/crd/apis.kcp.io/apiexports/#v1alpha1-.spec.identity","title":".spec.identity","text":""},{"location":"reference/crd/apis.kcp.io/apiexports/#v1alpha1-.spec.identity.secretRef","title":".spec.identity.secretRef","text":""},{"location":"reference/crd/apis.kcp.io/apiexports/#v1alpha1-.spec.identity.secretRef.name","title":".spec.identity.secretRef.name","text":""},{"location":"reference/crd/apis.kcp.io/apiexports/#v1alpha1-.spec.identity.secretRef.namespace","title":".spec.identity.secretRef.namespace","text":""},{"location":"reference/crd/apis.kcp.io/apiexports/#v1alpha1-.spec.latestResourceSchemas","title":".spec.latestResourceSchemas","text":""},{"location":"reference/crd/apis.kcp.io/apiexports/#v1alpha1-.spec.latestResourceSchemas[*]","title":".spec.latestResourceSchemas[*]","text":""},{"location":"reference/crd/apis.kcp.io/apiexports/#v1alpha1-.spec.maximalPermissionPolicy","title":".spec.maximalPermissionPolicy","text":""},{"location":"reference/crd/apis.kcp.io/apiexports/#v1alpha1-.spec.maximalPermissionPolicy.local","title":".spec.maximalPermissionPolicy.local","text":""},{"location":"reference/crd/apis.kcp.io/apiexports/#v1alpha1-.spec.permissionClaims","title":".spec.permissionClaims","text":""},{"location":"reference/crd/apis.kcp.io/apiexports/#v1alpha1-.spec.permissionClaims[*]","title":".spec.permissionClaims[*]","text":""},{"location":"reference/crd/apis.kcp.io/apiexports/#v1alpha1-.spec.permissionClaims[*].all","title":".spec.permissionClaims[*].all","text":""},{"location":"reference/crd/apis.kcp.io/apiexports/#v1alpha1-.spec.permissionClaims[*].group","title":".spec.permissionClaims[*].group","text":""},{"location":"reference/crd/apis.kcp.io/apiexports/#v1alpha1-.spec.permissionClaims[*].identityHash","title":".spec.permissionClaims[*].identityHash","text":""},{"location":"reference/crd/apis.kcp.io/apiexports/#v1alpha1-.spec.permissionClaims[*].resource","title":".spec.permissionClaims[*].resource","text":""},{"location":"reference/crd/apis.kcp.io/apiexports/#v1alpha1-.spec.permissionClaims[*].resourceSelector","title":".spec.permissionClaims[*].resourceSelector","text":""},{"location":"reference/crd/apis.kcp.io/apiexports/#v1alpha1-.spec.permissionClaims[*].resourceSelector[*]","title":".spec.permissionClaims[*].resourceSelector[*]","text":""},{"location":"reference/crd/apis.kcp.io/apiexports/#v1alpha1-.spec.permissionClaims[*].resourceSelector[*].name","title":".spec.permissionClaims[*].resourceSelector[*].name","text":""},{"location":"reference/crd/apis.kcp.io/apiexports/#v1alpha1-.spec.permissionClaims[*].resourceSelector[*].namespace","title":".spec.permissionClaims[*].resourceSelector[*].namespace","text":""},{"location":"reference/crd/apis.kcp.io/apiexports/#v1alpha1-.status","title":".status","text":""},{"location":"reference/crd/apis.kcp.io/apiexports/#v1alpha1-.status.conditions","title":".status.conditions","text":""},{"location":"reference/crd/apis.kcp.io/apiexports/#v1alpha1-.status.conditions[*]","title":".status.conditions[*]","text":""},{"location":"reference/crd/apis.kcp.io/apiexports/#v1alpha1-.status.conditions[*].lastTransitionTime","title":".status.conditions[*].lastTransitionTime","text":""},{"location":"reference/crd/apis.kcp.io/apiexports/#v1alpha1-.status.conditions[*].message","title":".status.conditions[*].message","text":""},{"location":"reference/crd/apis.kcp.io/apiexports/#v1alpha1-.status.conditions[*].reason","title":".status.conditions[*].reason","text":""},{"location":"reference/crd/apis.kcp.io/apiexports/#v1alpha1-.status.conditions[*].severity","title":".status.conditions[*].severity","text":""},{"location":"reference/crd/apis.kcp.io/apiexports/#v1alpha1-.status.conditions[*].status","title":".status.conditions[*].status","text":""},{"location":"reference/crd/apis.kcp.io/apiexports/#v1alpha1-.status.conditions[*].type","title":".status.conditions[*].type","text":""},{"location":"reference/crd/apis.kcp.io/apiexports/#v1alpha1-.status.identityHash","title":".status.identityHash","text":""},{"location":"reference/crd/apis.kcp.io/apiexports/#v1alpha1-.status.virtualWorkspaces","title":".status.virtualWorkspaces","text":""},{"location":"reference/crd/apis.kcp.io/apiexports/#v1alpha1-.status.virtualWorkspaces[*]","title":".status.virtualWorkspaces[*]","text":""},{"location":"reference/crd/apis.kcp.io/apiexports/#v1alpha1-.status.virtualWorkspaces[*].url","title":".status.virtualWorkspaces[*].url","text":""},{"location":"reference/crd/apis.kcp.io/apiresourceschemas/","title":"APIResourceSchema","text":""},{"location":"reference/crd/apis.kcp.io/apiresourceschemas/#apiresourceschema-crd-schema-reference-group-apiskcpio","title":"APIResourceSchema CRD schema reference (group apis.kcp.io)","text":"APIResourceSchema describes a resource, identified by (group, version, resource, schema).  An APIResourceSchema is immutable and cannot be deleted if they are referenced by an APIExport in the same workspace.  Full name: apiresourceschemas.apis.kcp.io Group: apis.kcp.io Singular name: apiresourceschema Plural name: apiresourceschemas Scope: Cluster Versions: v1alpha1"},{"location":"reference/crd/apis.kcp.io/apiresourceschemas/#v1alpha1","title":"Version v1alpha1","text":""},{"location":"reference/crd/apis.kcp.io/apiresourceschemas/#property-details-v1alpha1","title":"Properties","text":"string <p>APIVersion defines the versioned schema of this representation of an object. Servers should convert recognized schemas to the latest internal value, and may reject unrecognized values. More info: https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources</p> string <p>Kind is a string value representing the REST resource this object represents. Servers may infer this from the endpoint the client submits requests to. Cannot be updated. In CamelCase. More info: https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds</p> object object <p>Spec holds the desired state.</p> object <p>conversion defines conversion settings for the defined custom resource.</p> string Required <p>strategy specifies how custom resources are converted between versions. Allowed values are: - <code>\"None\"</code>: The converter only change the apiVersion and would not touch any other field in the custom resource. - <code>\"Webhook\"</code>: API Server will call to an external webhook to do the conversion. Additional information   is needed for this option. This requires spec.preserveUnknownFields to be false, and spec.conversion.webhook to be set.</p> object <p>webhook describes how to call the conversion webhook. Required when <code>strategy</code> is set to <code>\"Webhook\"</code>.</p> object <p>clientConfig is the instructions for how to call the webhook if strategy is <code>Webhook</code>.</p> string <p>caBundle is a PEM encoded CA bundle which will be used to validate the webhook\u2019s server certificate. If unspecified, system trust roots on the apiserver are used.</p> string <p>url gives the location of the webhook, in standard URL form (<code>scheme://host:port/path</code>).</p> <p>Please note that using <code>localhost</code> or <code>127.0.0.1</code> as a <code>host</code> is risky unless you take great care to run this webhook on all hosts which run an apiserver which might need to make calls to this webhook. Such installs are likely to be non-portable, i.e., not easy to turn up in a new cluster.</p> <p>The scheme must be \u201chttps\u201d; the URL must begin with \u201chttps://\u201d.</p> <p>A path is optional, and if present may be any string permissible in a URL. You may use the path to pass an arbitrary string to the webhook, for example, a cluster identifier.</p> <p>Attempting to use a user or basic auth e.g. \u201cuser:password@\u201d is not allowed. Fragments (\u201c#\u2026\u201d) and query parameters (\u201c?\u2026\u201d) are not allowed, either.</p> <p>Note: kcp does not support provided service names like Kubernetes does.</p> array Required <p>conversionReviewVersions is an ordered list of preferred <code>ConversionReview</code> versions the Webhook expects. The API server will use the first version in the list which it supports. If none of the versions specified in this list are supported by API server, conversion will fail for the custom resource. If a persisted Webhook configuration specifies allowed versions and does not include any versions known to the API Server, calls to the webhook will fail.</p> string string Required <p>group is the API group of the defined custom resource. Empty string means the core API group.     The resources are served under <code>/apis/&lt;group&gt;/...</code> or <code>/api</code> for the core group.</p> string <p>nameValidation can be used to configure name validation for bound APIs. Allowed values are <code>DNS1123Subdomain</code> and <code>PathSegmentName</code>. - DNS1123Subdomain: a lowercase RFC 1123 subdomain must consist of lower case   alphanumeric characters, \u2018-\u2019 or \u2018.\u2019, and must start and end with an alphanumeric character.   Regex used is \u2018a-z0-9?(\\.a-z0-9?)*\u2019 - PathSegmentName: validates the name can be safely encoded as a path segment.   The name may not be \u2018.\u2019 or \u2018..\u2019 and the name may not contain \u2018/\u2019 or \u2018%\u2019.</p> <p>Defaults to <code>DNS1123Subdomain</code>, matching the behaviour of CRDs.</p> object Required <p>names specify the resource and kind names for the custom resource.</p> array <p>categories is a list of grouped resources this custom resource belongs to (e.g. \u2018all\u2019). This is published in API discovery documents, and used by clients to support invocations like <code>kubectl get all</code>.</p> string string Required <p>kind is the serialized kind of the resource. It is normally CamelCase and singular. Custom resource instances will use this value as the <code>kind</code> attribute in API calls.</p> string <p>listKind is the serialized kind of the list for this resource. Defaults to \u201c<code>kind</code>List\u201d.</p> string Required <p>plural is the plural name of the resource to serve. The custom resources are served under <code>/apis/&lt;group&gt;/&lt;version&gt;/.../&lt;plural&gt;</code>. Must match the name of the CustomResourceDefinition (in the form <code>&lt;names.plural&gt;.&lt;group&gt;</code>). Must be all lowercase.</p> array <p>shortNames are short names for the resource, exposed in API discovery documents, and used by clients to support invocations like <code>kubectl get &lt;shortname&gt;</code>. It must be all lowercase.</p> string string <p>singular is the singular name of the resource. It must be all lowercase. Defaults to lowercased <code>kind</code>.</p> string Required <p>scope indicates whether the defined custom resource is cluster- or namespace-scoped. Allowed values are <code>Cluster</code> and <code>Namespaced</code>.</p> array Required <p>versions is the API version of the defined custom resource.</p> <p>Note: the OpenAPI v3 schemas must be equal for all versions until CEL       version migration is supported.</p> object <p>APIResourceVersion describes one API version of a resource.</p> array <p>additionalPrinterColumns specifies additional columns returned in Table output. See https://kubernetes.io/docs/reference/using-api/api-concepts/#receiving-resources-as-tables for details. If no columns are specified, a single column displaying the age of the custom resource is used.</p> object <p>CustomResourceColumnDefinition specifies a column for server side printing.</p> string <p>description is a human readable description of this column.</p> string <p>format is an optional OpenAPI type definition for this column. The \u2018name\u2019 format is applied to the primary identifier column to assist in clients identifying column is the resource name. See https://github.com/OAI/OpenAPI-Specification/blob/master/versions/2.0.md#data-types for details.</p> string Required <p>jsonPath is a simple JSON path (i.e. with array notation) which is evaluated against each custom resource to produce the value for this column.</p> string Required <p>name is a human readable name for the column.</p> integer <p>priority is an integer defining the relative importance of this column compared to others. Lower numbers are considered higher priority. Columns that may be omitted in limited space scenarios should be given a priority greater than 0.</p> string Required <p>type is an OpenAPI type definition for this column. See https://github.com/OAI/OpenAPI-Specification/blob/master/versions/2.0.md#data-types for details.</p> boolean <p>deprecated indicates this version of the custom resource API is deprecated. When set to true, API requests to this version receive a warning header in the server response. Defaults to false.</p> string <p>deprecationWarning overrides the default warning returned to API clients. May only be set when <code>deprecated</code> is true. The default warning indicates this version is deprecated and recommends use of the newest served version of equal or greater stability, if one exists.</p> string Required <p>name is the version name, e.g. \u201cv1\u201d, \u201cv2beta1\u201d, etc. The custom resources are served under this version at <code>/apis/&lt;group&gt;/&lt;version&gt;/...</code> if <code>served</code> is true.</p> object Required <p>schema describes the structural schema used for validation, pruning, and defaulting of this version of the custom resource.</p> boolean Required <p>served is a flag enabling/disabling this version from being served via REST APIs</p> boolean Required <p>storage indicates this version should be used when persisting custom resources to storage. There must be exactly one version with storage=true.</p> object <p>subresources specify what subresources this version of the defined custom resource have.</p> object <p>scale indicates the custom resource should serve a <code>/scale</code> subresource that returns an <code>autoscaling/v1</code> Scale object.</p> string <p>labelSelectorPath defines the JSON path inside of a custom resource that corresponds to Scale <code>status.selector</code>. Only JSON paths without the array notation are allowed. Must be a JSON Path under <code>.status</code> or <code>.spec</code>. Must be set to work with HorizontalPodAutoscaler. The field pointed by this JSON path must be a string field (not a complex selector struct) which contains a serialized label selector in string form. More info: https://kubernetes.io/docs/tasks/access-kubernetes-api/custom-resources/custom-resource-definitions#scale-subresource If there is no value under the given path in the custom resource, the <code>status.selector</code> value in the <code>/scale</code> subresource will default to the empty string.</p> string Required <p>specReplicasPath defines the JSON path inside of a custom resource that corresponds to Scale <code>spec.replicas</code>. Only JSON paths without the array notation are allowed. Must be a JSON Path under <code>.spec</code>. If there is no value under the given path in the custom resource, the <code>/scale</code> subresource will return an error on GET.</p> string Required <p>statusReplicasPath defines the JSON path inside of a custom resource that corresponds to Scale <code>status.replicas</code>. Only JSON paths without the array notation are allowed. Must be a JSON Path under <code>.status</code>. If there is no value under the given path in the custom resource, the <code>status.replicas</code> value in the <code>/scale</code> subresource will default to 0.</p> object <p>status indicates the custom resource should serve a <code>/status</code> subresource. When enabled: 1. requests to the custom resource primary endpoint ignore changes to the <code>status</code> stanza of the object. 2. requests to the custom resource <code>/status</code> subresource ignore changes to anything other than the <code>status</code> stanza of the object.</p>"},{"location":"reference/crd/apis.kcp.io/apiresourceschemas/#v1alpha1-.apiVersion","title":".apiVersion","text":""},{"location":"reference/crd/apis.kcp.io/apiresourceschemas/#v1alpha1-.kind","title":".kind","text":""},{"location":"reference/crd/apis.kcp.io/apiresourceschemas/#v1alpha1-.metadata","title":".metadata","text":""},{"location":"reference/crd/apis.kcp.io/apiresourceschemas/#v1alpha1-.spec","title":".spec","text":""},{"location":"reference/crd/apis.kcp.io/apiresourceschemas/#v1alpha1-.spec.conversion","title":".spec.conversion","text":""},{"location":"reference/crd/apis.kcp.io/apiresourceschemas/#v1alpha1-.spec.conversion.strategy","title":".spec.conversion.strategy","text":""},{"location":"reference/crd/apis.kcp.io/apiresourceschemas/#v1alpha1-.spec.conversion.webhook","title":".spec.conversion.webhook","text":""},{"location":"reference/crd/apis.kcp.io/apiresourceschemas/#v1alpha1-.spec.conversion.webhook.clientConfig","title":".spec.conversion.webhook.clientConfig","text":""},{"location":"reference/crd/apis.kcp.io/apiresourceschemas/#v1alpha1-.spec.conversion.webhook.clientConfig.caBundle","title":".spec.conversion.webhook.clientConfig.caBundle","text":""},{"location":"reference/crd/apis.kcp.io/apiresourceschemas/#v1alpha1-.spec.conversion.webhook.clientConfig.url","title":".spec.conversion.webhook.clientConfig.url","text":""},{"location":"reference/crd/apis.kcp.io/apiresourceschemas/#v1alpha1-.spec.conversion.webhook.conversionReviewVersions","title":".spec.conversion.webhook.conversionReviewVersions","text":""},{"location":"reference/crd/apis.kcp.io/apiresourceschemas/#v1alpha1-.spec.conversion.webhook.conversionReviewVersions[*]","title":".spec.conversion.webhook.conversionReviewVersions[*]","text":""},{"location":"reference/crd/apis.kcp.io/apiresourceschemas/#v1alpha1-.spec.group","title":".spec.group","text":""},{"location":"reference/crd/apis.kcp.io/apiresourceschemas/#v1alpha1-.spec.nameValidation","title":".spec.nameValidation","text":""},{"location":"reference/crd/apis.kcp.io/apiresourceschemas/#v1alpha1-.spec.names","title":".spec.names","text":""},{"location":"reference/crd/apis.kcp.io/apiresourceschemas/#v1alpha1-.spec.names.categories","title":".spec.names.categories","text":""},{"location":"reference/crd/apis.kcp.io/apiresourceschemas/#v1alpha1-.spec.names.categories[*]","title":".spec.names.categories[*]","text":""},{"location":"reference/crd/apis.kcp.io/apiresourceschemas/#v1alpha1-.spec.names.kind","title":".spec.names.kind","text":""},{"location":"reference/crd/apis.kcp.io/apiresourceschemas/#v1alpha1-.spec.names.listKind","title":".spec.names.listKind","text":""},{"location":"reference/crd/apis.kcp.io/apiresourceschemas/#v1alpha1-.spec.names.plural","title":".spec.names.plural","text":""},{"location":"reference/crd/apis.kcp.io/apiresourceschemas/#v1alpha1-.spec.names.shortNames","title":".spec.names.shortNames","text":""},{"location":"reference/crd/apis.kcp.io/apiresourceschemas/#v1alpha1-.spec.names.shortNames[*]","title":".spec.names.shortNames[*]","text":""},{"location":"reference/crd/apis.kcp.io/apiresourceschemas/#v1alpha1-.spec.names.singular","title":".spec.names.singular","text":""},{"location":"reference/crd/apis.kcp.io/apiresourceschemas/#v1alpha1-.spec.scope","title":".spec.scope","text":""},{"location":"reference/crd/apis.kcp.io/apiresourceschemas/#v1alpha1-.spec.versions","title":".spec.versions","text":""},{"location":"reference/crd/apis.kcp.io/apiresourceschemas/#v1alpha1-.spec.versions[*]","title":".spec.versions[*]","text":""},{"location":"reference/crd/apis.kcp.io/apiresourceschemas/#v1alpha1-.spec.versions[*].additionalPrinterColumns","title":".spec.versions[*].additionalPrinterColumns","text":""},{"location":"reference/crd/apis.kcp.io/apiresourceschemas/#v1alpha1-.spec.versions[*].additionalPrinterColumns[*]","title":".spec.versions[*].additionalPrinterColumns[*]","text":""},{"location":"reference/crd/apis.kcp.io/apiresourceschemas/#v1alpha1-.spec.versions[*].additionalPrinterColumns[*].description","title":".spec.versions[*].additionalPrinterColumns[*].description","text":""},{"location":"reference/crd/apis.kcp.io/apiresourceschemas/#v1alpha1-.spec.versions[*].additionalPrinterColumns[*].format","title":".spec.versions[*].additionalPrinterColumns[*].format","text":""},{"location":"reference/crd/apis.kcp.io/apiresourceschemas/#v1alpha1-.spec.versions[*].additionalPrinterColumns[*].jsonPath","title":".spec.versions[*].additionalPrinterColumns[*].jsonPath","text":""},{"location":"reference/crd/apis.kcp.io/apiresourceschemas/#v1alpha1-.spec.versions[*].additionalPrinterColumns[*].name","title":".spec.versions[*].additionalPrinterColumns[*].name","text":""},{"location":"reference/crd/apis.kcp.io/apiresourceschemas/#v1alpha1-.spec.versions[*].additionalPrinterColumns[*].priority","title":".spec.versions[*].additionalPrinterColumns[*].priority","text":""},{"location":"reference/crd/apis.kcp.io/apiresourceschemas/#v1alpha1-.spec.versions[*].additionalPrinterColumns[*].type","title":".spec.versions[*].additionalPrinterColumns[*].type","text":""},{"location":"reference/crd/apis.kcp.io/apiresourceschemas/#v1alpha1-.spec.versions[*].deprecated","title":".spec.versions[*].deprecated","text":""},{"location":"reference/crd/apis.kcp.io/apiresourceschemas/#v1alpha1-.spec.versions[*].deprecationWarning","title":".spec.versions[*].deprecationWarning","text":""},{"location":"reference/crd/apis.kcp.io/apiresourceschemas/#v1alpha1-.spec.versions[*].name","title":".spec.versions[*].name","text":""},{"location":"reference/crd/apis.kcp.io/apiresourceschemas/#v1alpha1-.spec.versions[*].schema","title":".spec.versions[*].schema","text":""},{"location":"reference/crd/apis.kcp.io/apiresourceschemas/#v1alpha1-.spec.versions[*].served","title":".spec.versions[*].served","text":""},{"location":"reference/crd/apis.kcp.io/apiresourceschemas/#v1alpha1-.spec.versions[*].storage","title":".spec.versions[*].storage","text":""},{"location":"reference/crd/apis.kcp.io/apiresourceschemas/#v1alpha1-.spec.versions[*].subresources","title":".spec.versions[*].subresources","text":""},{"location":"reference/crd/apis.kcp.io/apiresourceschemas/#v1alpha1-.spec.versions[*].subresources.scale","title":".spec.versions[*].subresources.scale","text":""},{"location":"reference/crd/apis.kcp.io/apiresourceschemas/#v1alpha1-.spec.versions[*].subresources.scale.labelSelectorPath","title":".spec.versions[*].subresources.scale.labelSelectorPath","text":""},{"location":"reference/crd/apis.kcp.io/apiresourceschemas/#v1alpha1-.spec.versions[*].subresources.scale.specReplicasPath","title":".spec.versions[*].subresources.scale.specReplicasPath","text":""},{"location":"reference/crd/apis.kcp.io/apiresourceschemas/#v1alpha1-.spec.versions[*].subresources.scale.statusReplicasPath","title":".spec.versions[*].subresources.scale.statusReplicasPath","text":""},{"location":"reference/crd/apis.kcp.io/apiresourceschemas/#v1alpha1-.spec.versions[*].subresources.status","title":".spec.versions[*].subresources.status","text":""},{"location":"reference/crd/core.kcp.io/logicalclusters/","title":"LogicalCluster","text":""},{"location":"reference/crd/core.kcp.io/logicalclusters/#logicalcluster-crd-schema-reference-group-corekcpio","title":"LogicalCluster CRD schema reference (group core.kcp.io)","text":"LogicalCluster describes the current logical cluster. It is used to authorize requests to the logical cluster and to track state.  A LogicalCluster is always named \"cluster\".  Full name: logicalclusters.core.kcp.io Group: core.kcp.io Singular name: logicalcluster Plural name: logicalclusters Scope: Cluster Versions: v1alpha1"},{"location":"reference/crd/core.kcp.io/logicalclusters/#v1alpha1","title":"Version v1alpha1","text":""},{"location":"reference/crd/core.kcp.io/logicalclusters/#property-details-v1alpha1","title":"Properties","text":"string <p>APIVersion defines the versioned schema of this representation of an object. Servers should convert recognized schemas to the latest internal value, and may reject unrecognized values. More info: https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources</p> string <p>Kind is a string value representing the REST resource this object represents. Servers may infer this from the endpoint the client submits requests to. Cannot be updated. In CamelCase. More info: https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds</p> object string object <p>LogicalClusterSpec is the specification of the LogicalCluster resource.</p> boolean <p>DirectlyDeletable indicates that this logical cluster can be directly deleted by the user from within by deleting the LogicalCluster object.</p> array <p>initializers are set on creation by the system and copied to status when initialization starts.</p> string <p>LogicalClusterInitializer is a unique string corresponding to a logical cluster initialization controller.</p> object <p>owner is a reference to a resource controlling the life-cycle of this logical cluster. On deletion of the LogicalCluster, the finalizer core.kcp.io/logicalcluster is removed from the owner.</p> <p>When this object is deleted, but the owner is not deleted, the owner is deleted too.</p> string Required <p>apiVersion is the group and API version of the owner.</p> string Required <p>cluster is the logical cluster in which the owner is located.</p> string Required <p>name is the name of the owner.</p> string <p>namespace is the optional namespace of the owner.</p> string Required <p>resource is API resource to access the owner.</p> string Required <p>UID is the UID of the owner.</p> object <p>LogicalClusterStatus communicates the observed state of the Workspace.</p> string <p>url is the address under which the Kubernetes-cluster-like endpoint can be found. This URL can be used to access the logical cluster with standard Kubernetes client libraries and command line tools.</p> array <p>Current processing state of the LogicalCluster.</p> object <p>Condition defines an observation of a object operational state.</p> string Required <p>Last time the condition transitioned from one status to another. This should be when the underlying condition changed. If that is not known, then using the time when the API field changed is acceptable.</p> string <p>A human readable message indicating details about the transition. This field may be empty.</p> string <p>The reason for the condition\u2019s last transition in CamelCase. The specific API may choose whether or not this field is considered a guaranteed API. This field may not be empty.</p> string <p>Severity provides an explicit classification of Reason code, so the users or machines can immediately understand the current situation and act accordingly. The Severity field MUST be set only when Status=False.</p> string Required <p>Status of the condition, one of True, False, Unknown.</p> string Required <p>Type of condition in CamelCase or in foo.example.com/CamelCase. Many .condition.type values are consistent across resources like Available, but because arbitrary conditions can be useful (see .node.status.conditions), the ability to deconflict is important.</p> array <p>initializers are set on creation by the system and must be cleared by a controller before the logical cluster can be used. The LogicalCluster object will stay in the phase \u201cInitializing\u201d state until all initializers are cleared.</p> string <p>LogicalClusterInitializer is a unique string corresponding to a logical cluster initialization controller.</p> string <p>Phase of the logical cluster (Initializing, Ready).</p>"},{"location":"reference/crd/core.kcp.io/logicalclusters/#v1alpha1-.apiVersion","title":".apiVersion","text":""},{"location":"reference/crd/core.kcp.io/logicalclusters/#v1alpha1-.kind","title":".kind","text":""},{"location":"reference/crd/core.kcp.io/logicalclusters/#v1alpha1-.metadata","title":".metadata","text":""},{"location":"reference/crd/core.kcp.io/logicalclusters/#v1alpha1-.metadata.name","title":".metadata.name","text":""},{"location":"reference/crd/core.kcp.io/logicalclusters/#v1alpha1-.spec","title":".spec","text":""},{"location":"reference/crd/core.kcp.io/logicalclusters/#v1alpha1-.spec.directlyDeletable","title":".spec.directlyDeletable","text":""},{"location":"reference/crd/core.kcp.io/logicalclusters/#v1alpha1-.spec.initializers","title":".spec.initializers","text":""},{"location":"reference/crd/core.kcp.io/logicalclusters/#v1alpha1-.spec.initializers[*]","title":".spec.initializers[*]","text":""},{"location":"reference/crd/core.kcp.io/logicalclusters/#v1alpha1-.spec.owner","title":".spec.owner","text":""},{"location":"reference/crd/core.kcp.io/logicalclusters/#v1alpha1-.spec.owner.apiVersion","title":".spec.owner.apiVersion","text":""},{"location":"reference/crd/core.kcp.io/logicalclusters/#v1alpha1-.spec.owner.cluster","title":".spec.owner.cluster","text":""},{"location":"reference/crd/core.kcp.io/logicalclusters/#v1alpha1-.spec.owner.name","title":".spec.owner.name","text":""},{"location":"reference/crd/core.kcp.io/logicalclusters/#v1alpha1-.spec.owner.namespace","title":".spec.owner.namespace","text":""},{"location":"reference/crd/core.kcp.io/logicalclusters/#v1alpha1-.spec.owner.resource","title":".spec.owner.resource","text":""},{"location":"reference/crd/core.kcp.io/logicalclusters/#v1alpha1-.spec.owner.uid","title":".spec.owner.uid","text":""},{"location":"reference/crd/core.kcp.io/logicalclusters/#v1alpha1-.status","title":".status","text":""},{"location":"reference/crd/core.kcp.io/logicalclusters/#v1alpha1-.status.URL","title":".status.URL","text":""},{"location":"reference/crd/core.kcp.io/logicalclusters/#v1alpha1-.status.conditions","title":".status.conditions","text":""},{"location":"reference/crd/core.kcp.io/logicalclusters/#v1alpha1-.status.conditions[*]","title":".status.conditions[*]","text":""},{"location":"reference/crd/core.kcp.io/logicalclusters/#v1alpha1-.status.conditions[*].lastTransitionTime","title":".status.conditions[*].lastTransitionTime","text":""},{"location":"reference/crd/core.kcp.io/logicalclusters/#v1alpha1-.status.conditions[*].message","title":".status.conditions[*].message","text":""},{"location":"reference/crd/core.kcp.io/logicalclusters/#v1alpha1-.status.conditions[*].reason","title":".status.conditions[*].reason","text":""},{"location":"reference/crd/core.kcp.io/logicalclusters/#v1alpha1-.status.conditions[*].severity","title":".status.conditions[*].severity","text":""},{"location":"reference/crd/core.kcp.io/logicalclusters/#v1alpha1-.status.conditions[*].status","title":".status.conditions[*].status","text":""},{"location":"reference/crd/core.kcp.io/logicalclusters/#v1alpha1-.status.conditions[*].type","title":".status.conditions[*].type","text":""},{"location":"reference/crd/core.kcp.io/logicalclusters/#v1alpha1-.status.initializers","title":".status.initializers","text":""},{"location":"reference/crd/core.kcp.io/logicalclusters/#v1alpha1-.status.initializers[*]","title":".status.initializers[*]","text":""},{"location":"reference/crd/core.kcp.io/logicalclusters/#v1alpha1-.status.phase","title":".status.phase","text":""},{"location":"reference/crd/core.kcp.io/shards/","title":"Shard","text":""},{"location":"reference/crd/core.kcp.io/shards/#shard-crd-schema-reference-group-corekcpio","title":"Shard CRD schema reference (group core.kcp.io)","text":"Shard describes a kcp instance on which a number of logical clusters will live  Full name: shards.core.kcp.io Group: core.kcp.io Singular name: shard Plural name: shards Scope: Cluster Versions: v1alpha1"},{"location":"reference/crd/core.kcp.io/shards/#v1alpha1","title":"Version v1alpha1","text":""},{"location":"reference/crd/core.kcp.io/shards/#property-details-v1alpha1","title":"Properties","text":"string <p>APIVersion defines the versioned schema of this representation of an object. Servers should convert recognized schemas to the latest internal value, and may reject unrecognized values. More info: https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources</p> string <p>Kind is a string value representing the REST resource this object represents. Servers may infer this from the endpoint the client submits requests to. Cannot be updated. In CamelCase. More info: https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds</p> object object <p>ShardSpec holds the desired state of the Shard.</p> string Required <p>baseURL is the address of the KCP shard for direct connections, e.g. by some front-proxy doing the fan-out to the shards.</p> string <p>externalURL is the externally visible address presented to users in Workspace URLs. Changing this will break all existing logical clusters on that shard, i.e. existing kubeconfigs of clients will be invalid. Hence, when changing this value, the old URL used by clients must keep working.</p> <p>The external address will not be unique if a front-proxy does a fan-out to shards, but all logical cluster clients will talk to the front-proxy. In that case, put the address of the front-proxy here.</p> <p>Note that movement of shards is only possible (in the future) between shards that share a common external URL.</p> <p>This will be defaulted to the value of the baseURL.</p> string <p>virtualWorkspaceURL is the address of the virtual workspace apiserver associated with this shard. It can be a direct address, an address of a front-proxy or even an address of an LB. As of today this address is assigned to APIExports.</p> <p>This will be defaulted to the value of the baseURL.</p> object <p>ShardStatus communicates the observed state of the Shard.</p> object <p>Set of integer resources that logical clusters can be scheduled into</p> array <p>Current processing state of the Shard.</p> object <p>Condition defines an observation of a object operational state.</p> string Required <p>Last time the condition transitioned from one status to another. This should be when the underlying condition changed. If that is not known, then using the time when the API field changed is acceptable.</p> string <p>A human readable message indicating details about the transition. This field may be empty.</p> string <p>The reason for the condition\u2019s last transition in CamelCase. The specific API may choose whether or not this field is considered a guaranteed API. This field may not be empty.</p> string <p>Severity provides an explicit classification of Reason code, so the users or machines can immediately understand the current situation and act accordingly. The Severity field MUST be set only when Status=False.</p> string Required <p>Status of the condition, one of True, False, Unknown.</p> string Required <p>Type of condition in CamelCase or in foo.example.com/CamelCase. Many .condition.type values are consistent across resources like Available, but because arbitrary conditions can be useful (see .node.status.conditions), the ability to deconflict is important.</p>"},{"location":"reference/crd/core.kcp.io/shards/#v1alpha1-.apiVersion","title":".apiVersion","text":""},{"location":"reference/crd/core.kcp.io/shards/#v1alpha1-.kind","title":".kind","text":""},{"location":"reference/crd/core.kcp.io/shards/#v1alpha1-.metadata","title":".metadata","text":""},{"location":"reference/crd/core.kcp.io/shards/#v1alpha1-.spec","title":".spec","text":""},{"location":"reference/crd/core.kcp.io/shards/#v1alpha1-.spec.baseURL","title":".spec.baseURL","text":""},{"location":"reference/crd/core.kcp.io/shards/#v1alpha1-.spec.externalURL","title":".spec.externalURL","text":""},{"location":"reference/crd/core.kcp.io/shards/#v1alpha1-.spec.virtualWorkspaceURL","title":".spec.virtualWorkspaceURL","text":""},{"location":"reference/crd/core.kcp.io/shards/#v1alpha1-.status","title":".status","text":""},{"location":"reference/crd/core.kcp.io/shards/#v1alpha1-.status.capacity","title":".status.capacity","text":""},{"location":"reference/crd/core.kcp.io/shards/#v1alpha1-.status.conditions","title":".status.conditions","text":""},{"location":"reference/crd/core.kcp.io/shards/#v1alpha1-.status.conditions[*]","title":".status.conditions[*]","text":""},{"location":"reference/crd/core.kcp.io/shards/#v1alpha1-.status.conditions[*].lastTransitionTime","title":".status.conditions[*].lastTransitionTime","text":""},{"location":"reference/crd/core.kcp.io/shards/#v1alpha1-.status.conditions[*].message","title":".status.conditions[*].message","text":""},{"location":"reference/crd/core.kcp.io/shards/#v1alpha1-.status.conditions[*].reason","title":".status.conditions[*].reason","text":""},{"location":"reference/crd/core.kcp.io/shards/#v1alpha1-.status.conditions[*].severity","title":".status.conditions[*].severity","text":""},{"location":"reference/crd/core.kcp.io/shards/#v1alpha1-.status.conditions[*].status","title":".status.conditions[*].status","text":""},{"location":"reference/crd/core.kcp.io/shards/#v1alpha1-.status.conditions[*].type","title":".status.conditions[*].type","text":""},{"location":"reference/crd/tenancy.kcp.io/workspaces/","title":"Workspace","text":""},{"location":"reference/crd/tenancy.kcp.io/workspaces/#workspace-crd-schema-reference-group-tenancykcpio","title":"Workspace CRD schema reference (group tenancy.kcp.io)","text":"Workspace defines a generic Kubernetes-cluster-like endpoint, with standard Kubernetes discovery APIs, OpenAPI and resource API endpoints.  A workspace can be backed by different concrete types of workspace implementation, depending on access pattern. All workspace implementations share the characteristic that the URL that serves a given workspace can be used with standard Kubernetes API machinery and client libraries and command line tools.  Full name: workspaces.tenancy.kcp.io Group: tenancy.kcp.io Singular name: workspace Plural name: workspaces Scope: Cluster Versions: v1alpha1"},{"location":"reference/crd/tenancy.kcp.io/workspaces/#v1alpha1","title":"Version v1alpha1","text":""},{"location":"reference/crd/tenancy.kcp.io/workspaces/#property-details-v1alpha1","title":"Properties","text":"string <p>APIVersion defines the versioned schema of this representation of an object. Servers should convert recognized schemas to the latest internal value, and may reject unrecognized values. More info: https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources</p> string <p>Kind is a string value representing the REST resource this object represents. Servers may infer this from the endpoint the client submits requests to. Cannot be updated. In CamelCase. More info: https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds</p> object string object Required <p>WorkspaceSpec holds the desired state of the Workspace.</p> string <p>URL is the address under which the Kubernetes-cluster-like endpoint can be found. This URL can be used to access the workspace with standard Kubernetes client libraries and command line tools.</p> <p>Set by the system.</p> string <p>cluster is the name of the logical cluster this workspace is stored under.</p> <p>Set by the system.</p> object <p>location constraints where this workspace can be scheduled to.</p> <p>If the no location is specified, an arbitrary location is chosen.</p> object <p>selector is a label selector that filters workspace scheduling targets.</p> array <p>matchExpressions is a list of label selector requirements. The requirements are ANDed.</p> object <p>A label selector requirement is a selector that contains values, a key, and an operator that relates the key and values.</p> string Required <p>key is the label key that the selector applies to.</p> string Required <p>operator represents a key\u2019s relationship to a set of values. Valid operators are In, NotIn, Exists and DoesNotExist.</p> array <p>values is an array of string values. If the operator is In or NotIn, the values array must be non-empty. If the operator is Exists or DoesNotExist, the values array must be empty. This array is replaced during a strategic merge patch.</p> string object <p>matchLabels is a map of {key,value} pairs. A single {key,value} in the matchLabels map is equivalent to an element of matchExpressions, whose key field is \u201ckey\u201d, the operator is \u201cIn\u201d, and the values array contains only \u201cvalue\u201d. The requirements are ANDed.</p> object <p>type defines properties of the workspace both on creation (e.g. initial resources and initially installed APIs) and during runtime (e.g. permissions). If no type is provided, the default type for the workspace in which this workspace is nesting will be used.</p> <p>The type is a reference to a WorkspaceType in the listed workspace, but lower-cased. The WorkspaceType existence is validated at admission during creation. The type is immutable after creation. The use of a type is gated via the RBAC workspacetypes/use resource permission.</p> string Required <p>name is the name of the WorkspaceType</p> string <p>path is an absolute reference to the workspace that owns this type, e.g. root:org:ws.</p> object <p>WorkspaceStatus communicates the observed state of the Workspace.</p> array <p>Current processing state of the Workspace.</p> object <p>Condition defines an observation of a object operational state.</p> string Required <p>Last time the condition transitioned from one status to another. This should be when the underlying condition changed. If that is not known, then using the time when the API field changed is acceptable.</p> string <p>A human readable message indicating details about the transition. This field may be empty.</p> string <p>The reason for the condition\u2019s last transition in CamelCase. The specific API may choose whether or not this field is considered a guaranteed API. This field may not be empty.</p> string <p>Severity provides an explicit classification of Reason code, so the users or machines can immediately understand the current situation and act accordingly. The Severity field MUST be set only when Status=False.</p> string Required <p>Status of the condition, one of True, False, Unknown.</p> string Required <p>Type of condition in CamelCase or in foo.example.com/CamelCase. Many .condition.type values are consistent across resources like Available, but because arbitrary conditions can be useful (see .node.status.conditions), the ability to deconflict is important.</p> array <p>initializers must be cleared by a controller before the workspace is ready and can be used.</p> string <p>LogicalClusterInitializer is a unique string corresponding to a logical cluster initialization controller.</p> string <p>Phase of the workspace (Scheduling, Initializing, Ready).</p>"},{"location":"reference/crd/tenancy.kcp.io/workspaces/#v1alpha1-.apiVersion","title":".apiVersion","text":""},{"location":"reference/crd/tenancy.kcp.io/workspaces/#v1alpha1-.kind","title":".kind","text":""},{"location":"reference/crd/tenancy.kcp.io/workspaces/#v1alpha1-.metadata","title":".metadata","text":""},{"location":"reference/crd/tenancy.kcp.io/workspaces/#v1alpha1-.metadata.name","title":".metadata.name","text":""},{"location":"reference/crd/tenancy.kcp.io/workspaces/#v1alpha1-.spec","title":".spec","text":""},{"location":"reference/crd/tenancy.kcp.io/workspaces/#v1alpha1-.spec.URL","title":".spec.URL","text":""},{"location":"reference/crd/tenancy.kcp.io/workspaces/#v1alpha1-.spec.cluster","title":".spec.cluster","text":""},{"location":"reference/crd/tenancy.kcp.io/workspaces/#v1alpha1-.spec.location","title":".spec.location","text":""},{"location":"reference/crd/tenancy.kcp.io/workspaces/#v1alpha1-.spec.location.selector","title":".spec.location.selector","text":""},{"location":"reference/crd/tenancy.kcp.io/workspaces/#v1alpha1-.spec.location.selector.matchExpressions","title":".spec.location.selector.matchExpressions","text":""},{"location":"reference/crd/tenancy.kcp.io/workspaces/#v1alpha1-.spec.location.selector.matchExpressions[*]","title":".spec.location.selector.matchExpressions[*]","text":""},{"location":"reference/crd/tenancy.kcp.io/workspaces/#v1alpha1-.spec.location.selector.matchExpressions[*].key","title":".spec.location.selector.matchExpressions[*].key","text":""},{"location":"reference/crd/tenancy.kcp.io/workspaces/#v1alpha1-.spec.location.selector.matchExpressions[*].operator","title":".spec.location.selector.matchExpressions[*].operator","text":""},{"location":"reference/crd/tenancy.kcp.io/workspaces/#v1alpha1-.spec.location.selector.matchExpressions[*].values","title":".spec.location.selector.matchExpressions[*].values","text":""},{"location":"reference/crd/tenancy.kcp.io/workspaces/#v1alpha1-.spec.location.selector.matchExpressions[*].values[*]","title":".spec.location.selector.matchExpressions[*].values[*]","text":""},{"location":"reference/crd/tenancy.kcp.io/workspaces/#v1alpha1-.spec.location.selector.matchLabels","title":".spec.location.selector.matchLabels","text":""},{"location":"reference/crd/tenancy.kcp.io/workspaces/#v1alpha1-.spec.type","title":".spec.type","text":""},{"location":"reference/crd/tenancy.kcp.io/workspaces/#v1alpha1-.spec.type.name","title":".spec.type.name","text":""},{"location":"reference/crd/tenancy.kcp.io/workspaces/#v1alpha1-.spec.type.path","title":".spec.type.path","text":""},{"location":"reference/crd/tenancy.kcp.io/workspaces/#v1alpha1-.status","title":".status","text":""},{"location":"reference/crd/tenancy.kcp.io/workspaces/#v1alpha1-.status.conditions","title":".status.conditions","text":""},{"location":"reference/crd/tenancy.kcp.io/workspaces/#v1alpha1-.status.conditions[*]","title":".status.conditions[*]","text":""},{"location":"reference/crd/tenancy.kcp.io/workspaces/#v1alpha1-.status.conditions[*].lastTransitionTime","title":".status.conditions[*].lastTransitionTime","text":""},{"location":"reference/crd/tenancy.kcp.io/workspaces/#v1alpha1-.status.conditions[*].message","title":".status.conditions[*].message","text":""},{"location":"reference/crd/tenancy.kcp.io/workspaces/#v1alpha1-.status.conditions[*].reason","title":".status.conditions[*].reason","text":""},{"location":"reference/crd/tenancy.kcp.io/workspaces/#v1alpha1-.status.conditions[*].severity","title":".status.conditions[*].severity","text":""},{"location":"reference/crd/tenancy.kcp.io/workspaces/#v1alpha1-.status.conditions[*].status","title":".status.conditions[*].status","text":""},{"location":"reference/crd/tenancy.kcp.io/workspaces/#v1alpha1-.status.conditions[*].type","title":".status.conditions[*].type","text":""},{"location":"reference/crd/tenancy.kcp.io/workspaces/#v1alpha1-.status.initializers","title":".status.initializers","text":""},{"location":"reference/crd/tenancy.kcp.io/workspaces/#v1alpha1-.status.initializers[*]","title":".status.initializers[*]","text":""},{"location":"reference/crd/tenancy.kcp.io/workspaces/#v1alpha1-.status.phase","title":".status.phase","text":""},{"location":"reference/crd/tenancy.kcp.io/workspacetypes/","title":"WorkspaceType","text":""},{"location":"reference/crd/tenancy.kcp.io/workspacetypes/#workspacetype-crd-schema-reference-group-tenancykcpio","title":"WorkspaceType CRD schema reference (group tenancy.kcp.io)","text":"WorkspaceType specifies behaviour of workspaces of this type.  Full name: workspacetypes.tenancy.kcp.io Group: tenancy.kcp.io Singular name: workspacetype Plural name: workspacetypes Scope: Cluster Versions: v1alpha1"},{"location":"reference/crd/tenancy.kcp.io/workspacetypes/#v1alpha1","title":"Version v1alpha1","text":""},{"location":"reference/crd/tenancy.kcp.io/workspacetypes/#property-details-v1alpha1","title":"Properties","text":"string <p>APIVersion defines the versioned schema of this representation of an object. Servers should convert recognized schemas to the latest internal value, and may reject unrecognized values. More info: https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources</p> string <p>Kind is a string value representing the REST resource this object represents. Servers may infer this from the endpoint the client submits requests to. Cannot be updated. In CamelCase. More info: https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds</p> object string object object <p>additionalWorkspaceLabels are a set of labels that will be added to a Workspace on creation.</p> array <p>defaultAPIBindings are the APIs to bind during initialization of workspaces created from this type. The APIBinding names will be generated dynamically.</p> object <p>APIExportReference provides the fields necessary to resolve an APIExport.</p> string Required <p>export is the name of the APIExport.</p> string <p>path is the fully-qualified path to the workspace containing the APIExport. If it is empty, the current workspace is assumed.</p> object <p>defaultChildWorkspaceType is the WorkspaceType that will be used by default if another, nested Workspace is created in a workspace of this type. When this field is unset, the user must specify a type when creating nested workspaces. Extending another WorkspaceType does not inherit its defaultChildWorkspaceType.</p> string Required <p>name is the name of the WorkspaceType</p> string <p>path is an absolute reference to the workspace that owns this type, e.g. root:org:ws.</p> object <p>extend is a list of other WorkspaceTypes whose initializers and limitAllowedChildren and limitAllowedParents this WorkspaceType is inheriting. By (transitively) extending another WorkspaceType, this WorkspaceType will be considered as that other type in evaluation of limitAllowedChildren and limitAllowedParents constraints.</p> <p>A dependency cycle stop this WorkspaceType from being admitted as the type of a Workspace.</p> <p>A non-existing dependency stop this WorkspaceType from being admitted as the type of a Workspace.</p> array <p>with are WorkspaceTypes whose initializers are added to the list for the owning type, and for whom the owning type becomes an alias, as long as all of their required types are not mentioned in without.</p> object <p>WorkspaceTypeReference is a globally unique, fully qualified reference to a workspace type.</p> string Required <p>name is the name of the WorkspaceType</p> string <p>path is an absolute reference to the workspace that owns this type, e.g. root:org:ws.</p> boolean <p>initializer determines if this WorkspaceType has an associated initializing controller. These controllers are used to add functionality to a Workspace; all controllers must finish their work before the Workspace becomes ready for use.</p> <p>One initializing controller is supported per WorkspaceType; the identifier for this initializer will be a colon-delimited string using the workspace in which the WorkspaceType is defined, and the type\u2019s name. For example, if a WorkspaceType <code>example</code> is created in the <code>root:org</code> workspace, the implicit initializer name is <code>root:org:example</code>.</p> object <p>limitAllowedChildren specifies constraints for sub-workspaces created in workspaces of this type. These are in addition to child constraints of types this one extends.</p> boolean <p>none means that no type matches.</p> array <p>types is a list of WorkspaceTypes that match. A workspace type extending another workspace type automatically is considered as that extended type as well (even transitively).</p> <p>An empty list matches all types.</p> object <p>WorkspaceTypeReference is a globally unique, fully qualified reference to a workspace type.</p> string Required <p>name is the name of the WorkspaceType</p> string <p>path is an absolute reference to the workspace that owns this type, e.g. root:org:ws.</p> object <p>limitAllowedParents specifies constraints for the parent workspace that workspaces of this type are created in. These are in addition to parent constraints of types this one extends.</p> boolean <p>none means that no type matches.</p> array <p>types is a list of WorkspaceTypes that match. A workspace type extending another workspace type automatically is considered as that extended type as well (even transitively).</p> <p>An empty list matches all types.</p> object <p>WorkspaceTypeReference is a globally unique, fully qualified reference to a workspace type.</p> string Required <p>name is the name of the WorkspaceType</p> string <p>path is an absolute reference to the workspace that owns this type, e.g. root:org:ws.</p> object <p>WorkspaceTypeStatus defines the observed state of WorkspaceType.</p> array <p>conditions is a list of conditions that apply to the APIExport.</p> object <p>Condition defines an observation of a object operational state.</p> string Required <p>Last time the condition transitioned from one status to another. This should be when the underlying condition changed. If that is not known, then using the time when the API field changed is acceptable.</p> string <p>A human readable message indicating details about the transition. This field may be empty.</p> string <p>The reason for the condition\u2019s last transition in CamelCase. The specific API may choose whether or not this field is considered a guaranteed API. This field may not be empty.</p> string <p>Severity provides an explicit classification of Reason code, so the users or machines can immediately understand the current situation and act accordingly. The Severity field MUST be set only when Status=False.</p> string Required <p>Status of the condition, one of True, False, Unknown.</p> string Required <p>Type of condition in CamelCase or in foo.example.com/CamelCase. Many .condition.type values are consistent across resources like Available, but because arbitrary conditions can be useful (see .node.status.conditions), the ability to deconflict is important.</p> array <p>virtualWorkspaces contains all APIExport virtual workspace URLs.</p> object string Required <p>url is a WorkspaceType initialization virtual workspace URL.</p>"},{"location":"reference/crd/tenancy.kcp.io/workspacetypes/#v1alpha1-.apiVersion","title":".apiVersion","text":""},{"location":"reference/crd/tenancy.kcp.io/workspacetypes/#v1alpha1-.kind","title":".kind","text":""},{"location":"reference/crd/tenancy.kcp.io/workspacetypes/#v1alpha1-.metadata","title":".metadata","text":""},{"location":"reference/crd/tenancy.kcp.io/workspacetypes/#v1alpha1-.metadata.name","title":".metadata.name","text":""},{"location":"reference/crd/tenancy.kcp.io/workspacetypes/#v1alpha1-.spec","title":".spec","text":""},{"location":"reference/crd/tenancy.kcp.io/workspacetypes/#v1alpha1-.spec.additionalWorkspaceLabels","title":".spec.additionalWorkspaceLabels","text":""},{"location":"reference/crd/tenancy.kcp.io/workspacetypes/#v1alpha1-.spec.defaultAPIBindings","title":".spec.defaultAPIBindings","text":""},{"location":"reference/crd/tenancy.kcp.io/workspacetypes/#v1alpha1-.spec.defaultAPIBindings[*]","title":".spec.defaultAPIBindings[*]","text":""},{"location":"reference/crd/tenancy.kcp.io/workspacetypes/#v1alpha1-.spec.defaultAPIBindings[*].export","title":".spec.defaultAPIBindings[*].export","text":""},{"location":"reference/crd/tenancy.kcp.io/workspacetypes/#v1alpha1-.spec.defaultAPIBindings[*].path","title":".spec.defaultAPIBindings[*].path","text":""},{"location":"reference/crd/tenancy.kcp.io/workspacetypes/#v1alpha1-.spec.defaultChildWorkspaceType","title":".spec.defaultChildWorkspaceType","text":""},{"location":"reference/crd/tenancy.kcp.io/workspacetypes/#v1alpha1-.spec.defaultChildWorkspaceType.name","title":".spec.defaultChildWorkspaceType.name","text":""},{"location":"reference/crd/tenancy.kcp.io/workspacetypes/#v1alpha1-.spec.defaultChildWorkspaceType.path","title":".spec.defaultChildWorkspaceType.path","text":""},{"location":"reference/crd/tenancy.kcp.io/workspacetypes/#v1alpha1-.spec.extend","title":".spec.extend","text":""},{"location":"reference/crd/tenancy.kcp.io/workspacetypes/#v1alpha1-.spec.extend.with","title":".spec.extend.with","text":""},{"location":"reference/crd/tenancy.kcp.io/workspacetypes/#v1alpha1-.spec.extend.with[*]","title":".spec.extend.with[*]","text":""},{"location":"reference/crd/tenancy.kcp.io/workspacetypes/#v1alpha1-.spec.extend.with[*].name","title":".spec.extend.with[*].name","text":""},{"location":"reference/crd/tenancy.kcp.io/workspacetypes/#v1alpha1-.spec.extend.with[*].path","title":".spec.extend.with[*].path","text":""},{"location":"reference/crd/tenancy.kcp.io/workspacetypes/#v1alpha1-.spec.initializer","title":".spec.initializer","text":""},{"location":"reference/crd/tenancy.kcp.io/workspacetypes/#v1alpha1-.spec.limitAllowedChildren","title":".spec.limitAllowedChildren","text":""},{"location":"reference/crd/tenancy.kcp.io/workspacetypes/#v1alpha1-.spec.limitAllowedChildren.none","title":".spec.limitAllowedChildren.none","text":""},{"location":"reference/crd/tenancy.kcp.io/workspacetypes/#v1alpha1-.spec.limitAllowedChildren.types","title":".spec.limitAllowedChildren.types","text":""},{"location":"reference/crd/tenancy.kcp.io/workspacetypes/#v1alpha1-.spec.limitAllowedChildren.types[*]","title":".spec.limitAllowedChildren.types[*]","text":""},{"location":"reference/crd/tenancy.kcp.io/workspacetypes/#v1alpha1-.spec.limitAllowedChildren.types[*].name","title":".spec.limitAllowedChildren.types[*].name","text":""},{"location":"reference/crd/tenancy.kcp.io/workspacetypes/#v1alpha1-.spec.limitAllowedChildren.types[*].path","title":".spec.limitAllowedChildren.types[*].path","text":""},{"location":"reference/crd/tenancy.kcp.io/workspacetypes/#v1alpha1-.spec.limitAllowedParents","title":".spec.limitAllowedParents","text":""},{"location":"reference/crd/tenancy.kcp.io/workspacetypes/#v1alpha1-.spec.limitAllowedParents.none","title":".spec.limitAllowedParents.none","text":""},{"location":"reference/crd/tenancy.kcp.io/workspacetypes/#v1alpha1-.spec.limitAllowedParents.types","title":".spec.limitAllowedParents.types","text":""},{"location":"reference/crd/tenancy.kcp.io/workspacetypes/#v1alpha1-.spec.limitAllowedParents.types[*]","title":".spec.limitAllowedParents.types[*]","text":""},{"location":"reference/crd/tenancy.kcp.io/workspacetypes/#v1alpha1-.spec.limitAllowedParents.types[*].name","title":".spec.limitAllowedParents.types[*].name","text":""},{"location":"reference/crd/tenancy.kcp.io/workspacetypes/#v1alpha1-.spec.limitAllowedParents.types[*].path","title":".spec.limitAllowedParents.types[*].path","text":""},{"location":"reference/crd/tenancy.kcp.io/workspacetypes/#v1alpha1-.status","title":".status","text":""},{"location":"reference/crd/tenancy.kcp.io/workspacetypes/#v1alpha1-.status.conditions","title":".status.conditions","text":""},{"location":"reference/crd/tenancy.kcp.io/workspacetypes/#v1alpha1-.status.conditions[*]","title":".status.conditions[*]","text":""},{"location":"reference/crd/tenancy.kcp.io/workspacetypes/#v1alpha1-.status.conditions[*].lastTransitionTime","title":".status.conditions[*].lastTransitionTime","text":""},{"location":"reference/crd/tenancy.kcp.io/workspacetypes/#v1alpha1-.status.conditions[*].message","title":".status.conditions[*].message","text":""},{"location":"reference/crd/tenancy.kcp.io/workspacetypes/#v1alpha1-.status.conditions[*].reason","title":".status.conditions[*].reason","text":""},{"location":"reference/crd/tenancy.kcp.io/workspacetypes/#v1alpha1-.status.conditions[*].severity","title":".status.conditions[*].severity","text":""},{"location":"reference/crd/tenancy.kcp.io/workspacetypes/#v1alpha1-.status.conditions[*].status","title":".status.conditions[*].status","text":""},{"location":"reference/crd/tenancy.kcp.io/workspacetypes/#v1alpha1-.status.conditions[*].type","title":".status.conditions[*].type","text":""},{"location":"reference/crd/tenancy.kcp.io/workspacetypes/#v1alpha1-.status.virtualWorkspaces","title":".status.virtualWorkspaces","text":""},{"location":"reference/crd/tenancy.kcp.io/workspacetypes/#v1alpha1-.status.virtualWorkspaces[*]","title":".status.virtualWorkspaces[*]","text":""},{"location":"reference/crd/tenancy.kcp.io/workspacetypes/#v1alpha1-.status.virtualWorkspaces[*].url","title":".status.virtualWorkspaces[*].url","text":""},{"location":"reference/crd/topology.kcp.io/partitions/","title":"Partition","text":""},{"location":"reference/crd/topology.kcp.io/partitions/#partition-crd-schema-reference-group-topologykcpio","title":"Partition CRD schema reference (group topology.kcp.io)","text":"Partition defines the selection of a set of shards along multiple dimensions. Partitions can get automatically generated through a partitioner or manually crafted.  Full name: partitions.topology.kcp.io Group: topology.kcp.io Singular name: partition Plural name: partitions Scope: Cluster Versions: v1alpha1"},{"location":"reference/crd/topology.kcp.io/partitions/#v1alpha1","title":"Version v1alpha1","text":""},{"location":"reference/crd/topology.kcp.io/partitions/#property-details-v1alpha1","title":"Properties","text":"string <p>APIVersion defines the versioned schema of this representation of an object. Servers should convert recognized schemas to the latest internal value, and may reject unrecognized values. More info: https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources</p> string <p>Kind is a string value representing the REST resource this object represents. Servers may infer this from the endpoint the client submits requests to. Cannot be updated. In CamelCase. More info: https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds</p> object object Required <p>spec holds the desired state.</p> object <p>selector (optional) is a label selector that filters shard targets.</p> array <p>matchExpressions is a list of label selector requirements. The requirements are ANDed.</p> object <p>A label selector requirement is a selector that contains values, a key, and an operator that relates the key and values.</p> string Required <p>key is the label key that the selector applies to.</p> string Required <p>operator represents a key\u2019s relationship to a set of values. Valid operators are In, NotIn, Exists and DoesNotExist.</p> array <p>values is an array of string values. If the operator is In or NotIn, the values array must be non-empty. If the operator is Exists or DoesNotExist, the values array must be empty. This array is replaced during a strategic merge patch.</p> string object <p>matchLabels is a map of {key,value} pairs. A single {key,value} in the matchLabels map is equivalent to an element of matchExpressions, whose key field is \u201ckey\u201d, the operator is \u201cIn\u201d, and the values array contains only \u201cvalue\u201d. The requirements are ANDed.</p>"},{"location":"reference/crd/topology.kcp.io/partitions/#v1alpha1-.apiVersion","title":".apiVersion","text":""},{"location":"reference/crd/topology.kcp.io/partitions/#v1alpha1-.kind","title":".kind","text":""},{"location":"reference/crd/topology.kcp.io/partitions/#v1alpha1-.metadata","title":".metadata","text":""},{"location":"reference/crd/topology.kcp.io/partitions/#v1alpha1-.spec","title":".spec","text":""},{"location":"reference/crd/topology.kcp.io/partitions/#v1alpha1-.spec.selector","title":".spec.selector","text":""},{"location":"reference/crd/topology.kcp.io/partitions/#v1alpha1-.spec.selector.matchExpressions","title":".spec.selector.matchExpressions","text":""},{"location":"reference/crd/topology.kcp.io/partitions/#v1alpha1-.spec.selector.matchExpressions[*]","title":".spec.selector.matchExpressions[*]","text":""},{"location":"reference/crd/topology.kcp.io/partitions/#v1alpha1-.spec.selector.matchExpressions[*].key","title":".spec.selector.matchExpressions[*].key","text":""},{"location":"reference/crd/topology.kcp.io/partitions/#v1alpha1-.spec.selector.matchExpressions[*].operator","title":".spec.selector.matchExpressions[*].operator","text":""},{"location":"reference/crd/topology.kcp.io/partitions/#v1alpha1-.spec.selector.matchExpressions[*].values","title":".spec.selector.matchExpressions[*].values","text":""},{"location":"reference/crd/topology.kcp.io/partitions/#v1alpha1-.spec.selector.matchExpressions[*].values[*]","title":".spec.selector.matchExpressions[*].values[*]","text":""},{"location":"reference/crd/topology.kcp.io/partitions/#v1alpha1-.spec.selector.matchLabels","title":".spec.selector.matchLabels","text":""},{"location":"reference/crd/topology.kcp.io/partitionsets/","title":"PartitionSet","text":""},{"location":"reference/crd/topology.kcp.io/partitionsets/#partitionset-crd-schema-reference-group-topologykcpio","title":"PartitionSet CRD schema reference (group topology.kcp.io)","text":"PartitionSet defines a target domain and dimensions to divide a set of shards into 1 or more partitions.  Full name: partitionsets.topology.kcp.io Group: topology.kcp.io Singular name: partitionset Plural name: partitionsets Scope: Cluster Versions: v1alpha1"},{"location":"reference/crd/topology.kcp.io/partitionsets/#v1alpha1","title":"Version v1alpha1","text":""},{"location":"reference/crd/topology.kcp.io/partitionsets/#property-details-v1alpha1","title":"Properties","text":"string <p>APIVersion defines the versioned schema of this representation of an object. Servers should convert recognized schemas to the latest internal value, and may reject unrecognized values. More info: https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources</p> string <p>Kind is a string value representing the REST resource this object represents. Servers may infer this from the endpoint the client submits requests to. Cannot be updated. In CamelCase. More info: https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds</p> object object Required <p>spec holds the desired state.</p> array <p>dimensions (optional) are used to group shards into partitions</p> string object <p>shardSelector (optional) specifies filtering for shard targets.</p> array <p>matchExpressions is a list of label selector requirements. The requirements are ANDed.</p> object <p>A label selector requirement is a selector that contains values, a key, and an operator that relates the key and values.</p> string Required <p>key is the label key that the selector applies to.</p> string Required <p>operator represents a key\u2019s relationship to a set of values. Valid operators are In, NotIn, Exists and DoesNotExist.</p> array <p>values is an array of string values. If the operator is In or NotIn, the values array must be non-empty. If the operator is Exists or DoesNotExist, the values array must be empty. This array is replaced during a strategic merge patch.</p> string object <p>matchLabels is a map of {key,value} pairs. A single {key,value} in the matchLabels map is equivalent to an element of matchExpressions, whose key field is \u201ckey\u201d, the operator is \u201cIn\u201d, and the values array contains only \u201cvalue\u201d. The requirements are ANDed.</p> object <p>status holds information about the current status</p> array <p>conditions is a list of conditions that apply to the APIExportEndpointSlice.</p> object <p>Condition defines an observation of a object operational state.</p> string Required <p>Last time the condition transitioned from one status to another. This should be when the underlying condition changed. If that is not known, then using the time when the API field changed is acceptable.</p> string <p>A human readable message indicating details about the transition. This field may be empty.</p> string <p>The reason for the condition\u2019s last transition in CamelCase. The specific API may choose whether or not this field is considered a guaranteed API. This field may not be empty.</p> string <p>Severity provides an explicit classification of Reason code, so the users or machines can immediately understand the current situation and act accordingly. The Severity field MUST be set only when Status=False.</p> string Required <p>Status of the condition, one of True, False, Unknown.</p> string Required <p>Type of condition in CamelCase or in foo.example.com/CamelCase. Many .condition.type values are consistent across resources like Available, but because arbitrary conditions can be useful (see .node.status.conditions), the ability to deconflict is important.</p> integer <p>count is the total number of partitions.</p>"},{"location":"reference/crd/topology.kcp.io/partitionsets/#v1alpha1-.apiVersion","title":".apiVersion","text":""},{"location":"reference/crd/topology.kcp.io/partitionsets/#v1alpha1-.kind","title":".kind","text":""},{"location":"reference/crd/topology.kcp.io/partitionsets/#v1alpha1-.metadata","title":".metadata","text":""},{"location":"reference/crd/topology.kcp.io/partitionsets/#v1alpha1-.spec","title":".spec","text":""},{"location":"reference/crd/topology.kcp.io/partitionsets/#v1alpha1-.spec.dimensions","title":".spec.dimensions","text":""},{"location":"reference/crd/topology.kcp.io/partitionsets/#v1alpha1-.spec.dimensions[*]","title":".spec.dimensions[*]","text":""},{"location":"reference/crd/topology.kcp.io/partitionsets/#v1alpha1-.spec.shardSelector","title":".spec.shardSelector","text":""},{"location":"reference/crd/topology.kcp.io/partitionsets/#v1alpha1-.spec.shardSelector.matchExpressions","title":".spec.shardSelector.matchExpressions","text":""},{"location":"reference/crd/topology.kcp.io/partitionsets/#v1alpha1-.spec.shardSelector.matchExpressions[*]","title":".spec.shardSelector.matchExpressions[*]","text":""},{"location":"reference/crd/topology.kcp.io/partitionsets/#v1alpha1-.spec.shardSelector.matchExpressions[*].key","title":".spec.shardSelector.matchExpressions[*].key","text":""},{"location":"reference/crd/topology.kcp.io/partitionsets/#v1alpha1-.spec.shardSelector.matchExpressions[*].operator","title":".spec.shardSelector.matchExpressions[*].operator","text":""},{"location":"reference/crd/topology.kcp.io/partitionsets/#v1alpha1-.spec.shardSelector.matchExpressions[*].values","title":".spec.shardSelector.matchExpressions[*].values","text":""},{"location":"reference/crd/topology.kcp.io/partitionsets/#v1alpha1-.spec.shardSelector.matchExpressions[*].values[*]","title":".spec.shardSelector.matchExpressions[*].values[*]","text":""},{"location":"reference/crd/topology.kcp.io/partitionsets/#v1alpha1-.spec.shardSelector.matchLabels","title":".spec.shardSelector.matchLabels","text":""},{"location":"reference/crd/topology.kcp.io/partitionsets/#v1alpha1-.status","title":".status","text":""},{"location":"reference/crd/topology.kcp.io/partitionsets/#v1alpha1-.status.conditions","title":".status.conditions","text":""},{"location":"reference/crd/topology.kcp.io/partitionsets/#v1alpha1-.status.conditions[*]","title":".status.conditions[*]","text":""},{"location":"reference/crd/topology.kcp.io/partitionsets/#v1alpha1-.status.conditions[*].lastTransitionTime","title":".status.conditions[*].lastTransitionTime","text":""},{"location":"reference/crd/topology.kcp.io/partitionsets/#v1alpha1-.status.conditions[*].message","title":".status.conditions[*].message","text":""},{"location":"reference/crd/topology.kcp.io/partitionsets/#v1alpha1-.status.conditions[*].reason","title":".status.conditions[*].reason","text":""},{"location":"reference/crd/topology.kcp.io/partitionsets/#v1alpha1-.status.conditions[*].severity","title":".status.conditions[*].severity","text":""},{"location":"reference/crd/topology.kcp.io/partitionsets/#v1alpha1-.status.conditions[*].status","title":".status.conditions[*].status","text":""},{"location":"reference/crd/topology.kcp.io/partitionsets/#v1alpha1-.status.conditions[*].type","title":".status.conditions[*].type","text":""},{"location":"reference/crd/topology.kcp.io/partitionsets/#v1alpha1-.status.count","title":".status.count","text":""},{"location":"setup/","title":"Setting Up kcp","text":""},{"location":"setup/#quickstart","title":"Quickstart","text":"<p>Get started with kcp by running it locally.</p>"},{"location":"setup/#installation-with-helm","title":"Installation with Helm","text":"<p>Install kcp on an existing Kubernetes cluster via the official Helm chart.</p>"},{"location":"setup/#kubectl-plugins","title":"kubectl Plugins","text":"<p>How to install and use the kubectl kcp plugin.</p>"},{"location":"setup/helm/","title":"Installation with Helm","text":""},{"location":"setup/helm/#kcp-chart","title":"<code>kcp</code> Chart","text":"<p>We provide a Helm chart to install kcp on top of an existing Kubernetes cluster. Its source is available in kcp-dev/helm-charts.</p> <p>The chart repository can be added via:</p> <pre><code>helm repo add kcp https://kcp-dev.github.io/helm-charts\n</code></pre> <p>The chart can then be installed as <code>kcp/kcp</code>:</p> <pre><code>helm install my-kcp kcp/kcp -f my-values.yaml\n</code></pre>"},{"location":"setup/helm/#values","title":"Values","text":"<p>At the very least, <code>.externalHostname</code> needs to be set and point to a DNS hostname that will point to the front-proxy's external hostname.</p> <p>A list of all values is available in the git repository.</p>"},{"location":"setup/helm/#multi-shard-charts","title":"Multi-Shard Charts","text":"<p>We are also working on a collection of charts that allow for a multi-shard deployment. Those are currently WIP and considered highly experimental, but instructions are also available on GitHub.</p>"},{"location":"setup/kubectl-plugin/","title":"kubectl Plugins","text":"<p>kcp provides kubectl plugins that simplify the operations with the kcp server.</p> <p>You can install the plugins from the current repo:</p> <pre><code>$ make install\n</code></pre> <p>or use krew:</p> <pre><code>$ kubectl krew index add kcp-dev https://github.com/kcp-dev/krew-index.git\n$ kubectl krew install kcp-dev/kcp\n$ kubectl krew install kcp-dev/ws\n$ kubectl krew install kcp-dev/create-workspace\n</code></pre> <p>The plugins will be automatically discovered by your current <code>kubectl</code> binary:</p> <pre><code>$ kubectl kcp\nKCP is the easiest way to manage Kubernetes applications against one or more clusters, by giving you a personal control plane that schedules your workloads onto one or many clusters, and making it simple to pick up and move. Advanced use cases including spreading your apps across clusters for resiliency, scheduling batch workloads onto clusters with free capacity, and enabling collaboration for individual teams without having access to the underlying clusters.\n\nThis command provides KCP specific sub-command for kubectl.\n\nUsage:\n  kcp [command]\n\nAvailable Commands:\n  bind        Bind different types into current workspace.\n  claims      Operations related to viewing or updating permission claims\n  completion  Generate the autocompletion script for the specified shell\n  crd         CRD related operations\n  help        Help about any command\n  workspace   Manages KCP workspaces\n\nFlags:\n      --add_dir_header                   If true, adds the file directory to the header of the log messages\n      --alsologtostderr                  log to standard error as well as files (no effect when -logtostderr=true)\n  -h, --help                             help for kcp\n      --log_backtrace_at traceLocation   when logging hits line file:N, emit a stack trace (default :0)\n      --log_dir string                   If non-empty, write log files in this directory (no effect when -logtostderr=true)\n      --log_file string                  If non-empty, use this log file (no effect when -logtostderr=true)\n      --log_file_max_size uint           Defines the maximum size a log file can grow to (no effect when -logtostderr=true). Unit is megabytes. If the value is 0, the maximum file size is unlimited. (default 1800)\n      --logtostderr                      log to standard error instead of files (default true)\n      --one_output                       If true, only write logs to their native severity level (vs also writing to each lower severity level; no effect when -logtostderr=true)\n      --skip_headers                     If true, avoid header prefixes in the log messages\n      --skip_log_headers                 If true, avoid headers when opening log files (no effect when -logtostderr=true)\n      --stderrthreshold severity         logs at or above this threshold go to stderr when writing to files and stderr (no effect when -logtostderr=true or -alsologtostderr=true) (default 2)\n  -v, --v Level                          number for the log level verbosity\n      --version                          version for kcp\n      --vmodule moduleSpec               comma-separated list of pattern=N settings for file-filtered logging\n\nUse \"kcp [command] --help\" for more information about a command.\n\n$ kubectl ws .                                # a short-cut for kubectl kcp workspace\n$ kubectl create workspace my-workspace       # a short-cut for kubectl kcp workspace create\n</code></pre>"},{"location":"setup/quickstart/","title":"Quickstart","text":""},{"location":"setup/quickstart/#prerequisites","title":"Prerequisites","text":"<ul> <li>kubectl</li> </ul>"},{"location":"setup/quickstart/#download-kcp","title":"Download kcp","text":"<p>Visit our latest release page and download <code>kcp</code> and <code>kubectl-kcp-plugin</code> that match your operating system and architecture.</p> <p>Extract <code>kcp</code> and <code>kubectl-kcp-plugin</code> and place all the files in the <code>bin</code> directories somewhere in your <code>$PATH</code>.</p>"},{"location":"setup/quickstart/#start-kcp","title":"Start kcp","text":"<p>You can start kcp using this command:</p> <pre><code>kcp start\n</code></pre> <p>This launches kcp in the foreground. You can press <code>ctrl-c</code> to stop it.</p> <p>To see a complete list of server options, run <code>kcp start options</code>.</p>"},{"location":"setup/quickstart/#set-your-kubeconfig","title":"Set your KUBECONFIG","text":"<p>During its startup, kcp generates a kubeconfig in <code>.kcp/admin.kubeconfig</code>. Use this to connect to kcp and display the version to confirm it's working:</p> <pre><code>$ export KUBECONFIG=.kcp/admin.kubeconfig\n$ kubectl version\nClient Version: v1.31.1\nKustomize Version: v5.4.2\nServer Version: v1.31.0+kcp-v0.26.0\n</code></pre>"},{"location":"setup/quickstart/#next-steps","title":"Next steps","text":"<p>Thanks for checking out our quickstart!</p> <p>If you're interested in learning more about all the features kcp has to offer, please check out our additional documentation:</p> <ul> <li>Concepts - a high level overview of kcp concepts</li> <li>Workspaces - a more thorough introduction on kcp's workspaces</li> <li>kubectl plugin</li> <li>Authorization - how kcp manages access control to workspaces and content</li> <li>Virtual workspaces - details on kcp's mechanism for virtual views of workspace content</li> </ul>"}]}